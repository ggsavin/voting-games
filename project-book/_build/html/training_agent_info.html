

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Training Agents &#8212; Voting in Hidden Role Games</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'training_agent_info';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Gameplay Indicators" href="indicators.html" />
    <link rel="prev" title="Agent Implementation" href="agent-implementation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/midjourney-werewolves.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/midjourney-werewolves.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    COMP 5903 - Voting in Hidden Role Games
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="game-description.html">Werewolf - The Game</a></li>
<li class="toctree-l1"><a class="reference internal" href="literature-review.html">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="game-implementation.html">Werewolf - The Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent-implementation.html">Agent Implementation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Training Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="indicators.html">Gameplay Indicators</a></li>
<li class="toctree-l1"><a class="reference internal" href="game_visualization.html">Replay visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="plurality-analysis.html">Plurality Voting</a></li>
<li class="toctree-l1"><a class="reference internal" href="approval-analysis.html">Approval Voting</a></li>
<li class="toctree-l1"><a class="reference internal" href="dicsussion.html">Discussion</a></li>
<li class="toctree-l1"><a class="reference internal" href="conclusion-future.html">Conclusion and Future Directions</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games/issues/new?title=Issue%20on%20page%20%2Ftraining_agent_info.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/training_agent_info.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training Agents</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#filling-buffer">Filling Buffer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantage-estimation">Advantage estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-calculation">Loss calculation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlflow">MLflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#observation-vectorization">Observation Vectorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-time">Training Time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-results">Training Results</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game, convert_obs
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
import mlflow
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="training-agents">
<h1>Training Agents<a class="headerlink" href="#training-agents" title="Permalink to this heading">#</a></h1>
<p>Because we used PPO to train our actor-critic truncated BPTT LSTM, we modified our code to closely resemble the work done in <span id="id1">[<a class="reference internal" href="bibliography.html#id39" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span>.
The steps we follow are very closely related to our <a class="reference internal" href="terminology.html#ppo-alg-header"><span class="std std-ref">PPO Algorithm</span></a>.</p>
<p>We first create a trainer class that stores our config and uses it to set up our buffer, our central villager policy and our optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PPOTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">wolf_policy</span><span class="p">,</span> <span class="n">run_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s2">&quot;run&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="n">voting_type</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</pre></div>
</div>
<p>We then call our train function which sets up the main loop consisting of gatheric metrics periodically, filling up our buffer, calculating our loss and updating our neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_threshold</span><span class="o">=</span><span class="mf">50.0</span><span class="p">):</span>

    <span class="c1"># TODO: put mlflow logging into a callback</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_game&quot;</span><span class="p">][</span><span class="s1">&#39;gameplay&#39;</span><span class="p">])</span>

    <span class="n">loop</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;updates&quot;</span><span class="p">]),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># if the average wins when we do periodic checks of the models scoring is above the save threshold, we save or overwrite the model</span>
    <span class="c1"># model_save_threshold = 50.0</span>
    <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loop</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">tid</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print(f&#39;Playing games with our trained agent after {epid} epochs&#39;)</span>
            <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Playing games and averaging score&quot;</span><span class="p">)</span>
            <span class="n">wins</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">score_gathering</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">replays</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">score_gathering</span><span class="p">:</span>
                <span class="n">game_wins</span><span class="p">,</span> <span class="n">game_replays</span> <span class="o">=</span> <span class="n">play_recurrent_game</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                <span class="n">num_times</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                <span class="n">hidden_state_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">],</span>
                                                <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>
                <span class="n">wins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_wins</span><span class="p">)</span>
                <span class="n">replays</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_replays</span><span class="p">)</span>
                <span class="n">score_gathering</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg wins with current policy : </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># flatten all of our replays and get our current stats</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">aggregate_stats_from_replays</span><span class="p">(</span>
                <span class="p">[</span><span class="n">replay</span> <span class="k">for</span> <span class="n">game_replays</span> <span class="ow">in</span> <span class="n">replays</span> <span class="k">for</span> <span class="n">replay</span> <span class="ow">in</span> <span class="n">game_replays</span><span class="p">],</span> <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">))</span>

            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;avg_wins/100&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">save_threshold</span><span class="p">:</span>
                <span class="n">save_threshold</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">log_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s1">&#39;checkpoint&#39;</span><span class="p">)</span>
                <span class="c1"># torch.save(self.agent.state_dict(), f&#39;{voting_type}_agent_{self.config[&quot;config_game&quot;][&quot;gameplay&quot;][&quot;num_agents&quot;]}_score_{save_threshold}&#39;)</span>


        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Filling buffer&quot;</span><span class="p">)</span>
        <span class="c1"># fill buffer</span>
        <span class="c1"># _scaled_rewards</span>
        <span class="n">buff</span> <span class="o">=</span> <span class="n">fill_recurrent_buffer_scaled_rewards</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">],</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span>
                                        <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>

        <span class="c1"># train info will hold our metrics</span>
        <span class="n">train_info</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO List how many items we are training on</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch Training on </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">games</span><span class="si">}</span><span class="s1"> games&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;epochs&#39;</span><span class="p">]):</span>
            <span class="c1"># run through batches and train network</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">buff</span><span class="o">.</span><span class="n">get_minibatch_generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]):</span>
                <span class="n">train_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_minibatch_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                        <span class="n">batch</span><span class="p">,</span> 
                                                        <span class="n">clip_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;clip_range&#39;</span><span class="p">],</span> 
                                                        <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span> 
                                                        <span class="n">v_loss_coef</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;value_loss_coefficient&#39;</span><span class="p">],</span>
                                                        <span class="n">grad_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;max_grad_norm&#39;</span><span class="p">],</span>
                                                        <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">))</span>

        <span class="n">train_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_info</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;policy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;value loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;total loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;entropy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<section id="filling-buffer">
<span id="fill-buffer"></span><h2>Filling Buffer<a class="headerlink" href="#filling-buffer" title="Permalink to this heading">#</a></h2>
<p>To fill our buffer, we collect each players full game trajectories <span class="math notranslate nohighlight">\(\tau\)</span>, with special attention given to the reward distributed to dead agents, rewarding agents that died earlier slightly less. This is a heuristic we developed to reward all the agents, however, we do have a fill function where this reward is not distributed to dead players.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward_at_max_round</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">**</span><span class="p">(</span><span class="n">max_game_rounds</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">fill_recurrent_buffer_scaled_rewards</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">wolf_policy</span><span class="p">,</span> <span class="n">villager_agent</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="n">buffer</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span> <span class="n">gae_lambda</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;gae_lambda&quot;</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;buffer_games_per_update&quot;</span><span class="p">]):</span>
        <span class="c1">## Play the game </span>
        <span class="n">next_observations</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminations</span><span class="p">,</span> <span class="n">truncations</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="c1"># init recurrent stuff for actor and critic to 0 as well</span>
        <span class="n">magent_obs</span> <span class="o">=</span> <span class="p">{</span><span class="n">agent</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;actions&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;logprobs&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;values&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;terms&#39;</span><span class="p">:</span> <span class="p">[],</span>

                              <span class="c1"># obs size, and 1,1,64 as we pass batch first</span>
                              <span class="s1">&#39;hcxs&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> 
                                        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))]</span>
                    <span class="p">}</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">agent_roles</span><span class="p">[</span><span class="n">agent</span><span class="p">]}</span>
        
        <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">while</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">:</span>
            <span class="n">observations</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">next_observations</span><span class="p">)</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">villagers</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span>
            <span class="n">wolves</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span>

             <span class="c1">## VILLAGER LOGIC ##</span>
            <span class="n">v_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">convert_obs</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;observation&#39;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">])</span>

            <span class="c1"># TODO: maybe this can be sped up? </span>
            <span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span><span class="p">)</span> <span class="k">for</span> <span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span> <span class="ow">in</span> <span class="p">[</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;hcxs&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">]])</span>
            <span class="n">hxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hxs</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cxs</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">policies</span><span class="p">,</span> <span class="n">value</span> <span class="p">,</span> <span class="n">cells</span> <span class="o">=</span> <span class="n">villager_agent</span><span class="p">(</span><span class="n">v_obs</span><span class="p">,</span> <span class="p">(</span><span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span><span class="p">))</span>
            <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">policies</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">hxs_new</span><span class="p">,</span> <span class="n">cxs_new</span> <span class="o">=</span> <span class="n">cells</span>
            <span class="n">hxs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">hxs_new</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">cxs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">cxs_new</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">villager</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">villagers</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
                    <span class="n">actions</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
                    <span class="n">actions</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;hcxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">hxs_new</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">cxs_new</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)))</span>


                <span class="c1"># TODO : Update these somehow</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;obs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">v_obs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="mi">0</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
                    <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
                <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
                    <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># how do we get these</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">policy</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="k">for</span> <span class="n">policy</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policies</span><span class="p">,</span> <span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">])],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>

            <span class="c1"># wolf steps</span>
            <span class="n">phase</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">wolf</span> <span class="ow">in</span> <span class="n">wolves</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="n">wolf_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">wolf</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="n">wolf_action</span><span class="p">)</span>
                <span class="n">actions</span><span class="p">[</span><span class="n">wolf</span><span class="p">]</span> <span class="o">=</span> <span class="n">wolf_action</span>

            <span class="c1"># actions = actions | wolf_policy(env)</span>
        
            <span class="n">next_observations</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminations</span><span class="p">,</span> <span class="n">truncations</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">:</span>
                <span class="c1"># dividing rewards by 100</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;rewards&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span><span class="o">/</span><span class="mf">100.0</span><span class="p">)</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;terms&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">terminations</span><span class="p">[</span><span class="n">villager</span><span class="p">])</span>

            <span class="c1"># update wolf_action appropriately</span>
            <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">NIGHT</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>
            
            <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">ACCUSATION</span> <span class="ow">and</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">NIGHT</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1">## Update the end_game rewards for villagers that died before the end</span>
        <span class="n">max_game_rounds</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
        <span class="n">a_villager_who_made_it_to_end</span> <span class="o">=</span> <span class="p">[</span><span class="n">villager</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="n">max_game_rounds</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">reward_at_max_round</span> <span class="o">=</span> <span class="n">magent_obs</span><span class="p">[</span><span class="n">a_villager_who_made_it_to_end</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">:</span>
            <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward_at_max_round</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">**</span><span class="p">(</span><span class="n">max_game_rounds</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])))</span>

        <span class="c1">## Fill bigger buffer, keeping in mind sequence</span>
        <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="p">:</span>
            <span class="n">buffer</span><span class="o">.</span><span class="n">add_replay</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">agent</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>
</div>
</section>
<section id="advantage-estimation">
<span id="estimate-adv"></span><h2>Advantage estimation<a class="headerlink" href="#advantage-estimation" title="Permalink to this heading">#</a></h2>
<p>As mentioned in our <span class="xref myst">PPO algorithm</span>, we use generalized advantage estimates <span id="id2">[<a class="reference internal" href="bibliography.html#id38" title="John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.">SML+15</a>]</span>, and apply it to every single completed trajectory as it gets appended to the buffer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_calculate_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">game</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generalized advantage estimation (GAE)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]))):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">][</span><span class="nb">max</span><span class="p">((</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]),</span><span class="n">t</span><span class="p">)]</span> <span class="o">-</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
            <span class="n">advantages</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gae_lambda</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">[</span><span class="nb">max</span><span class="p">((</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]),</span><span class="n">t</span><span class="p">)]</span>

    <span class="c1"># adv and returns</span>
    <span class="k">return</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="loss-calculation">
<span id="minibatch-loss"></span><h2>Loss calculation<a class="headerlink" href="#loss-calculation" title="Permalink to this heading">#</a></h2>
<p>We calculate the loss per mini-batch, which is also a hypeparameter choice. This works for both approval and plurality models due to the policy outputs being in an array. We return and average out the policy, value, entropy and total losses.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_minibatch_loss</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">samples</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">clip_range</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">v_loss_coef</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">policies</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;observations&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;hxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;cxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()))</span>
    
    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropies</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">policy_head</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">policies</span><span class="p">):</span>
        <span class="c1"># append the log_probs for 1 -&gt; n other agent opinions</span>
        <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;actions&#39;</span><span class="p">][:,</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">entropies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">])</span>

    <span class="c1"># normalize advantages</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="c1"># need to repeat for amount of shape of policies (this way we know how many policy heads we need to watch out for)</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="n">norm_advantage</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">num_votes</span><span class="p">)</span>

    <span class="c1"># policy loss w/ surrogates</span>
    <span class="n">surr1</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">ratio</span>
    <span class="n">surr2</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">clip_range</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">clip_range</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">policy_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Value  function loss</span>
    <span class="n">clipped_values</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="n">clip_range</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">clip_range</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;returns&#39;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">clipped_values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">vf_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Entropy Bonus</span>
    <span class="n">entropy_loss</span> <span class="o">=</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Complete loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">policy_loss</span> <span class="o">-</span> <span class="n">v_loss_coef</span> <span class="o">*</span> <span class="n">vf_loss</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">entropy_loss</span><span class="p">)</span>

    <span class="c1"># Compute gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">grad_norm</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    
    <span class="k">return</span> <span class="p">[</span><span class="n">policy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>     <span class="c1"># policy loss</span>
            <span class="n">vf_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>         <span class="c1"># value loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>            <span class="c1"># total loss</span>
            <span class="n">entropy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>    <span class="c1"># entropy loss</span>
</pre></div>
</div>
</section>
<section id="mlflow">
<span id="mlflow-info"></span><h2>MLflow<a class="headerlink" href="#mlflow" title="Permalink to this heading">#</a></h2>
<p>MLflow<a class="footnote-reference brackets" href="#mlflow-tooling" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> is a powerful and popular MLOps open source tool for end-to-end machine learning lifecycle management. We compared this platform with others and simpler alternatives that just forcused on parameter and metric tracking, however due to the adoption of MLflow in industry, we felt it would be a good addition to the project as a sort of tutorial for any other researchers extending this work.</p>
<p>For every training loop, we track all the associated hyperparameters of the experiment, we also track the following metrics:</p>
<ul class="simple">
<li><p>Average villager wins periodically</p></li>
<li><p>Our Policy, Value, Entropy and total loss associated with the PPO algorithm</p></li>
<li><p><a class="reference internal" href="indicators.html#gameplay-indicators-list"><span class="std std-ref">Various indicators</span></a> we try to use to understand the trained agent behaviors</p></li>
</ul>
<p>If the average villager wins passes a threshold, the model dict is stored for future usage, and this new rate is set as the threshold.</p>
</section>
<section id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">#</a></h2>
<p>There are just under 20 hyperparameters, and can be split between gameplay parameters, neural network model parameters and PPO training parameters. Each value in this configuration object below can be changed, and is passed throughout training functions.</p>
<p>Initializations and hyper-parameter choices were greatly influenced by the exhaustive work in <span id="id4">[<a class="reference internal" href="bibliography.html#id36" title="Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in On-Policy reinforcement learning? a Large-Scale empirical study. arXiv preprint arXiv:2006.05990, June 2020. arXiv:2006.05990.">ARStanczyk+20</a>]</span> and <span id="id5">[<a class="reference internal" href="bibliography.html#id39" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span> for the LSTM portion.</p>
<ul class="simple">
<li><p>Model</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">recurrent_hidden_size</span></code>: ww</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">joint_mlp_size</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">split_mlp_size</span></code>:</p></li>
</ul>
</li>
<li><p>Training</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">updates</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer_games_per_update</span></code>:</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>:</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config_training</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;recurrent_layers&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;joint_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;split_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;num_votes&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;approval_states&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;training&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
        <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="mi">501</span><span class="p">,</span> 
        <span class="s2">&quot;buffer_games_per_update&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span> 
        <span class="s2">&quot;clip_range&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;value_loss_coefficient&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> 
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> 
        <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span> 
        <span class="s2">&quot;adam_eps&quot;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> 
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
        <span class="s2">&quot;gae_lambda&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> 
    <span class="p">}</span>
<span class="p">}</span>


<span class="n">config_game</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;rewards&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_death&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_win&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;player_loss&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;self_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_villager&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_wolf&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;no_viable_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;no_sleep&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;gameplay&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;accusation_phases&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;num_agents&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;num_werewolves&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;config_game&quot;</span><span class="p">:</span> <span class="n">config_game</span><span class="p">,</span>
    <span class="s2">&quot;config_training&quot;</span><span class="p">:</span> <span class="n">config_training</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="observation-vectorization">
<span id="convert-obs"></span><h2>Observation Vectorization<a class="headerlink" href="#observation-vectorization" title="Permalink to this heading">#</a></h2>
<p>While not a hyperparameter, the way we choose to vectorize our observations before inputting them into our neural network can have a big impact on overall training. We have a util function <code class="docutils literal notranslate"><span class="pre">convert_obs</span></code> which flattens the observation object into one 1D vector, or can also one-hot encode everything that is not already represented by <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(0\)</span>. For our training purposes, we found the not one-hot encoded flattening to work better, but experimenting with this vectorization and representation could be a fruitful avenue for future work.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_obs</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    observation will have the following information</span>
<span class="sd">        day (int)</span>
<span class="sd">        phase (int) </span>
<span class="sd">        self_id (int)</span>
<span class="sd">        player_status (list) - 0/1 for alive or dead</span>
<span class="sd">        roles (list) - 0/1 for villager or werewolf</span>
<span class="sd">        votes (dict) - dict with player and associated vote</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;votes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;player_status&quot;</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">()</span>
    
    <span class="c1"># phase length</span>
    <span class="n">day</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">]])</span>

    <span class="c1"># we can make the phase a one hot, hardcoded 3 phases</span>
    <span class="k">if</span> <span class="n">one_hot</span><span class="p">:</span>
        <span class="n">phase</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">self_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;self_id&#39;</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">])</span><span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

        <span class="n">phase</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]])</span>
        <span class="n">self_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;self_id&#39;</span><span class="p">]])</span>


    <span class="c1"># PLAYER STATUS (ALIVE OR DEAD)</span>
    <span class="n">player_status</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;player_status&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">player_roles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">day</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">self_id</span><span class="p">,</span> <span class="n">player_status</span><span class="p">,</span> <span class="n">player_roles</span><span class="p">,</span> <span class="n">votes</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="training-time">
<h2>Training Time<a class="headerlink" href="#training-time" title="Permalink to this heading">#</a></h2>
<p>TODO: Mention run-times with different buffer sizes</p>
<p>Maybe mention this in conclusions, future work
Training for 1 agent took roughly 2-3 hours, but completing runs was not as sucessful. For approval, it was longer.
Would take a 2-3 days at a time to collect stats, and there were bugs/mistakes.</p>
<p>TODO: Show the difference between reward sharing vs not for training.</p>
</section>
<section id="training-results">
<h2>Training Results<a class="headerlink" href="#training-results" title="Permalink to this heading">#</a></h2>
<p>We found more consistent results when the clip range was lower.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in a docker container
mlflow_client = mlflow.tracking.MlflowClient(tracking_uri=&quot;http://mlflow:5000&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># We have organized the runs in such a way
plurality_experiment = mlflow_client.get_experiment_by_name(&quot;Plurality Training&quot;)
plurality_runs = mlflow_client.search_runs([plurality_experiment.experiment_id])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>four_accusations = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;4_accusations&#39;&quot;)
three_accusations = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;3_accusations&#39;&quot;)
four_accusation_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in four_accusations]
three_accusation_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in three_accusations]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def tolerant_mean(arrs):
    lens = [len(i) for i in arrs]
    arr = np.ma.empty((np.max(lens),len(arrs)))
    arr.mask = True
    for idx, l in enumerate(arrs):
        arr[:len(l),idx] = l
    return arr.mean(axis = -1), arr.std(axis=-1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()
y_3, error_3 = tolerant_mean(three_accusation_votes)
y_4, error_4 = tolerant_mean(four_accusation_votes)

ax.plot(np.arange(len(y_3))+1, y_3, color=&#39;green&#39;)
ax.fill_between(np.arange(len(y_3))+1, y_3-error_3, y_3+error_3, color=&#39;green&#39;, alpha=0.3)

ax.plot(np.arange(len(y_4))+1, y_4, color=&#39;blue&#39;)
ax.fill_between(np.arange(len(y_4))+1, y_4-error_4, y_4+error_4, color=&#39;blue&#39;, alpha=0.3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PolyCollection at 0x7f33c8a5f220&gt;
</pre></div>
</div>
<img alt="_images/46942a7242469be478f81b0df3de84ab60b02b224faa957236e9e978c47e518e.png" src="_images/46942a7242469be478f81b0df3de84ab60b02b224faa957236e9e978c47e518e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()
y_3, error_3 = tolerant_mean(three_accusation_votes)
y_4, error_4 = tolerant_mean(four_accusation_votes)

ax.plot(np.arange(len(y_3))+1, y_3, color=&#39;green&#39;)
#ax.fill_between(np.arange(len(y_3))+1, y_3-error_3, y_3+error_3, color=&#39;green&#39;, alpha=0.3)

ax.plot(np.arange(len(y_4))+1, y_4, color=&#39;blue&#39;)
#ax.fill_between(np.arange(len(y_4))+1, y_4-error_4, y_4+error_4, color=&#39;blue&#39;, alpha=0.3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7f33c8acb040&gt;]
</pre></div>
</div>
<img alt="_images/048355c5963fdfa7046f9107ed4d4d189b7c7fcf636406efc26d2f4d2e2d2665.png" src="_images/048355c5963fdfa7046f9107ed4d4d189b7c7fcf636406efc26d2f4d2e2d2665.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>approval_experiment = mlflow_client.get_experiment_by_name(&quot;Approval Training&quot;)
approval_runs = mlflow_client.search_runs([plurality_experiment.experiment_id])

one_accusations = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;1_accusations&#39;&quot;)
#four_accusations = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;4_accusations&#39;&quot;)
#three_accusations = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;3_accusations&#39;&quot;)
one_accusation_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in one_accusations]

# four_accusation_votes = \
#     [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in four_accusations]
# three_accusation_votes = \
#     [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in three_accusations]

fig, ax = plt.subplots()
y_1, error_1 = tolerant_mean(one_accusation_votes)

ax.plot(np.arange(len(y_1))+1, y_1, color=&#39;green&#39;)
ax.fill_between(np.arange(len(y_1))+1, y_1-error_1, y_1+error_1, color=&#39;green&#39;, alpha=0.3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PolyCollection at 0x7f33c897b400&gt;
</pre></div>
</div>
<img alt="_images/e6809cc08ebdf4c1a17bc7ccd63a248bf3a199d8fb05610f606bab6ea6e68f77.png" src="_images/e6809cc08ebdf4c1a17bc7ccd63a248bf3a199d8fb05610f606bab6ea6e68f77.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()

ax.plot(np.arange(len(y_1))+1, y_1, color=&#39;green&#39;, label=&quot;Approval&quot;)
ax.plot(np.arange(len(y_4))+1, y_4, color=&#39;red&#39;, label=&quot;Plurality&quot;)
ax.legend()
# ax.fill_between(np.arange(len(y_1))+1, y_1-error_1, y_1+error_1, color=&#39;green&#39;, alpha=0.3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f33c88d2bf0&gt;
</pre></div>
</div>
<img alt="_images/f5ea17ca0dac5df57610ab768328a55dae77dd059a44e39622a6084a4f2b42e3.png" src="_images/f5ea17ca0dac5df57610ab768328a55dae77dd059a44e39622a6084a4f2b42e3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># plurality_runs[1]
</pre></div>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="mlflow-tooling" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://mlflow.org/">https://mlflow.org/</a></p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="agent-implementation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Agent Implementation</p>
      </div>
    </a>
    <a class="right-next"
       href="indicators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gameplay Indicators</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#filling-buffer">Filling Buffer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantage-estimation">Advantage estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-calculation">Loss calculation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlflow">MLflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#observation-vectorization">Observation Vectorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-time">Training Time</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-results">Training Results</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By George Savin
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>