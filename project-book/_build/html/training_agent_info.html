

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Training Agents &#8212; Voting in Hidden Role Games</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'training_agent_info';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Gameplay Indicators" href="indicators.html" />
    <link rel="prev" title="Agent Implementation" href="agent-implementation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/midjourney-werewolves.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/midjourney-werewolves.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    COMP 5903 - Voting in Hidden Role Games
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="game-description.html">Werewolf - The Game</a></li>
<li class="toctree-l1"><a class="reference internal" href="literature-review.html">Literature Review</a></li>
<li class="toctree-l1"><a class="reference internal" href="game-implementation.html">Werewolf - The Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="agent-implementation.html">Agent Implementation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Training Agents</a></li>
<li class="toctree-l1"><a class="reference internal" href="indicators.html">Gameplay Indicators</a></li>
<li class="toctree-l1"><a class="reference internal" href="game_visualization.html">Replay visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="future.html">Future Directions</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games/issues/new?title=Issue%20on%20page%20%2Ftraining_agent_info.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/training_agent_info.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training Agents</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlflow">MLflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game, convert_obs
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;png&#39;
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="training-agents">
<h1>Training Agents<a class="headerlink" href="#training-agents" title="Permalink to this heading">#</a></h1>
<p>Because we used PPO to train our actor-critic truncated BPTT LSTM, we modified our code to closely resemble the work done in <span id="id1">[<a class="reference internal" href="bibliography.html#id37" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span>.
The steps we follow are very closely related to our <a class="reference internal" href="terminology.html#ppo-alg-header"><span class="std std-ref">PPO Algorithm</span></a>.</p>
<p>We first create a trainer class that stores our config and uses it to set up our buffer, our central villager policy and our optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PPOTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">wolf_policy</span><span class="p">,</span> <span class="n">run_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s2">&quot;run&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="n">voting_type</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</pre></div>
</div>
<p>We then call our train function which sets up the main loop consisting of gatheric metrics periodically, filling up our buffer, calculating our loss and updating our neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_threshold</span><span class="o">=</span><span class="mf">50.0</span><span class="p">):</span>

    <span class="c1"># TODO: put mlflow logging into a callback</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_game&quot;</span><span class="p">][</span><span class="s1">&#39;gameplay&#39;</span><span class="p">])</span>

    <span class="n">loop</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;updates&quot;</span><span class="p">]),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># if the average wins when we do periodic checks of the models scoring is above the save threshold, we save or overwrite the model</span>
    <span class="c1"># model_save_threshold = 50.0</span>
    <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loop</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">tid</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print(f&#39;Playing games with our trained agent after {epid} epochs&#39;)</span>
            <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Playing games and averaging score&quot;</span><span class="p">)</span>
            <span class="n">wins</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">score_gathering</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">replays</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">score_gathering</span><span class="p">:</span>
                <span class="n">game_wins</span><span class="p">,</span> <span class="n">game_replays</span> <span class="o">=</span> <span class="n">play_recurrent_game</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                <span class="n">num_times</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                <span class="n">hidden_state_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">],</span>
                                                <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>
                <span class="n">wins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_wins</span><span class="p">)</span>
                <span class="n">replays</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_replays</span><span class="p">)</span>
                <span class="n">score_gathering</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg wins with current policy : </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># flatten all of our replays and get our current stats</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">aggregate_stats_from_replays</span><span class="p">(</span>
                <span class="p">[</span><span class="n">replay</span> <span class="k">for</span> <span class="n">game_replays</span> <span class="ow">in</span> <span class="n">replays</span> <span class="k">for</span> <span class="n">replay</span> <span class="ow">in</span> <span class="n">game_replays</span><span class="p">],</span> <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">))</span>

            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;avg_wins/100&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">save_threshold</span><span class="p">:</span>
                <span class="n">save_threshold</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">log_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s1">&#39;checkpoint&#39;</span><span class="p">)</span>
                <span class="c1"># torch.save(self.agent.state_dict(), f&#39;{voting_type}_agent_{self.config[&quot;config_game&quot;][&quot;gameplay&quot;][&quot;num_agents&quot;]}_score_{save_threshold}&#39;)</span>


        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Filling buffer&quot;</span><span class="p">)</span>
        <span class="c1"># fill buffer</span>
        <span class="c1"># _scaled_rewards</span>
        <span class="n">buff</span> <span class="o">=</span> <span class="n">fill_recurrent_buffer_scaled_rewards</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">],</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span>
                                        <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>

        <span class="c1"># train info will hold our metrics</span>
        <span class="n">train_info</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO List how many items we are training on</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch Training on </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">games</span><span class="si">}</span><span class="s1"> games&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;epochs&#39;</span><span class="p">]):</span>
            <span class="c1"># run through batches and train network</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">buff</span><span class="o">.</span><span class="n">get_minibatch_generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]):</span>
                <span class="n">train_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_minibatch_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                        <span class="n">batch</span><span class="p">,</span> 
                                                        <span class="n">clip_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;clip_range&#39;</span><span class="p">],</span> 
                                                        <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span> 
                                                        <span class="n">v_loss_coef</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;value_loss_coefficient&#39;</span><span class="p">],</span>
                                                        <span class="n">grad_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;max_grad_norm&#39;</span><span class="p">],</span>
                                                        <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">))</span>

        <span class="n">train_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_info</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;policy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;value loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;total loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;entropy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<p>We calculate the loss per mini-batch, which is also a hypeparameter choice. This works for both approval and plurality models due to the policy outputs being in an array. We return and average out the policy, value, entropy and total losses.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_minibatch_loss</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">samples</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">clip_range</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">v_loss_coef</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">policies</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;observations&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;hxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;cxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()))</span>
    
    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropies</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">policy_head</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">policies</span><span class="p">):</span>
        <span class="c1"># append the log_probs for 1 -&gt; n other agent opinions</span>
        <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;actions&#39;</span><span class="p">][:,</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">entropies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">])</span>

    <span class="c1"># normalize advantages</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="c1"># need to repeat for amount of shape of policies (this way we know how many policy heads we need to watch out for)</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="n">norm_advantage</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">num_votes</span><span class="p">)</span>

    <span class="c1"># policy loss w/ surrogates</span>
    <span class="n">surr1</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">ratio</span>
    <span class="n">surr2</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">clip_range</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">clip_range</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">policy_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Value  function loss</span>
    <span class="n">clipped_values</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="n">clip_range</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">clip_range</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;returns&#39;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">clipped_values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">vf_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Entropy Bonus</span>
    <span class="n">entropy_loss</span> <span class="o">=</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Complete loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">policy_loss</span> <span class="o">-</span> <span class="n">v_loss_coef</span> <span class="o">*</span> <span class="n">vf_loss</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">entropy_loss</span><span class="p">)</span>

    <span class="c1"># Compute gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">grad_norm</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    
    <span class="k">return</span> <span class="p">[</span><span class="n">policy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>     <span class="c1"># policy loss</span>
            <span class="n">vf_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>         <span class="c1"># value loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>            <span class="c1"># total loss</span>
            <span class="n">entropy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>    <span class="c1"># entropy loss</span>
</pre></div>
</div>
<section id="mlflow">
<span id="mlflow-info"></span><h2>MLflow<a class="headerlink" href="#mlflow" title="Permalink to this heading">#</a></h2>
<p>MLflow<a class="footnote-reference brackets" href="#mlflow-tooling" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> is a powerful and popular MLOps open source tool for end-to-end machine learning lifecycle management. We compared this platform with others and simpler alternatives that just forcused on parameter and metric tracking, however due to the adoption of MLflow in industry, we felt it would be a good addition to the project as a sort of tutorial for any other researchers extending this work.</p>
<p>For every training loop, we track all the associated hyperparameters of the experiment, we also track the following metrics:</p>
<ul class="simple">
<li><p>Average villager wins periodically</p></li>
<li><p>Our Policy, Value, Entropy and total loss associated with the PPO algorithm</p></li>
<li><p><a class="reference internal" href="indicators.html#gameplay-indicators-list"><span class="std std-ref">Various indicators</span></a> we try to use to understand the trained agent behaviors</p></li>
</ul>
<p>If the average villager wins passes a threshold, the model dict is stored for future usage, and this new rate is set as the threshold.</p>
</section>
<section id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">#</a></h2>
<p>There are just under 20 hyperparameters, and can be split between gameplay parameters, neural network model parameters and PPO training parameters. Each value in this configuration object below can be changed, and is passed throughout training functions.</p>
<p>Initializations and hyper-parameter choices were greatly influenced by the exhaustive work in <span id="id3">[<a class="reference internal" href="bibliography.html#id34" title="Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in On-Policy reinforcement learning? a Large-Scale empirical study. arXiv preprint arXiv:2006.05990, June 2020. arXiv:2006.05990.">ARStanczyk+20</a>]</span> and <span id="id4">[<a class="reference internal" href="bibliography.html#id37" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span> for the LSTM portion.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config_training</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;recurrent_layers&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;joint_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;split_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;num_votes&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;approval_states&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;training&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
        <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="mi">501</span><span class="p">,</span> 
        <span class="s2">&quot;buffer_games_per_update&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span> 
        <span class="s2">&quot;clip_range&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;value_loss_coefficient&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> 
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> 
        <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span> 
        <span class="s2">&quot;adam_eps&quot;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> 
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
        <span class="s2">&quot;gae_lambda&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> 
    <span class="p">}</span>
<span class="p">}</span>


<span class="n">config_game</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;rewards&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_death&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_win&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;player_loss&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;self_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_villager&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_wolf&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;no_viable_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;no_sleep&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;gameplay&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;accusation_phases&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;num_agents&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;num_werewolves&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;config_game&quot;</span><span class="p">:</span> <span class="n">config_game</span><span class="p">,</span>
    <span class="s2">&quot;config_training&quot;</span><span class="p">:</span> <span class="n">config_training</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="mlflow-tooling" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://mlflow.org/">https://mlflow.org/</a></p>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="agent-implementation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Agent Implementation</p>
      </div>
    </a>
    <a class="right-next"
       href="indicators.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gameplay Indicators</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlflow">MLflow</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters">Hyperparameters</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By George Savin
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>