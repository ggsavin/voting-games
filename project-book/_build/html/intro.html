

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Voting in Hidden Role Games</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="#">
  
  
  
  
    
    
      
    
    
    <img src="_static/midjourney-werewolves.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/midjourney-werewolves.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    COMP 5903 - Learning to vote differently in Werewolf
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">













</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ggsavin/voting-games/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>COMP 5903 - Learning to vote differently in Werewolf</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game-description">Werewolf - The Game</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-literature-review">Literature Review</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game-implementation">Werewolf - The Implementation</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-terminology">Terminology</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-agent-implementation">Agent Implementation</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-training_agent_info">Training Agents</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-indicators">Gameplay Indicators</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game_visualization">Replay visualization</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-plurality-analysis">Plurality Voting</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-approval-analysis">Approval Voting</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-dicsussion">Discussion</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-conclusion-future">Conclusion and Future Directions</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-bibliography">Bibliography</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="comp-5903-learning-to-vote-differently-in-werewolf">
<h1>COMP 5903 - Learning to vote differently in Werewolf<a class="headerlink" href="#comp-5903-learning-to-vote-differently-in-werewolf" title="Permalink to this heading">#</a></h1>
<section id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this heading">#</a></h2>
<p>Social Deduction Games (SDGs), and especially Hidden Role variants such as Werewolf provide players with challenging and dynamic gameplay as teams with asymmetric information try to win. This imbalance of information favors the smaller, deceptive team as the majority try to figure out who the traitors are before it is too late. While player communication and elucidation of traitors has been a main focus of research, the voting mechanism underpinning the games is quite overlooked and is in general computationally challenging in Multi-Agent settings <span id="id1">[<a class="reference internal" href="intro.html#id16" title="A Zorica Dodevska. Computational social choice and challenges of voting in multi-agent systems. Tehnika, 2019.">Dod19</a>]</span>. In this project, we substitute the usual plurality scheme with an approval one in a custom Werewolf environment, and show that it is possible to train approval agents without using communication. We hope this spurs more social choice theory research in SDGs.</p>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Social Deduction Games model tentative cooperation between groups with uncertain motives. In these games, different players and teams/coalitions have access to different information, with their goals ranging from cooperative to atagonistic<span id="id2">[<a class="reference internal" href="intro.html#id20" title="Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo, K. Larson, and Thore Graepel. Open problems in cooperative ai. ArXiv, 2020. URL: https://api.semanticscholar.org/CorpusID:229220772.">DHB+20</a>]</span>. A special type of SDG is a Hidden Role game where the smaller team of players with an information advantage over the majority group also have their roles masked to “blend” into this majority. It is then up to the uniformed majority to build trust amongst themselves while ferreting out the deceptors. Well known games such as Amongst Us, Avalon, Secret Hitler and Mafia fall in this category.</p>
<div class="admonition-mechanisms-employed-in-mafia-werewolf admonition">
<p class="admonition-title">Mechanisms employed in Mafia/Werewolf</p>
<p><span class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary">Hidden Roles</span>, <span class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary">Negotiation</span>, <span class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary">Asymmetric information</span>, <span class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary">Voting</span>, <span class="sd-sphinx-override sd-badge sd-bg-primary sd-bg-text-primary">Elimination</span></p>
</div>
<p>Given this dichotomy and duplicity, SDGs are ripe for psychological analysis, and indeed, mafia was first played in the Psychology Department of Moscow State University<a class="footnote-reference brackets" href="#mafia-wikipedia" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. A gamut of emotions are experienced playing these types of games, from catharsis when identifying deceptors, to perverse joy from lying, cheating and manipulating<a class="footnote-reference brackets" href="#amongst-us-article" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. Other psychological uses of SDGs have been to treat anxiety through symbolic interaction <span id="id5">[<a class="reference internal" href="intro.html#id26" title="Christopher T Conner and Nicholas M Baxter. Are you a werewolf? teaching symbolic interaction theory through game play. Teach. Sociol., 50(1):17–27, January 2022.">CB22</a>]</span>, quantifying entertainment dynamics of SDGs<span id="id6">[<a class="reference internal" href="intro.html#id33" title="Hong Ri, Xiaohan Kang, Mohd Nor Akmal Khalid, and Hiroyuki Iida. The dynamics of minority versus majority behaviors: a case study of the mafia game. Information, 13(3):134, March 2022.">RKKI22</a>]</span> and studying non-verbal communication <span id="id7">[<a class="reference internal" href="intro.html#id32" title="Daisuke Katagami, Shono Takaku, Michimasa Inaba, Hirotaka Osawa, Kosuke Shinoda, Junji Nishino, and Fujio Toriumi. Investigation of the effects of nonverbal information on werewolf. In 2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 982–987. IEEE, 2014.">KTI+14</a>]</span>.</p>
<div class="admonition-what-is-mafia-werewolf admonition">
<p class="admonition-title">What is Mafia/Werewolf?</p>
<p>Mafia is a hidden role game where a team of villagers try to guess who the mafia members are (by voting to eliminating players), while the actual mafia team who are posing as villagers try to eliminate all the villagers. Mafia players have an advantage because they can both vote with the villagers as well as during their own hidden turn. Villagers on the other hand do not know who the mafia members, so they need to communicate and synthesize misleading information to identify mafiosos and win the game.</p>
<p>Werewolf is simply a more recent re-imagining of the mafia game where the mafia members are swapped with werewolves, and the hidden turn is during the night when villagers sleep, and werewolves plot who to kill.</p>
<p>More information can be found in the game <a class="reference internal" href="intro.html#game-desc"><span class="std std-ref">description</span></a>.</p>
</div>
<p>IN SDGs, communication and deception present unique challenges for AI agents trying to learn optimal strategies on top of the already challenging Multi-Agent-RL (MARL) setting <span id="id8">[<a class="reference internal" href="intro.html#id20" title="Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo, K. Larson, and Thore Graepel. Open problems in cooperative ai. ArXiv, 2020. URL: https://api.semanticscholar.org/CorpusID:229220772.">DHB+20</a>]</span>. To better study these topics along with machine learning, multi-agent simulation, human-agent interaction and cognitive science, an yearly competition called AIWolf <span id="id9">[<a class="reference internal" href="intro.html#id11" title="Fujio Toriumi, Hirotaka Osawa, Michimasa Inaba, Daisuke Katagami, Kosuke Shinoda, and Hitoshi Matsubara. Ai wolf contest—development of game ai using collective intelligence—. In Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5, 101–115. Springer, 2017.">TOI+17</a>]</span> was started that is still being held to this day. Werewolf, the game played in this competition is simply a re-imagining of Mafia where the mafia are now werewolves, and villagers vote to eliminate who they believe are werewolves during the day.</p>
<p>Successfully trained agents can do more than play against themselves, and have been shown to do well against human players <span id="id10">[<a class="reference internal" href="intro.html#id8" title="Maria de Fátima Loureiro. A natural language capable agent to play a werewolf or mafia game. Master's thesis, University of Lisbon, 2017. URL: https://api.semanticscholar.org/CorpusID:216077572.">dFatimaL17</a>, <a class="reference internal" href="intro.html#id34" title="Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and foe in multi-agent games. Adv. Neural Inf. Process. Syst., 2019.">SKWPT19</a>]</span>, extending into the field of HCI (Human Computer Interaction).</p>
</section>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading">#</a></h2>
<p>Social Choice Theory<a class="footnote-reference brackets" href="#soc-choice" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> underpins a core mechanism of all SDGs in the gathering and enforcment of choices made by each individual agent. In all the works we have looked through, it seems as though this aspect of SDGs have not been permuted and explored. <em>By changing the underlying voting mechanism, we are interested in seeing how the dynamics between groups change</em></p>
<p>Approval voting has more implicit and explicit information, and allows for more expression when voting. Naturally, we ask ourselves what kind of impact changing the voting mechanism underlying werewolf would have on the ability of the simulated agents to learn and their chances of winning.</p>
</section>
<section id="contribution">
<h2>Contribution<a class="headerlink" href="#contribution" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Creation of a custom environment using PettingZoo<a class="footnote-reference brackets" href="#petting-zoo" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> for the purpose of voting mechanism research within the Werewolf SDG.</p></li>
<li><p>Sucessfully implemented approval voting as the voting mechanism in our environment</p></li>
<li><p>Successfully trained plurality and approval RL agents in our environment</p></li>
<li><p>Designed a wide range of indicators to explore behavior and voting patterns of agents</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="mafia-wikipedia" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Mafia_(party_game)">https://en.wikipedia.org/wiki/Mafia_(party_game)</a></p>
</aside>
<aside class="footnote brackets" id="amongst-us-article" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.sportskeeda.com/among-us/the-psychology-among-us">https://www.sportskeeda.com/among-us/the-psychology-among-us</a></p>
</aside>
<aside class="footnote brackets" id="soc-choice" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Social_choice_theory">https://en.wikipedia.org/wiki/Social_choice_theory</a></p>
</aside>
<aside class="footnote brackets" id="petting-zoo" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">4</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://pettingzoo.farama.org/index.html">https://pettingzoo.farama.org/index.html</a></p>
</aside>
</section>
<div class="toctree-wrapper compound">
<span id="document-game-description"></span><section class="tex2jax_ignore mathjax_ignore" id="werewolf-the-game">
<span id="game-desc"></span><h2>Werewolf - The Game<a class="headerlink" href="#werewolf-the-game" title="Permalink to this heading">#</a></h2>
<section id="quick-history">
<h3>Quick History<a class="headerlink" href="#quick-history" title="Permalink to this heading">#</a></h3>
<p>Werewolf is a 1997 re-imagining by Andrew Plotkin <a class="footnote-reference brackets" href="#werewolf-boardgame" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> of a game known as mafia<a class="footnote-reference brackets" href="#mafia-wikipedia" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, credited to Dimitry Davidoff in the Psychology Department of Moscow State University in 1986.</p>
<p>The game models a conflict between an informed minority (mafiosi) and an unaware majority(villagers).</p>
<p>Originally created for psychological research, it has spread all over the world and is now in the top 50 most historically and culturally significant games published<a class="footnote-reference brackets" href="#top50" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
</section>
<section id="description">
<h3>Description<a class="headerlink" href="#description" title="Permalink to this heading">#</a></h3>
<p>Werewolf is a multi player game between two competing teams comprised of villagers and werewolves. The werewolf team is a hidden subset of the villager team as true villagers cannot tell who the werewolves are, and must try and deduce their identities.</p>
<div class="admonition-team-composition admonition">
<p class="admonition-title">Team Composition</p>
<p>There are a total of <span class="math notranslate nohighlight">\(N\)</span> players, where <span class="math notranslate nohighlight">\(M\)</span> belong to the werewolf team and the remaining <span class="math notranslate nohighlight">\(N-M\)</span> belong to the villager team. At the beginning of the game, each player is given their own identity (werewolf or villager) and every werewolf is also given the identity of all other werewolves.</p>
</div>
<p>Gameplay alternates between day and night phases. During the day, villagers vote on who amongst them should be executed for the crime of being a werewolf, and at night, wolves vote on which helpless villager to kill. Communication between agents either takes place during the voting round, or precedes it in multiple communication rounds. Once a player dies, they reveal their role to the rest of the players.</p>
<div class="admonition-gameplay-loop admonition">
<p class="admonition-title">Gameplay Loop</p>
<p>The way we have implemented Werewolf consists of interations of the following three phases until the game terminates.</p>
<ol class="arabic simple">
<li><p><strong>Accusation Phase</strong>: Villagers and Werewolves make their vote known and broadcast it to all other players. There are <span class="math notranslate nohighlight">\(K\)</span> accusation rounds before the final decision and vote is taken. Because we have removed any explicit communication channel, we allow for this variable length of accusation rounds so agents can focus on all broadcasted votes.</p></li>
<li><p><strong>Vote Phase</strong>: All players vote, and based on the chosen voting mechanism (plurality or approval), a player is selected to be eliminated, and will reveal their role to the rest of the players.</p></li>
<li><p><strong>Night Phase</strong>: Wolves select which player to kill without leaking any information to the rest of the villagers. All villagers see is a null action, but the identity of the dead player is revealed along with their game role.</p></li>
</ol>
<p>A round <span class="math notranslate nohighlight">\(t\)</span> starts in the accusation phase where there are <span class="math notranslate nohighlight">\(N_t = N - 2t\)</span> villagers. <span class="math notranslate nohighlight">\(W_t\)</span> and <span class="math notranslate nohighlight">\(V_t\)</span> denote the numbers of werewolves and villagers at the start of round <span class="math notranslate nohighlight">\(t\)</span> respectively.</p>
<p><strong>Win conditions</strong>:</p>
<ul class="simple">
<li><p>Villagers win when all Werewolves are killed, <span class="math notranslate nohighlight">\(W_t = 0\)</span></p></li>
<li><p>Werewolves win when there are more werewolves than villagers <span class="math notranslate nohighlight">\(W_t \geq V_t\)</span>.</p></li>
</ul>
<p>Whenever <span class="math notranslate nohighlight">\(W_t = V_t\)</span>, villagers need to force a tie and get a lucky break to avoid a loss.</p>
</div>
<p>Different versions of the game slightly alter rules, or introduce various new roles with their own special abilities. In our case, we have simplified the game as much as possible to focus on the voting mechanisms used by agents to determine who to execute and kill.</p>
<p>The game ends when either all the werewolves have been executed, or there are more werewolves than villagers remaining.</p>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="werewolf-boardgame" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://boardgamegeek.com/boardgame/925/werewolf">https://boardgamegeek.com/boardgame/925/werewolf</a></p>
</aside>
<aside class="footnote brackets" id="mafia-wikipedia" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Mafia_(party_game)">https://en.wikipedia.org/wiki/Mafia_(party_game)</a></p>
</aside>
<aside class="footnote brackets" id="top50" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.thesprucecrafts.com/board-and-card-games-timeline-409387">https://www.thesprucecrafts.com/board-and-card-games-timeline-409387</a></p>
</aside>
</section>
</section>
<span id="document-literature-review"></span><section class="tex2jax_ignore mathjax_ignore" id="literature-review">
<h2>Literature Review<a class="headerlink" href="#literature-review" title="Permalink to this heading">#</a></h2>
<p>While the idea of using different voting mechanisms in an SDG seems to be a novel permutation, research involving SDGs is not, and there is a lot of work done in this regard as well as specific research done on the mafia/werewolf SDG.</p>
<section id="game-theorertic-work-on-mafia-werewolf">
<h3>Game theorertic work on mafia/werewolf<a class="headerlink" href="#game-theorertic-work-on-mafia-werewolf" title="Permalink to this heading">#</a></h3>
<p>Early iterative game theoretic research in mafia/werewolf <span id="id1">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>, <a class="reference internal" href="intro.html#id48" title="Piotr Migdał. A mathematical model of the mafia game. arXiv preprint arXiv:1009.1031, 2010.">Migdal10</a>, <a class="reference internal" href="intro.html#id46" title="Erlin Yao. A theoretical study of mafia games. arXiv preprint arXiv:0804.0071, 2008.">Yao08</a>]</span> answered questions such as how large does the minority group (greater than or equal to the square root of total players) need to be to dominate a game and what strategies should they follow (random policy). Although the game model was kept simple, and advanced behavior was not considered, it provided very good baseline insights for which further work could leverage.</p>
</section>
<section id="aiwolf-and-other-werewolf-environments">
<h3>AIWolf and other Werewolf environments<a class="headerlink" href="#aiwolf-and-other-werewolf-environments" title="Permalink to this heading">#</a></h3>
<p>A more complete werewolf environment, AIWolf <span id="id2">[<a class="reference internal" href="intro.html#id11" title="Fujio Toriumi, Hirotaka Osawa, Michimasa Inaba, Daisuke Katagami, Kosuke Shinoda, and Hitoshi Matsubara. Ai wolf contest—development of game ai using collective intelligence—. In Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5, 101–115. Springer, 2017.">TOI+17</a>]</span>, was proposed to study communication in the Werewolf game. AIWolf splits the comeptition in two tracks, one where communication is based off of a protocol, and one where the full english vocabulary can be used. This communication occurs during communication phases, and need to be handled in tandem with choosing appropriate actions.</p>
<p>To train agents, Q-learning has been shown to work <span id="id3">[<a class="reference internal" href="intro.html#id30" title="Makoto Hagiwara, Ahmed Moustafa, and Takayuki Ito. Using q-learning and estimation of role in werewolf game. In Proceedings of the Annual Conference of JSAI 33rd (2019), 2O5E303–2O5E303. The Japanese Society for Artificial Intelligence, 2019.">HMI19</a>, <a class="reference internal" href="intro.html#id23" title="Tianhe Wang and Tomoyuki Kaneko. Application of deep reinforcement learning in werewolf game agents. In 2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI), 28–33. November 2018.">WK18</a>, <a class="reference internal" href="intro.html#id27" title="Shengjing Zhang. Designing aiwolf agents by rule-based algorithm and by deep q-learning. SCSE Student Reports (FYP/IA/PA/PI), 2021.">Zha21</a>]</span>, as it was used in both 5-player and 10+ AIWolf game settings. Some of these <span id="id4">[<a class="reference internal" href="intro.html#id30" title="Makoto Hagiwara, Ahmed Moustafa, and Takayuki Ito. Using q-learning and estimation of role in werewolf game. In Proceedings of the Annual Conference of JSAI 33rd (2019), 2O5E303–2O5E303. The Japanese Society for Artificial Intelligence, 2019.">HMI19</a>, <a class="reference internal" href="intro.html#id23" title="Tianhe Wang and Tomoyuki Kaneko. Application of deep reinforcement learning in werewolf game agents. In 2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI), 28–33. November 2018.">WK18</a>]</span> even used multiple models for various in-game tasks. However, using AIWolf would have required changing a lot of internal server code, and would have been too much for a project, with minimal impact on the actual desired topic of voting mechanisms.</p>
<p>RLupus <span id="id5">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>]</span> is a simplified environment to focus on emergent communication in Werewolf, and we considered extending it, however too much would have had to be changed, and ultimately we went with a focused custom environment.</p>
</section>
<section id="other-sdgs">
<h3>Other SDGs<a class="headerlink" href="#other-sdgs" title="Permalink to this heading">#</a></h3>
<p>Hidden Agenda <span id="id6">[<a class="reference internal" href="intro.html#id19" title="Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha Vezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki, Joel Z. Leibo, and Thore Graepel. Hidden agenda: a social deduction game with diverse learned equilibria. ArXiv, 2022. URL: https://api.semanticscholar.org/CorpusID:245769696.">KDuenezGuzmanM+22</a>]</span> is simplified version of <em>Among us</em> where agents use visual information to synthesize and learn from potentially unreliable information about other agents. Being a 2D environment, visualizing behavior such as camping and chasing were observed, as well as learning to spatially vote (using plurality as the underlying mechanism).</p>
<div class="admonition-descriptions-of-other-mentioned-sdgs admonition">
<p class="admonition-title">Descriptions of other mentioned SDGs</p>
<p><strong>Among us</strong>: Inspired by Werewolf, This video game sees 10 players get placed in an alien spaceship, and must complete tasks all the while being stalked by one or more traitorous players looking to murder everyone else.</p>
<p><strong>Secret Hitler</strong>: A boardgame where players are put onto two teams of “liberals” and “fascists” and compete by enacting policies and completing side-quests related to the Hitler role assigned to a random player. In the same vein as other hidden role games, the liberals are the majority and do not know who the fascists are.</p>
<p><strong>The Resistance</strong>: Another boardgame for 5-10 players inspired by Werewolf, players are assigned to either resistance operators or imperial spies and have to carry out missions against the empire. Unlike Werewolf, there are no eliminations of players however, players vote on who to carry out missions, and then the mission members choose whether or not to fail or pass the mission. Operators win when three missions succeed.</p>
</div>
<p>Other research developped a neural network model based on counterfactual regret minimization to train agents to play The Resistance <span id="id7">[<a class="reference internal" href="intro.html#id34" title="Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and foe in multi-agent games. Adv. Neural Inf. Process. Syst., 2019.">SKWPT19</a>]</span>. Variants of Monte-Carlo Tree Search were used to play Secret Hitler, and showed that it can play as well as simpler rule based agents <span id="id8">[<a class="reference internal" href="intro.html#id35" title="Jackson T Reinhardt. Competing in a complex hidden role game with information set monte carlo tree search. ArXiv, 2020. URL: https://api.semanticscholar.org/CorpusID:218628845.">Rei20</a>]</span>.</p>
</section>
<section id="communication">
<h3>Communication<a class="headerlink" href="#communication" title="Permalink to this heading">#</a></h3>
<p>The nature of duplicitous communication in Werewolf and more generally SDGs, leads to a wide range of research focusing solely on various aspects of it. Determining player roles or itentions based on their communication <span id="id9">[<a class="reference internal" href="intro.html#id49" title="Amos Azaria, Ariella Richardson, and Sarit Kraus. An agent for deception detection in discussion based environments. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing, 218–227. 2015.">ARK15</a>, <a class="reference internal" href="intro.html#id40" title="Markus Eger and Chris Martens. Keeping the story straight: a comparison of commitment strategies for a social deduction game. AIIDE, 14(1):24–30, September 2018.">EM18</a>, <a class="reference internal" href="intro.html#id29" title="Yuya Hirata, Michimasa Inaba, Kenichi Takahashi, Fujio Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda. Werewolf game modeling using action probabilities based on play log analysis. In Computers and Games, 103–114. Springer International Publishing, 2016.">HIT+16</a>, <a class="reference internal" href="intro.html#id47" title="Samee Ibraheem, Gaoyue Zhou, and John DeNero. Putting the con in context: identifying deceptive actors in the game of mafia. 2022. arXiv:2207.02253.">IZD22</a>, <a class="reference internal" href="intro.html#id28" title="Shoma Kato, Tomoya Okumura, Itsuki Toda, Takanori Fukui, Kazunori Iwata, and Nobuhiro Ito. Consideration of the essential topics for role estimation for aiwolf. In 2019 6th International Conference on Computational Science/Intelligence and Applied Informatics (CSII), volume, 72-77. 2019. doi:10.1109/CSII.2019.00020.">KOT+19</a>, <a class="reference internal" href="intro.html#id31" title="Ema Nishizaki and Tomonobu Ozaki. Behavior analysis of executed and attacked players in werewolf game by ilp. In ILP (Short Papers), 48–53. 2016.">NO16</a>, <a class="reference internal" href="intro.html#id22" title="Yingxue Sun and Tomoyuki Kaneko. Prediction of werewolf players by sentiment analysis of game dialogue in japanese. In Proc. of the 26th Game Programming Workshop 2021, 186–191. 2021.">SK21</a>]</span> is a common avenue of research, with interesting variations such as tracking commitment via speech <span id="id10">[<a class="reference internal" href="intro.html#id40" title="Markus Eger and Chris Martens. Keeping the story straight: a comparison of commitment strategies for a social deduction game. AIIDE, 14(1):24–30, September 2018.">EM18</a>]</span> or using sentiment analysis <span id="id11">[<a class="reference internal" href="intro.html#id22" title="Yingxue Sun and Tomoyuki Kaneko. Prediction of werewolf players by sentiment analysis of game dialogue in japanese. In Proc. of the 26th Game Programming Workshop 2021, 186–191. 2021.">SK21</a>]</span>. Explicitly studying impacts of communication in Werewolf was also explored by allowing agents to enhance actions with variable length integer vectors <span id="id12">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>]</span>, and a follow-up in that environment <span id="id13">[<a class="reference internal" href="intro.html#id10" title="Olaf Lipinski, Adam J. Sobey, F. Cerutti, and T. Norman. E mergent p assword s ignalling in the g ame of w erewolf. In EmeCom Workshop, ICLR 2022. 2022. URL: https://api.semanticscholar.org/CorpusID:248983549.">LSCN22</a>]</span> showed that a single word vocabulary was a good enough heuristic. They also explored longer communication rounds, much like we allow multiple accusation rounds. This matches the idea in <span id="id14">[<a class="reference internal" href="intro.html#id28" title="Shoma Kato, Tomoya Okumura, Itsuki Toda, Takanori Fukui, Kazunori Iwata, and Nobuhiro Ito. Consideration of the essential topics for role estimation for aiwolf. In 2019 6th International Conference on Computational Science/Intelligence and Applied Informatics (CSII), volume, 72-77. 2019. doi:10.1109/CSII.2019.00020.">KOT+19</a>]</span> that a single utterance impacts the learning agents the most.
While communication is inherently critical to such games, we need to cut it out to focus solely on the elucidation the implicit vote signaling can provide to agents.</p>
</section>
<section id="voting-in-reinforcement-learning">
<h3>Voting in Reinforcement Learning<a class="headerlink" href="#voting-in-reinforcement-learning" title="Permalink to this heading">#</a></h3>
<p>Most of the voting in Reinforcement Learning (RL) and Multi-Agent Reinforcement Leanring (MARL) research can be split into two categories, the first being voting to select a joint action out of individual agent submissions <span id="id15">[<a class="reference internal" href="intro.html#id24" title="Siqi Chen, Yang Yang, and Ran Su. Deep reinforcement learning with emergent communication for coalitional negotiation games. Math. Biosci. Eng, 19(5):4592–4609, 2022.">CYS22</a>, <a class="reference internal" href="intro.html#id14" title="Natsuki Matsunami, Shun Okuhara, and Takayuki Ito. Agents that learn to vote for a joint action through Multi-Agent reinforcement learning. In 2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI), 832–833. September 2020.">MOI20</a>, <a class="reference internal" href="intro.html#id18" title="Ioannis Partalas, Ioannis Feneris, and Ioannis Vlahavas. Multi-agent reinforcement learning using strategies and voting. In 19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007), volume 2, 318–324. October 2007.">PFV07</a>, <a class="reference internal" href="intro.html#id13" title="Yue Xu, Zengde Deng, Mengdi Wang, Wenjun Xu, Anthony Man-Cho So, and Shuguang Cui. Voting-based multiagent reinforcement learning for intelligent iot. IEEE Internet of Things Journal, 8:2681-2693, 2020. URL: https://api.semanticscholar.org/CorpusID:212954448.">XDW+20</a>]</span>. The usecases are varied, such as Joint Action Learning for IoT device coordination <span id="id16">[<a class="reference internal" href="intro.html#id13" title="Yue Xu, Zengde Deng, Mengdi Wang, Wenjun Xu, Anthony Man-Cho So, and Shuguang Cui. Voting-based multiagent reinforcement learning for intelligent iot. IEEE Internet of Things Journal, 8:2681-2693, 2020. URL: https://api.semanticscholar.org/CorpusID:212954448.">XDW+20</a>]</span> that compete with individual learners, to implenting VCG mechanisms <span id="id17">[<a class="reference internal" href="intro.html#id14" title="Natsuki Matsunami, Shun Okuhara, and Takayuki Ito. Agents that learn to vote for a joint action through Multi-Agent reinforcement learning. In 2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI), 832–833. September 2020.">MOI20</a>]</span> to incentivize truthful behavior in games maximizing total profit. Given that our reward function is essentially their local reward case, it would be an interesting problem to see how a VCG mechanism would impact players in Hidden Role games. A similar idea to VCG mechanisms is the implementation of counterfactuals <span id="id18">[<a class="reference internal" href="intro.html#id3" title="Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32. 2018.">FFA+18</a>]</span> that have been used in hidden role games.</p>
<p>The other research avenue with a focus on voting is the use of RL and ML to intuite and classify voting outcomes <span id="id19">[<a class="reference internal" href="intro.html#id17" title="Stéphane Airiau, Umberto Grandi, and Filipo Studzinski Perotto. Learning agents for iterative voting. In Algorithmic Decision Theory, 139–152. Springer International Publishing, 2017.">AGP17</a>, <a class="reference internal" href="intro.html#id25" title="Dávid Burka, Clemens Puppe, László Szepesváry, and Attila Tasnádi. Voting: a machine learning approach. European Journal of Operational Research, 299(3):1003–1017, 2022.">BPSzepesvaryTasnadi22</a>, <a class="reference internal" href="intro.html#id15" title="Tangui Le Gléau, Xavier Marjou, Tayeb Lemlouma, and Benoit Radier. Multi-agents ultimatum game with reinforcement learning. In Highlights in Practical Applications of Agents, Multi-Agent Systems, and Trust-worthiness. The PAAMS Collection, 267–278. Springer International Publishing, 2020.">LGleauMLR20</a>]</span>. The work in <span id="id20">[<a class="reference internal" href="intro.html#id15" title="Tangui Le Gléau, Xavier Marjou, Tayeb Lemlouma, and Benoit Radier. Multi-agents ultimatum game with reinforcement learning. In Highlights in Practical Applications of Agents, Multi-Agent Systems, and Trust-worthiness. The PAAMS Collection, 267–278. Springer International Publishing, 2020.">LGleauMLR20</a>]</span> also designed novel Neural Nets to have agents learn to output discrete partitions of finite resources. While not directly applicable to the work we are doing, the novel outputting could help when implementing different voting mechanisms in the future.</p>
</section>
</section>
<span id="document-game-implementation"></span><section class="tex2jax_ignore mathjax_ignore" id="werewolf-the-implementation">
<h2>Werewolf - The Implementation<a class="headerlink" href="#werewolf-the-implementation" title="Permalink to this heading">#</a></h2>
<section id="pettingzoo">
<h3>Pettingzoo<a class="headerlink" href="#pettingzoo" title="Permalink to this heading">#</a></h3>
<p><img alt="PettingZoo logo" src="https://pettingzoo.farama.org/_images/pettingzoo-text.png" /></p>
<p>While there are many routes to take when creating a custom environment, using a popular underlying framework makes reproducibility and reusability trivial. For this werewolf game a simple Multi-Agent Reinforcement Learning (MARL) API standard provided by <a class="reference external" href="https://pettingzoo.farama.org/">PettingZoo</a> <span id="id1">[<a class="reference internal" href="intro.html#id2" title="J Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, and others. Pettingzoo: gym for multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 34:15032–15043, 2021.">TBG+21</a>]</span> was chosen. The <a class="reference external" href="https://farama.org/">Farama Foundation</a> which oversees PettingZoo’s development is also in charge of mainting the popular <a class="reference external" href="https://gymnasium.farama.org/">Gym (now Gymnasium)</a> <span id="id2">[<a class="reference internal" href="intro.html#id6" title="Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium. March 2023. URL: https://zenodo.org/record/8127025 (visited on 2023-07-08), doi:10.5281/zenodo.8127026.">TTK+23</a>]</span> RL framework.</p>
</section>
<section id="werewolf">
<h3>Werewolf<a class="headerlink" href="#werewolf" title="Permalink to this heading">#</a></h3>
<section id="game-flow-overview">
<span id="game-flow"></span><h4>Game flow overview<a class="headerlink" href="#game-flow-overview" title="Permalink to this heading">#</a></h4>
<p><img alt="gameplay flowchart" src="_images/werewolf-flowchart.svg" /></p>
</section>
<section id="start-of-the-game">
<h4>Start of the game<a class="headerlink" href="#start-of-the-game" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_agents</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">werewolves</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_accusations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rewards</span><span class="o">=</span><span class="n">REWARDS</span><span class="p">)</span>
</pre></div>
</div>
<p>The initialization signature aboe highlights the <span class="math notranslate nohighlight">\(K\)</span> amount of  accusation phases set to a default value of 1, along with
a custom reward object that <a class="reference internal" href="#game-rewards"><span class="std std-ref">defaults to the following values</span></a>. These values were partially based off of work done in RLupus <span id="id3">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>]</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>square root of the number of agents is the max allowed werewolf count given game theoretic advantages for werewolves <span id="id4">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>]</span></p>
</div>
<p>A permutation of roles (villager or werewolf) based on the initialization values is selected and assigned to the <span class="math notranslate nohighlight">\(N\)</span> agents playing the game.</p>
</section>
<section id="roles-and-phases">
<span id="roles-phases"></span><h4>Roles and Phases<a class="headerlink" href="#roles-and-phases" title="Permalink to this heading">#</a></h4>
<p>We currently implement only two roles, villager or werewolf to simplify the environment for analysis.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Roles</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">VILLAGER</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">WEREWOLF</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Currently, the day consists of <span class="math notranslate nohighlight">\(K\)</span> accusation phases and one voting phase, while the night is its own phase as wolf policies are static.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Phase</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">ACCUSATION</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">VOTING</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">NIGHT</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>During accusastion phases, agents vote as usual, but nobody dies. It is a way to “finger-point” and allow agents to broadcast their intent.</p>
</section>
<section id="action-space">
<span id="env-spaces"></span><h4>Action Space<a class="headerlink" href="#action-space" title="Permalink to this heading">#</a></h4>
<p>For both plurality and approval voting mechanisms, the action space is their vote.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># plurality</span>
<span class="bp">self</span><span class="o">.</span><span class="n">action_spaces</span> <span class="o">=</span> <span class="p">{</span> <span class="n">name</span><span class="p">:</span> <span class="n">Discrete</span><span class="p">(</span><span class="n">num_agents</span><span class="p">)</span>  <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span> <span class="p">}</span>
</pre></div>
</div>
<p>In the case of plurality voting, the action space is an integer between <span class="math notranslate nohighlight">\([0,N]\)</span> where a vote represented by the integer <span class="math notranslate nohighlight">\(N\)</span> represents a null vote. This is because agent IDs start at <span class="math notranslate nohighlight">\(0\)</span> and the vote is simply their ID represented as an integer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># approval</span>
<span class="bp">self</span><span class="o">.</span><span class="n">action_spaces</span> <span class="o">=</span> <span class="p">{</span> <span class="n">name</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span> <span class="p">}</span>
</pre></div>
</div>
<p>In approval voting, the action space is now represented as a multi-discrete integer array of length <span class="math notranslate nohighlight">\(N\)</span>, where elements can have one of three values:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(-1\)</span> : dissaproval</p></li>
<li><p><span class="math notranslate nohighlight">\(0\)</span>: neutral</p></li>
<li><p><span class="math notranslate nohighlight">\(1\)</span> : approval</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For in-game logic and calculation, the <span class="math notranslate nohighlight">\(-1\)</span> acts like a plurality vote, where the index of the <span class="math notranslate nohighlight">\(-1\)</span> is the integer identifier of the taget. Other values are not used to determine voting outcomes.</p>
</div>
</section>
<section id="observation-space">
<h4>Observation Space<a class="headerlink" href="#observation-space" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">observation_spaces</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Dict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;observation&quot;</span><span class="p">:</span> <span class="n">Dict</span><span class="p">({</span>
                <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="n">Discrete</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">num_agents</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="s2">&quot;phase&quot;</span><span class="p">:</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
                <span class="s2">&quot;self_id&quot;</span><span class="p">:</span> <span class="n">Discrete</span><span class="p">(</span><span class="n">num_agents</span><span class="p">),</span> <span class="c1"># TODO: FINISH THIS</span>
                <span class="s2">&quot;player_status&quot;</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span>
                <span class="s2">&quot;roles&quot;</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span> 
                <span class="s2">&quot;votes&quot;</span> <span class="p">:</span> <span class="n">Dict</span><span class="p">({</span>
                    <span class="n">name</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span><span class="p">}),</span> <span class="c1"># approval</span>
                    <span class="n">name</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">num_agents</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,))</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span><span class="p">}),</span> <span class="c1"># plurality</span>
            <span class="p">}),</span>
            <span class="s2">&quot;action_mask&quot;</span><span class="p">:</span> <span class="n">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_agents</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Every environment step, players are provided an observation object and an action mask representing dead or alive players. The player status and action masks are identical, however we have chosen to seperate them like this for more clarity and to conform to more training APIs.</p>
<p>As we have implemented our own PPO loop, we have a seperate <a class="reference internal" href="intro.html#convert-obs"><span class="std std-ref">utility function</span></a> that we use to convert the observations into an input vector for our neural networks.</p>
<p>In the observation object we return:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">day</span></code>: an integer representing the current day, starting at <span class="math notranslate nohighlight">\(1\)</span></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because at the end of a day <span class="math notranslate nohighlight">\(t\)</span> we have <span class="math notranslate nohighlight">\(N_t = N - 2t\)</span> players left, after half the agents die, one of our win conditions will be met, so we can assume the maximum amount of days will be roughly <span class="math notranslate nohighlight">\(N/2\)</span>.</p>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">phase</span></code>: an integer, corresponding to the current <a class="reference internal" href="#roles-phases"><span class="std std-ref">phase</span></a> agents are acting in.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self_id</span></code>: an integer between <span class="math notranslate nohighlight">\(0,N-1\)</span> representing the current agents ID.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">player_status</span></code>: an array where each index corresponds to an agent ID, and the value, either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span> signifies if that agent is alive or dead</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">roles</span></code>: an array where each index corresponds to an agent ID, and the value is represented by our <a class="reference internal" href="#roles-phases"><span class="std std-ref">Roles enum</span></a>. <span class="math notranslate nohighlight">\(0\)</span> represents a villager, <span class="math notranslate nohighlight">\(1\)</span> a werewolf.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As wolves have a role represented by <span class="math notranslate nohighlight">\(1\)</span>, the only time a villager will see an array with anything other than <span class="math notranslate nohighlight">\(0\)</span>’s is when a wolf gets killed and has to reveal their role.</p>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">votes</span></code>: this is a dictionary keyed by a string <code class="docutils literal notranslate"><span class="pre">player_{n}</span></code> where <span class="math notranslate nohighlight">\(n\)</span> is the integer corresponding to the player ID. Each value is the vote of that corresponding player, which is either an integer in the case of plurality voting, or an array in the case of approval voting. These values are derived from the action space of agents.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>During the first accusation phase of a day, villagers are not shown the true votes the wolves have taken overnight. Instead they are shown either all votes being <span class="math notranslate nohighlight">\(N\)</span>, the null vote in the case of plurality voting, or an array of all <span class="math notranslate nohighlight">\(0\)</span> for approval voting, representing neutral feelings.</p>
</div>
</section>
<section id="game-state">
<h4>Game state<a class="headerlink" href="#game-state" title="Permalink to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">world_state</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="n">day</span><span class="p">,</span>
    <span class="s2">&quot;phase&quot;</span><span class="p">:</span> <span class="n">phase</span><span class="p">,</span>
    <span class="s2">&quot;round&quot;</span><span class="p">:</span> <span class="n">accusation_round</span><span class="p">,</span>
    <span class="s2">&quot;alive&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
    <span class="s2">&quot;killed&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;executed&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;werewolves_remaining&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;villagers_remaining&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;votes&quot;</span><span class="p">:</span> <span class="p">{},</span>
    <span class="s2">&quot;winners&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The state object is more comprehensive than the observation an agent receives given that this game is represented by a POMDP. It was also designed to make parsing game history easier and to render on-screen via <code class="docutils literal notranslate"><span class="pre">env.render</span></code> while debugging early on in the project.</p>
<p>This state object tracks the following:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">day</span></code>: an integer representing the current day, starting at <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">phase</span></code>: an integer, corresponding to the current <a class="reference internal" href="#roles-phases"><span class="std std-ref">phase</span></a> agents are acting in.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">round</span></code>: an integer between <span class="math notranslate nohighlight">\([0,K)\)</span> representing the current accusation round out of <span class="math notranslate nohighlight">\(K\)</span> repititions.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>We chose <strong>not</strong> to return this as an observation as it was added later in the project and our results did not change. In fact, it could act as a reason to remove the day value as well, since actions should be tied more strongly to the <code class="docutils literal notranslate"><span class="pre">phase</span></code></p>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">alive</span></code>: an array containing all agents that are still alive</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">killed</span></code>: an array representing agents killed by werewolves</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">executed</span></code>: an array representing agents executed by group consensus</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">werewolves_remaining</span></code>: an array containing the remaining werewolves</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">villagers_remaining</span></code>: an array containing the remaining villagers, excluding werewolves</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">votes</span></code>: this is a dictionary keyed by a string <code class="docutils literal notranslate"><span class="pre">player_{n}</span></code> where <span class="math notranslate nohighlight">\(n\)</span> is the integer corresponding to the player ID. Each value is the vote of that corresponding player, which is either an integer in the case of plurality voting, or an array in the case of approval voting. These values are derived from the action space of agents.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">winners</span></code>: If the game is over, this will either be a <span class="math notranslate nohighlight">\(0\)</span> for villagers, or <span class="math notranslate nohighlight">\(1\)</span> for werewolves. Otherwise, it will be <code class="docutils literal notranslate"><span class="pre">null</span></code>. It is mainly used as a flag for the main game loop in <code class="docutils literal notranslate"><span class="pre">env.step</span></code>.</p></li>
</ul>
</section>
<section id="voting-mechanisms">
<h4>Voting Mechanisms<a class="headerlink" href="#voting-mechanisms" title="Permalink to this heading">#</a></h4>
<p>Eliminating a player requires gathering and determining a group consensus. Each environment (plurality and approval) has their own function to determine who that target consensus is.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_get_player_to_be_killed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">object</span><span class="p">]:</span>
</pre></div>
</div>
<p>This function also returns an information object with flags for each agent if they voted for themselves, a dead player, or if they had a viable vote. It is this object that is used further inform the rewards individual agents receive each step taken in the environment.</p>
<div class="admonition-ties admonition">
<p class="admonition-title">Ties</p>
<p>For both approval and plurality voting, if ties exist, a target is chosen by breaking ties randomly. If however the target with the majority of votes is a dead player, the next highest targetted agent is taken.</p>
</div>
</section>
<section id="rewards">
<span id="game-rewards"></span><h4>Rewards<a class="headerlink" href="#rewards" title="Permalink to this heading">#</a></h4>
<p>Reward shaping in RL (to bias towards certain desired behaviors) is very hard, and can usally lead to unintended results.
The following object is the default reward structure for the game, and it was based on reward structures found in other Werewolf focused papers <span id="id5">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>, <a class="reference internal" href="intro.html#id21" title="Georgi Ventsislavov Velikov. RLereWolf–Reinforcement Learning Agent Development Framework For The Social Deduction Game Werewolf. PhD thesis, University of Aberdeen, 2021.">Vel21</a>]</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">REWARDS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;player_death&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;player_win&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;player_loss&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;dead_wolf&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;dead_villager&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;self_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;dead_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;no_viable_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;no_sleep&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The rewards object is comrpised of:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">day</span></code>: a negative reward trying to incentize agents to complete the game faster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">player_death</span></code>: a negative reward given to the player who was either killed or executed. <em>This could be further seperated based on killing or execution.</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">player_win</span></code>: the reward given to a player who has won the game</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">player_loss</span></code>: the reward given to losing players.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dead_wolf</span></code>: a reward given to villagers who were able to execute a werewolf</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dead_villager</span></code>: a negative reward given to villagers if an actual villager was executed</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">self_vote</span></code>: a negative reward given if a player targets themselves</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dead_vote</span></code>: a negative reward given if a player targets another dead player.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">no_viable_vote</span></code>: An additional penalty if the vote was not a viable one</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">no_sleep</span></code>: This penalty is not currently implemented, but was intended to shape villager learning towards selecting the appropriate <code class="docutils literal notranslate"><span class="pre">null</span></code> action during night votes.</p></li>
</ul>
<div class="admonition-training-rewards admonition">
<p class="admonition-title">Training Rewards</p>
<p>Currently, we only further distribute a fractional win/loss reward to dead players during <span class="xref myst">training</span>, however targetting dead players may be another reward we want to punish with a multiplicative effect proportional to the amount of villagers voting for a dead player.</p>
</div>
</section>
<section id="game-loop">
<h4>Game loop<a class="headerlink" href="#game-loop" title="Permalink to this heading">#</a></h4>
<p>As seen in the <a class="reference internal" href="#game-flow"><span class="std std-ref">flowchart</span></a>, the logic is all implemented in the <code class="docutils literal notranslate"><span class="pre">env.step()</span></code> function which expects a dictionary of agent ids and their respective actions.</p>
<p>For every phase, the target is determined via <code class="docutils literal notranslate"><span class="pre">self._get_player_to_be_killed(actions)</span></code>. If this was a voting or night-time phase, the target is added to executed (voting) or killed (night-time) lists, and this dying agent is given a negative reward for dying.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>During the night-time, we still allow actions from villagers, however these are completely ignored.</p>
</div>
<p>We then check for winners and reward players accordingly, before appending the history and possibly incrementing the game round, phase and day.</p>
<p>Finally the remainder of rewards based on agent voting information is distributed to the agents that provided actions during this current environment step.</p>
</section>
</section>
</section>
<span id="document-terminology"></span><section class="tex2jax_ignore mathjax_ignore" id="terminology">
<h2>Terminology<a class="headerlink" href="#terminology" title="Permalink to this heading">#</a></h2>
<section id="pomdps">
<h3>POMDPs<a class="headerlink" href="#pomdps" title="Permalink to this heading">#</a></h3>
<p>Partially Observable Markov Decision Processes (POMDPs<a class="footnote-reference brackets" href="#id14" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>) are a special case of a Markov Decision Process (MDP) where the agent does not have direct observability to the state, but rather gets their own, possibly unique, observation. Formally, it is a tuple consisting of:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span>, a set of states</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span>, a set of actions</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span>, a set of transition probabilities between states</p></li>
<li><p><span class="math notranslate nohighlight">\(R: S \times A \rightarrow \mathbb{R}\)</span> is the reward function</p></li>
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span>, a set of observations</p></li>
<li><p><span class="math notranslate nohighlight">\(O\)</span>, a set of observation probabilities</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> is a discount factor, bounded by <span class="math notranslate nohighlight">\([0,1)\)</span></p></li>
</ul>
<p>Given a current state <span class="math notranslate nohighlight">\(s \in S\)</span>, an agent will take an action <span class="math notranslate nohighlight">\(a \in A\)</span> based on some observation <span class="math notranslate nohighlight">\(o \in O\)</span> and will transition to a new state <span class="math notranslate nohighlight">\(s'\)</span> with probability <span class="math notranslate nohighlight">\(T(s'|s,a)\)</span> and receive a reward <span class="math notranslate nohighlight">\(r = R(s,a)\)</span></p>
<p>RL and MARL enviornments usually satisfy the Markov property that future states depend only on current state and action pairs, and thus can be formulated as MDPs.</p>
</section>
<section id="rl-marl">
<h3>RL &amp; MARL<a class="headerlink" href="#rl-marl" title="Permalink to this heading">#</a></h3>
<p><a class="reference internal" href="#single-rl"><span class="std std-ref">Reinforcement Learning</span></a> is a training method wherein an agent takes an action in an environment and is either rewarded or punished while transitioning to a new state. This is repeated until the agent reaches a terminal state. Ideally an agent will learn an optimal <em>policy</em> that maps observations to actions through many interactions in their environment.</p>
<p>When it comes to <a class="reference internal" href="#multi-agent-rl"><span class="std std-ref">Multi-Agent Learning</span></a>, multiple agents are either learning in a decentralized manner, or being directed by a central policy. The dynamic nature of of Multi-Agent systems make learning much more challenging, although certain algorithms such as PPO and DQN still work.</p>
<p>Agents can learn using value-based, policy-based and model-based algorithms. A good taxonomy of different types of algorithms can be seen in the diagram below.</p>
<figure class="align-default" id="rl-taxonomy">
<img alt="Taxonomy of RL algorithms" src="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" /><figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Taxonomy of RL Algorithms<a class="footnote-reference brackets" href="#rl-tax" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></span><a class="headerlink" href="#rl-taxonomy" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A good reference on RL is the timeless book <em>Reinforcement Learning: An Introduction</em><a class="footnote-reference brackets" href="#sutton-barto-book" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> by Sutton &amp; Barto.</p>
<figure class="align-default" id="single-rl">
<img alt="single agent RL" src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ews7HaMiSn2l8r70eeIszQ.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Single Agent Reinforcement Learning<a class="footnote-reference brackets" href="#rl-pictures" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></span><a class="headerlink" href="#single-rl" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="multi-agent-rl">
<img alt="multi agent RL" src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*1o1oeH3vpzsfJukLbFsekw.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Multi-Agent Reinforcement Learning<a class="footnote-reference brackets" href="#rl-pictures" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></span><a class="headerlink" href="#multi-agent-rl" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="ppo">
<span id="ppo-alg-header"></span><h3>PPO<a class="headerlink" href="#ppo" title="Permalink to this heading">#</a></h3>
<p>Proxmial Policy Optimization (PPO) was chosen because it stil seems to be the most widely used on-policy algorithm and it was also used in a similar setting as our work <span id="id6">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>, <a class="reference internal" href="intro.html#id14" title="Natsuki Matsunami, Shun Okuhara, and Takayuki Ito. Agents that learn to vote for a joint action through Multi-Agent reinforcement learning. In 2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI), 832–833. September 2020.">MOI20</a>]</span>.</p>
<p>Because we are also implementing an LSTM, the action <span class="math notranslate nohighlight">\(a_t\)</span> selected by the policy <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> depends on both the observation <span class="math notranslate nohighlight">\(o_t\)</span> and the hidden state <span class="math notranslate nohighlight">\(h_t\)</span>.</p>
<p>We implemented our own following some works using a truncated BPTT <span id="id7">[<a class="reference internal" href="intro.html#id12" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=jHc8dCx6DDr.">PPZP23</a>]</span> and CLeanRL, however relying on a framework might have been a better choice.</p>
<div class="proof algorithm admonition" id="ppo-alg">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Proximal Policy Optimization w/ Clipped Surrogate)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Initial policy parameters <span class="math notranslate nohighlight">\(\theta_0\)</span>, clipping threshold <span class="math notranslate nohighlight">\(\epsilon\)</span></p>
<ol class="arabic">
<li><p>for <span class="math notranslate nohighlight">\(i=0,1,2,3,...\)</span> do</p>
<ol class="arabic">
<li><p>for <span class="math notranslate nohighlight">\(agent=1,2,...,N\)</span> do</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="intro.html#fill-buffer"><span class="std std-ref">Collect set of full game trajectories</span></a> <span class="math notranslate nohighlight">\(\tau\)</span> on policy <span class="math notranslate nohighlight">\(\pi(\theta_i)\)</span>.
<strong>We store the hidden state of the recurrent layer <span class="math notranslate nohighlight">\(h\)</span> at each step of each trajectory</strong></p></li>
<li><p><a class="reference internal" href="intro.html#estimate-adv"><span class="std std-ref">Estimate advatanges</span></a> <span class="math notranslate nohighlight">\(\hat{A}^{\pi_i}_t\)</span> using GAE <span id="id8">[<a class="reference internal" href="intro.html#id38" title="John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.">SML+15</a>]</span></p></li>
</ol>
</li>
<li><p><a class="reference internal" href="intro.html#minibatch-loss"><span class="std std-ref">Compute Policy Update</span></a> using clipped surrogate <span id="id9">[<a class="reference internal" href="intro.html#id37" title="John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.">SWD+17</a>]</span></p>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}^C(\theta)\)</span> is our clipped surrogate objective function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
                \mathcal{L}^C(\theta) &amp; = E [\sum_{t=0}^{\tau}[min(r_t(\theta)\hat{A}_t, clip(r_t(\theta), 1-\epsilon, 1+ \epsilon)A_t)]] \\
                r_t(\theta) &amp; = \frac{\pi_{\theta}(a_t | o_t, h_t)}{\pi_{\theta-old}(a_t | o_t, h_t)}
            \end{split}\]</div>
</li>
<li><p>Squared error loss optimization for the value function</p>
<div class="math notranslate nohighlight">
\[
                \mathcal{L}^V_t(\theta) = (V_{\theta}(o_t,h_t) - V^{target}_t)^2
            \]</div>
</li>
<li><p>Entropy bonus to promote exploitation</p>
<div class="math notranslate nohighlight">
\[
                \mathcal{H}[\pi_{\theta}](o_t, h_t)
            \]</div>
</li>
<li><p>Putting it all together with scaling coefficients <span class="math notranslate nohighlight">\(c_1, c_2\)</span> for the value loss and entropy bonus respectively</p>
<div class="math notranslate nohighlight">
\[
                L^{CVH}_t(\theta) = E [\mathcal{L}^C(\theta) - c_1\cdot \mathcal{L}^V_t(\theta) + c_2 \mathcal{H}[\pi_{\theta}](o_t, h_t)]
            \]</div>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The 37 implementation details of PPO<a class="footnote-reference brackets" href="#details-ppo" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> is a great blog post detailing many more intricacies of PPO implementations</p>
</div>
</section>
<section id="lstms">
<h3>LSTMs<a class="headerlink" href="#lstms" title="Permalink to this heading">#</a></h3>
<p>Long-short Term Memory (LSTM) networks are a type of deep neural network that is tailored for sequential data. They are an evolution of Recurrent Neural Networks (RNNs) that have been designed to deal with backpropagation issues and can handle much longer sequences <span id="id11">[<a class="reference internal" href="intro.html#id7" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">HS97</a>]</span>.</p>
<p>These types of networks have loops in them, allowing information  to persist. In the below <a class="reference internal" href="#unrolled-rnn"><span class="std std-ref">figure</span></a>, we can see what an unrolled RNN looks like, and in following <a class="reference internal" href="#lstm-internal-cell"><span class="std std-ref">figure</span></a>, we see the LSTM,along with its inner workings and different gates that the cells are composed of.</p>
<p>For our PPO Implementation, we store the hidden state <span class="math notranslate nohighlight">\(h_n\)</span> and cell state <span class="math notranslate nohighlight">\(c_n\)</span> from our LSTM output for each state/action pair taken by an agent so we can use it again the next time we want to call our model or calculate losses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A great resource to understanding LSTMs and RNNs is <a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">this famous blogpost</a> by Christopher Olah.</p>
</div>
<figure class="align-default" id="unrolled-rnn">
<img alt="unrolled RNN" src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" />
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">An unrolled RNN highlights it’s sequential nature<a class="footnote-reference brackets" href="#understand-lstm" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></span><a class="headerlink" href="#unrolled-rnn" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="lstm-internal-cell">
<img alt="lstm cell" src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">LSTM Cells and their internals<a class="footnote-reference brackets" href="#understand-lstm" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></span><a class="headerlink" href="#lstm-internal-cell" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="id14" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process</a></p>
</aside>
<aside class="footnote brackets" id="rl-tax" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html</a></p>
</aside>
<aside class="footnote brackets" id="sutton-barto-book" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></p>
</aside>
<aside class="footnote brackets" id="rl-pictures" role="note">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id4">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p><a class="reference external" href="https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b">https://towardsdatascience.com/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b</a></p>
</aside>
<aside class="footnote brackets" id="details-ppo" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">5</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/</a></p>
</aside>
<aside class="footnote brackets" id="understand-lstm" role="note">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id12">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p><a class="reference external" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
</aside>
</section>
</section>
<span id="document-agent-implementation"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
from tqdm import tqdm
from collections import Counter
from torchview import draw_graph
import graphviz
graphviz.set_jupyter_format(&#39;png&#39;)
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game, convert_obs
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
import notebooks.learning_agents.stats as indicators 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="agent-implementation">
<h2>Agent Implementation<a class="headerlink" href="#agent-implementation" title="Permalink to this heading">#</a></h2>
<p>We have implemented hand-crafted agents for both villager and werewolf roles, along with actor-critic models for villagers that were trained against hand-crafted werewolves. We based our static policies on the work done in past games implementing werewolf <span id="id1">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>, <a class="reference internal" href="intro.html#id10" title="Olaf Lipinski, Adam J. Sobey, F. Cerutti, and T. Norman. E mergent p assword s ignalling in the g ame of w erewolf. In EmeCom Workshop, ICLR 2022. 2022. URL: https://api.semanticscholar.org/CorpusID:248983549.">LSCN22</a>]</span>. This was to have good baselines to compare our trained villagers with, and a reliable wolf strategy to train our villagers against. Below are the strategies and their associated names we use to reference in analaysis.</p>
<section id="villagers">
<h3>Villagers<a class="headerlink" href="#villagers" title="Permalink to this heading">#</a></h3>
<p>There are three types of random villager policies we tested. The fully random, the one randomly targetting living players, and the coordinated random target (which was the strategy mentioned in <span id="id2">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>]</span>).</p>
<section id="random">
<h4>Random<a class="headerlink" href="#random" title="Permalink to this heading">#</a></h4>
<p>The truly random agent simply samples from the <a class="reference internal" href="intro.html#env-spaces"><span class="std std-ref">action space</span></a>.</p>
<div class="highlight-python notranslate" id="r-villagers"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_agent_action</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
   <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="l-targets">
<span id="l-villagers"></span><h4>L-Targets<a class="headerlink" href="#l-targets" title="Permalink to this heading">#</a></h4>
<p>L-Targets stands for Living-Targets, and while the villagers select random agents, they only select them from agents that are still alive, without targetting the agent itself.</p>
<p>More specifically, during each phase, each villager <span class="math notranslate nohighlight">\(v_i\)</span> picks a random target out of remaining players at round <span class="math notranslate nohighlight">\(t\)</span> excluding themselves <span class="math notranslate nohighlight">\(N_t \backslash \{v_i\}\)</span>. Although agents act simultanously, the program loops sequentially through agents, so if agents are coordinating via an action parameter as in <a class="reference internal" href="#cl-villagers"><span class="std std-ref">CL-Targets</span></a>, then the first agents choice gets used by all subsequent villagers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_approval_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;alive&quot;</span><span class="p">])</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>
    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">)</span>
    <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">action</span>

<span class="k">def</span> <span class="nf">random_plurality_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;alive&quot;</span><span class="p">])</span> <span class="o">-</span> <span class="nb">set</span><span class="p">([</span><span class="n">agent</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="cl-targets">
<span id="cl-villagers"></span><h4>CL-Targets<a class="headerlink" href="#cl-targets" title="Permalink to this heading">#</a></h4>
<p>CL-Targets stands for Coordinated-Living Targets, and it builds off of <a class="reference internal" href="#l-villagers"><span class="std std-ref">L-Targets</span></a>. The coordination is not realistic, but we do it to achieve a similar result as in <span id="id3">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>]</span>. The coordination is done via the passed in <code class="docutils literal notranslate"><span class="pre">action</span></code> function parameter which gets set by the first agent to act. This has no realistic analogy in the actual game because villagers have no way to exclude werewolves from their planning, but we needed it to force coordination.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_coordinated_approval_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">action</span> <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span> <span class="k">else</span> \
        <span class="n">random_approval_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">random_coordinated_plurality_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">action</span> <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span> <span class="k">else</span> \
        <span class="n">random_plurality_villager</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="trained-wolf-strategy-here">
<span id="trained-villagers"></span><h4>Trained-<a class="reference internal" href="#wolf-strategies"><span class="std std-ref">wolf-strategy-here</span></a><a class="headerlink" href="#trained-wolf-strategy-here" title="Permalink to this heading">#</a></h4>
<p>Trained agents are hyphenated with the wolf strategy they were trained against. For this project, we only trained our agents against <a class="reference internal" href="#crwolves"><span class="std std-ref">CRWolves</span></a>, so in the analysis, you</p>
</section>
<section id="learning-agents">
<h4>Learning Agents<a class="headerlink" href="#learning-agents" title="Permalink to this heading">#</a></h4>
<section id="model-architecture">
<h5>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this heading">#</a></h5>
<p>Determining the proper model architecture for Agents is not a straightforward task, and takes some experimentation. Initially we had gone with seperate networks for value and policy models, however, we settled on a shared network for both, inspired by a truncated BPTT implementation with PPO <span id="id4">[<a class="reference internal" href="intro.html#id12" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=jHc8dCx6DDr.">PPZP23</a>]</span>. We treat agents as independent learners with a shared policy.</p>
<p>Models are  tied to the number of agents per game they were trained on, so a 5-player game environment agent will not work if fed observations from a 10-player game environment.</p>
<p>Most of the training and work was done in 10-player environments as it allowed for more interesting gameplay in a setting where no communication other than voting is allowed.</p>
<p>Below is our model architecture that is shared amongst plurality and approval agents and has trained villagers. The input tensors are the previous hidden states of the LSTM, and the flattened observations the agent receives from the environment.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()

obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

untrained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128, 
                                        &quot;rec_layers&quot;: 1,
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)

obs = convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;], voting_type=&#39;plurality&#39;)
obs = torch.unsqueeze(obs,0)
hxcs = (torch.zeros((1,1,128), dtype=torch.float32), torch.zeros((1,1,128), dtype=torch.float32))

model_graph = draw_graph(untrained_plurality_agent, input_data=[obs,hxcs], device=torch.device(&#39;cpu&#39;))
model_graph.visual_graph
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/0227968ac2f23bf101c7f964c326316ae275c935a5df3862bd49978592a7ff0e.png" src="_images/0227968ac2f23bf101c7f964c326316ae275c935a5df3862bd49978592a7ff0e.png" />
</div>
</div>
</section>
</section>
</section>
<section id="werewolves">
<span id="wolf-strategies"></span><h3>Werewolves<a class="headerlink" href="#werewolves" title="Permalink to this heading">#</a></h3>
<p>Werewolf behavior for both plurality and approval environments used hand-crafted policies because the focus was on villager voting and training.</p>
<p>These various wolf behaviors are reasonable and mimic other logical agents employed in AIWolf <span id="id5">[<a class="reference internal" href="intro.html#id11" title="Fujio Toriumi, Hirotaka Osawa, Michimasa Inaba, Daisuke Katagami, Kosuke Shinoda, and Hitoshi Matsubara. Ai wolf contest—development of game ai using collective intelligence—. In Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5, 101–115. Springer, 2017.">TOI+17</a>]</span> and RLupus <span id="id6">[<a class="reference internal" href="intro.html#id9" title="Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. ArXiv, 2021.">BGI21</a>]</span>.</p>
<p>Below are the wolf strategies we used in our analysis.</p>
<section id="rwolves">
<span id="id7"></span><h4>RWolves<a class="headerlink" href="#rwolves" title="Permalink to this heading">#</a></h4>
<p>This is the truly random wolf strategy, and each werewolf samples independently from the <a class="reference internal" href="intro.html#env-spaces"><span class="std std-ref">action space</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_agent_action</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
   <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="crwolves">
<span id="id8"></span><h4>CRWolves<a class="headerlink" href="#crwolves" title="Permalink to this heading">#</a></h4>
<p>CRWolves stands for Coordinated Random Wolves, and it was the policy we used to train our learning agents against. Much like the <a class="reference internal" href="#l-villagers"><span class="std std-ref">L-Target</span></a> and <a class="reference internal" href="#cl-villagers"><span class="std std-ref">CL-Target</span></a> villager policies, the first wolf’s action is used by subsequent wolves. They do not target any other wolves, and in the approval environment, they also like all wolves.</p>
<p>During the day, the wolves maintain their action throughout the accusation and voting phases, and choose new targets at night.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_plurality_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">action</span> <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">random_approval_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="c1"># pick a living target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span>

    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">)</span>
    <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">curr_wolf</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">:</span>
        <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">curr_wolf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</section>
<section id="revwolves">
<span id="id9"></span><h4>RevWolves<a class="headerlink" href="#revwolves" title="Permalink to this heading">#</a></h4>
<p>RevWolves refer to Revenge Wolves, where each wolf tries to target a villager that has targetted them in a previous round. If they were not targetted, they pick a random villager that is still alive.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">revenge_plurality_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="n">prev_votes</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span>
    <span class="n">villagers_targetting_you</span> <span class="o">=</span> <span class="p">[</span><span class="n">player</span> <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="n">prev_votes</span> <span class="k">if</span> <span class="sa">f</span><span class="s1">&#39;player_</span><span class="si">{</span><span class="n">prev_votes</span><span class="p">[</span><span class="n">player</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span> <span class="o">==</span> <span class="n">agent</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">villagers_targetting_you</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_targetting_you</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">revenge_approval_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="n">prev_votes</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span>
    <span class="n">agent_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">villagers_targetting_you</span> <span class="o">=</span> <span class="p">[</span><span class="n">player</span> <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">prev_votes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">prev_votes</span><span class="p">[</span><span class="n">player</span><span class="p">][</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">villagers_targetting_you</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">revenge_target_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_targetting_you</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">revenge_target_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">)</span>
    <span class="n">action</span><span class="p">[</span><span class="n">revenge_target_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">curr_wolf</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">:</span>
        <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">curr_wolf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
<section id="crevwolves">
<span id="id10"></span><h5>CRevWolves<a class="headerlink" href="#crevwolves" title="Permalink to this heading">#</a></h5>
<p>Much like CRWolves, CRevWolves simply coordinate the revenge across all werewolves, based on the first wolf to act and vote. The only difference being that the first wolf considers votes against all the wolves and not just themselves.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">coordinated_revenge_plurality_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="n">prev_votes</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span>
    <span class="n">villagers_targetting_wolves</span> <span class="o">=</span> <span class="p">[</span><span class="n">player</span> <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="n">prev_votes</span> <span class="k">if</span> <span class="sa">f</span><span class="s1">&#39;player_</span><span class="si">{</span><span class="n">prev_votes</span><span class="p">[</span><span class="n">player</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">villagers_targetting_wolves</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_targetting_wolves</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">coordinated_revenge_approval_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">action</span>
    
    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="n">prev_votes</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span>
    <span class="n">wolf_ids</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">]</span>
    <span class="n">villagers_targetting_wolves</span> <span class="o">=</span> <span class="p">[</span><span class="n">player</span> <span class="k">for</span> <span class="n">player</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">prev_votes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> 
                                   <span class="nb">sum</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wolf_ids</span> <span class="k">if</span> <span class="n">prev_votes</span><span class="p">[</span><span class="n">player</span><span class="p">][</span><span class="n">w</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">villagers_targetting_wolves</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">revenge_target_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_targetting_wolves</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">revenge_target_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">)</span>
    <span class="n">action</span><span class="p">[</span><span class="n">revenge_target_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">curr_wolf</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">:</span>
        <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">curr_wolf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</section>
</section>
<section id="crlwolves">
<span id="id11"></span><h4>CRLWolves<a class="headerlink" href="#crlwolves" title="Permalink to this heading">#</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Approval Environment only strategy</p>
</div>
<p>This strategy extends the CRWolves (which agents were trained against), with the additional caveat that for non-targets, each wolf randomizes their neutrals and likes to hopefully confuse trained agents that have learned to synthesize the like and neutral choices of other agents.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_likes_approval_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">action</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># just randomly assign a like or a neutral</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">action</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="n">villagers_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>

    <span class="c1"># pick a living target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">villagers_remaining</span><span class="p">))</span>

    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">))]</span>
    <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</section>
<section id="aggrowolves">
<span id="id12"></span><h4>AggroWolves<a class="headerlink" href="#aggrowolves" title="Permalink to this heading">#</a></h4>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Approval Environment only strategy</p>
</div>
<p>Aggressive wolves simply target every agent that is not a wolf.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">aggressive_approval_wolf</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">wolves_remaining</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;alive&#39;</span><span class="p">])</span>
    <span class="n">action</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">possible_agents</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">curr_wolf</span> <span class="ow">in</span> <span class="n">wolves_remaining</span><span class="p">:</span>
        <span class="n">action</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">curr_wolf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</section>
</section>
</section>
<span id="document-training_agent_info"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
import time
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game, convert_obs
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
import mlflow
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="training-agents">
<h2>Training Agents<a class="headerlink" href="#training-agents" title="Permalink to this heading">#</a></h2>
<p>Because we used PPO to train our actor-critic truncated BPTT LSTM, we modified our code to closely resemble the work done in <span id="id1">[<a class="reference internal" href="intro.html#id39" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span>.
The steps we follow are very closely related to our <a class="reference internal" href="intro.html#ppo-alg-header"><span class="std std-ref">PPO Algorithm</span></a>.</p>
<p>We first create a trainer class that stores our config and uses it to set up our buffer, our central villager policy and our optimizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PPOTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">wolf_policy</span><span class="p">,</span> <span class="n">run_id</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s2">&quot;run&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="n">voting_type</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</pre></div>
</div>
<p>We then call our train function which sets up the main loop consisting of gatheric metrics periodically, filling up our buffer, calculating our loss and updating our neural network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_threshold</span><span class="o">=</span><span class="mf">50.0</span><span class="p">):</span>

    <span class="c1"># TODO: put mlflow logging into a callback</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">])</span>
    <span class="n">mlflow</span><span class="o">.</span><span class="n">log_params</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_game&quot;</span><span class="p">][</span><span class="s1">&#39;gameplay&#39;</span><span class="p">])</span>

    <span class="n">loop</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;updates&quot;</span><span class="p">]),</span> <span class="n">position</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># if the average wins when we do periodic checks of the models scoring is above the save threshold, we save or overwrite the model</span>
    <span class="c1"># model_save_threshold = 50.0</span>
    <span class="k">for</span> <span class="n">tid</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loop</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">tid</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># print(f&#39;Playing games with our trained agent after {epid} epochs&#39;)</span>
            <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Playing games and averaging score&quot;</span><span class="p">)</span>
            <span class="n">wins</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">score_gathering</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">position</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">replays</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">score_gathering</span><span class="p">:</span>
                <span class="n">game_wins</span><span class="p">,</span> <span class="n">game_replays</span> <span class="o">=</span> <span class="n">play_recurrent_game</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                <span class="n">num_times</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                <span class="n">hidden_state_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">][</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">],</span>
                                                <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>
                <span class="n">wins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_wins</span><span class="p">)</span>
                <span class="n">replays</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">game_replays</span><span class="p">)</span>
                <span class="n">score_gathering</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Avg wins with current policy : </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="c1"># flatten all of our replays and get our current stats</span>
            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metrics</span><span class="p">(</span><span class="n">aggregate_stats_from_replays</span><span class="p">(</span>
                <span class="p">[</span><span class="n">replay</span> <span class="k">for</span> <span class="n">game_replays</span> <span class="ow">in</span> <span class="n">replays</span> <span class="k">for</span> <span class="n">replay</span> <span class="ow">in</span> <span class="n">game_replays</span><span class="p">],</span> <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">))</span>

            <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;avg_wins/100&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">save_threshold</span><span class="p">:</span>
                <span class="n">save_threshold</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">wins</span><span class="p">))</span>
                <span class="n">mlflow</span><span class="o">.</span><span class="n">pytorch</span><span class="o">.</span><span class="n">log_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">artifact_path</span><span class="o">=</span><span class="s1">&#39;checkpoint&#39;</span><span class="p">)</span>
                <span class="c1"># torch.save(self.agent.state_dict(), f&#39;{voting_type}_agent_{self.config[&quot;config_game&quot;][&quot;gameplay&quot;][&quot;num_agents&quot;]}_score_{save_threshold}&#39;)</span>


        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Filling buffer&quot;</span><span class="p">)</span>
        <span class="c1"># fill buffer</span>
        <span class="c1"># _scaled_rewards</span>
        <span class="n">buff</span> <span class="o">=</span> <span class="n">fill_recurrent_buffer_scaled_rewards</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;config_training&quot;</span><span class="p">],</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">wolf_policy</span><span class="p">,</span> 
                                        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span>
                                        <span class="n">voting_type</span><span class="o">=</span><span class="n">voting_type</span><span class="p">)</span>

        <span class="c1"># train info will hold our metrics</span>
        <span class="n">train_info</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># TODO List how many items we are training on</span>
        <span class="n">loop</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch Training on </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">games</span><span class="si">}</span><span class="s1"> games&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;epochs&#39;</span><span class="p">]):</span>
            <span class="c1"># run through batches and train network</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">buff</span><span class="o">.</span><span class="n">get_minibatch_generator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]):</span>
                <span class="n">train_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calc_minibatch_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> 
                                                        <span class="n">batch</span><span class="p">,</span> 
                                                        <span class="n">clip_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;clip_range&#39;</span><span class="p">],</span> 
                                                        <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span> 
                                                        <span class="n">v_loss_coef</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;value_loss_coefficient&#39;</span><span class="p">],</span>
                                                        <span class="n">grad_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;config_training&#39;</span><span class="p">][</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s1">&#39;max_grad_norm&#39;</span><span class="p">],</span>
                                                        <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">))</span>

        <span class="n">train_stats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_info</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;policy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;value loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;total loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">mlflow</span><span class="o">.</span><span class="n">log_metric</span><span class="p">(</span><span class="s2">&quot;entropy loss&quot;</span><span class="p">,</span> <span class="n">train_stats</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
<section id="filling-buffer">
<span id="fill-buffer"></span><h3>Filling Buffer<a class="headerlink" href="#filling-buffer" title="Permalink to this heading">#</a></h3>
<p>To fill our buffer, we collect each players full game trajectories <span class="math notranslate nohighlight">\(\tau\)</span>, with special attention given to the reward distributed to dead agents, rewarding agents that died earlier slightly less. This is a heuristic we developed to reward all the agents, however, we do have a fill function where this reward is not distributed to dead players.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward_at_max_round</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">**</span><span class="p">(</span><span class="n">max_game_rounds</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">fill_recurrent_buffer_scaled_rewards</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span><span class="nb">dict</span><span class="p">,</span> <span class="n">wolf_policy</span><span class="p">,</span> <span class="n">villager_agent</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="n">buffer</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;gamma&quot;</span><span class="p">],</span> <span class="n">gae_lambda</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;gae_lambda&quot;</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">][</span><span class="s2">&quot;buffer_games_per_update&quot;</span><span class="p">]):</span>
        <span class="c1">## Play the game </span>
        <span class="n">next_observations</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminations</span><span class="p">,</span> <span class="n">truncations</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="c1"># init recurrent stuff for actor and critic to 0 as well</span>
        <span class="n">magent_obs</span> <span class="o">=</span> <span class="p">{</span><span class="n">agent</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;obs&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;actions&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;logprobs&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;values&#39;</span><span class="p">:</span> <span class="p">[],</span> 
                              <span class="s1">&#39;terms&#39;</span><span class="p">:</span> <span class="p">[],</span>

                              <span class="c1"># obs size, and 1,1,64 as we pass batch first</span>
                              <span class="s1">&#39;hcxs&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> 
                                        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))]</span>
                    <span class="p">}</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">agent_roles</span><span class="p">[</span><span class="n">agent</span><span class="p">]}</span>
        
        <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">while</span> <span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">:</span>
            <span class="n">observations</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">next_observations</span><span class="p">)</span>
            <span class="n">actions</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">villagers</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;villagers&quot;</span><span class="p">])</span>
            <span class="n">wolves</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">agents</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s2">&quot;werewolves&quot;</span><span class="p">])</span>

             <span class="c1">## VILLAGER LOGIC ##</span>
            <span class="n">v_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">convert_obs</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;observation&#39;</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">])</span>

            <span class="c1"># TODO: maybe this can be sped up? </span>
            <span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[(</span><span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span><span class="p">)</span> <span class="k">for</span> <span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span> <span class="ow">in</span> <span class="p">[</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;hcxs&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">]])</span>
            <span class="n">hxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hxs</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">cxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cxs</span><span class="p">),</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">policies</span><span class="p">,</span> <span class="n">value</span> <span class="p">,</span> <span class="n">cells</span> <span class="o">=</span> <span class="n">villager_agent</span><span class="p">(</span><span class="n">v_obs</span><span class="p">,</span> <span class="p">(</span><span class="n">hxs</span><span class="p">,</span> <span class="n">cxs</span><span class="p">))</span>
            <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">policies</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">hxs_new</span><span class="p">,</span> <span class="n">cxs_new</span> <span class="o">=</span> <span class="n">cells</span>
            <span class="n">hxs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">hxs_new</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">cxs_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">cxs_new</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">villager</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">villagers</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
                    <span class="n">actions</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
                    <span class="n">actions</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;hcxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">hxs_new</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">cxs_new</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)))</span>


                <span class="c1"># TODO : Update these somehow</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;obs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">v_obs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="mi">0</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
                    <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
                <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
                    <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># how do we get these</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;logprobs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">policy</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="k">for</span> <span class="n">policy</span><span class="p">,</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policies</span><span class="p">,</span> <span class="n">v_actions</span><span class="p">[</span><span class="n">i</span><span class="p">])],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;values&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>

            <span class="c1"># wolf steps</span>
            <span class="n">phase</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">wolf</span> <span class="ow">in</span> <span class="n">wolves</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="n">wolf_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">wolf</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="n">wolf_action</span><span class="p">)</span>
                <span class="n">actions</span><span class="p">[</span><span class="n">wolf</span><span class="p">]</span> <span class="o">=</span> <span class="n">wolf_action</span>

            <span class="c1"># actions = actions | wolf_policy(env)</span>
        
            <span class="n">next_observations</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">terminations</span><span class="p">,</span> <span class="n">truncations</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">:</span>
                <span class="c1"># dividing rewards by 100</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;rewards&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">villager</span><span class="p">]</span><span class="o">/</span><span class="mf">100.0</span><span class="p">)</span>
                <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s2">&quot;terms&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">terminations</span><span class="p">[</span><span class="n">villager</span><span class="p">])</span>

            <span class="c1"># update wolf_action appropriately</span>
            <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">NIGHT</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>
            
            <span class="k">if</span> <span class="n">env</span><span class="o">.</span><span class="n">world_state</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">ACCUSATION</span> <span class="ow">and</span> <span class="n">phase</span> <span class="o">==</span> <span class="n">Phase</span><span class="o">.</span><span class="n">NIGHT</span><span class="p">:</span>
                <span class="n">wolf_action</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1">## Update the end_game rewards for villagers that died before the end</span>
        <span class="n">max_game_rounds</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
        <span class="n">a_villager_who_made_it_to_end</span> <span class="o">=</span> <span class="p">[</span><span class="n">villager</span> <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])</span> <span class="o">==</span> <span class="n">max_game_rounds</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">reward_at_max_round</span> <span class="o">=</span> <span class="n">magent_obs</span><span class="p">[</span><span class="n">a_villager_who_made_it_to_end</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">villager</span> <span class="ow">in</span> <span class="n">villagers</span><span class="p">:</span>
            <span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward_at_max_round</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.9</span><span class="o">**</span><span class="p">(</span><span class="n">max_game_rounds</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">villager</span><span class="p">][</span><span class="s1">&#39;rewards&#39;</span><span class="p">])))</span>

        <span class="c1">## Fill bigger buffer, keeping in mind sequence</span>
        <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">magent_obs</span><span class="p">:</span>
            <span class="n">buffer</span><span class="o">.</span><span class="n">add_replay</span><span class="p">(</span><span class="n">magent_obs</span><span class="p">[</span><span class="n">agent</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">buffer</span>
</pre></div>
</div>
</section>
<section id="advantage-estimation">
<span id="estimate-adv"></span><h3>Advantage estimation<a class="headerlink" href="#advantage-estimation" title="Permalink to this heading">#</a></h3>
<p>As mentioned in our <span class="xref myst">PPO algorithm</span>, we use generalized advantage estimates <span id="id2">[<a class="reference internal" href="intro.html#id38" title="John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.">SML+15</a>]</span>, and apply it to every single completed trajectory as it gets appended to the buffer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">_calculate_advantages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">game</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generalized advantage estimation (GAE)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">advantages</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]))):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">][</span><span class="nb">max</span><span class="p">((</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]),</span><span class="n">t</span><span class="p">)]</span> <span class="o">-</span> <span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">][</span><span class="n">t</span><span class="p">]</span>
            <span class="n">advantages</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gae_lambda</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">[</span><span class="nb">max</span><span class="p">((</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;rewards&#39;</span><span class="p">]),</span><span class="n">t</span><span class="p">)]</span>

    <span class="c1"># adv and returns</span>
    <span class="k">return</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">advantages</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">game</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="loss-calculation">
<span id="minibatch-loss"></span><h3>Loss calculation<a class="headerlink" href="#loss-calculation" title="Permalink to this heading">#</a></h3>
<p>We calculate the loss per mini-batch, which is also a hypeparameter choice. This works for both approval and plurality models due to the policy outputs being in an array. We return and average out the policy, value, entropy and total losses.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calc_minibatch_loss</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">samples</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">clip_range</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">v_loss_coef</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">policies</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;observations&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;hxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;cxs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()))</span>
    
    <span class="n">log_probs</span><span class="p">,</span> <span class="n">entropies</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">policy_head</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">policies</span><span class="p">):</span>
        <span class="c1"># append the log_probs for 1 -&gt; n other agent opinions</span>
        <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s1">&#39;actions&#39;</span><span class="p">][:,</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy_head</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">entropies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probs</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;logprobs&#39;</span><span class="p">])</span>

    <span class="c1"># normalize advantages</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;advantages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="c1"># need to repeat for amount of shape of policies (this way we know how many policy heads we need to watch out for)</span>
    <span class="n">norm_advantage</span> <span class="o">=</span> <span class="n">norm_advantage</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">num_votes</span><span class="p">)</span>

    <span class="c1"># policy loss w/ surrogates</span>
    <span class="n">surr1</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">ratio</span>
    <span class="n">surr2</span> <span class="o">=</span> <span class="n">norm_advantage</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">clip_range</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">clip_range</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">policy_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Value  function loss</span>
    <span class="n">clipped_values</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;values&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="n">clip_range</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">clip_range</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="n">values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s1">&#39;returns&#39;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">clipped_values</span> <span class="o">-</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;returns&quot;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">vf_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Entropy Bonus</span>
    <span class="n">entropy_loss</span> <span class="o">=</span> <span class="n">entropies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Complete loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">policy_loss</span> <span class="o">-</span> <span class="n">v_loss_coef</span> <span class="o">*</span> <span class="n">vf_loss</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">entropy_loss</span><span class="p">)</span>

    <span class="c1"># Compute gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">grad_norm</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    
    <span class="k">return</span> <span class="p">[</span><span class="n">policy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>     <span class="c1"># policy loss</span>
            <span class="n">vf_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>         <span class="c1"># value loss</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>            <span class="c1"># total loss</span>
            <span class="n">entropy_loss</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>    <span class="c1"># entropy loss</span>
</pre></div>
</div>
</section>
<section id="mlflow">
<span id="mlflow-info"></span><h3>MLflow<a class="headerlink" href="#mlflow" title="Permalink to this heading">#</a></h3>
<p>MLflow<a class="footnote-reference brackets" href="#mlflow-tooling" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> is a powerful and popular MLOps open source tool for end-to-end machine learning lifecycle management. We compared this platform with others and simpler alternatives that just forcused on parameter and metric tracking, however due to the adoption of MLflow in industry, we felt it would be a good addition to the project as a sort of tutorial for any other researchers extending this work.</p>
<p>For every training loop, we track all the associated hyperparameters of the experiment, we also track the following metrics:</p>
<ul class="simple">
<li><p>Average villager wins periodically</p></li>
<li><p>Our Policy, Value, Entropy and total loss associated with the PPO algorithm</p></li>
<li><p><a class="reference internal" href="intro.html#gameplay-indicators-list"><span class="std std-ref">Various indicators</span></a> we try to use to understand the trained agent behaviors</p></li>
</ul>
<p>If the average villager wins passes a threshold, the model dict is stored for future usage, and this new rate is set as the threshold.</p>
</section>
<section id="hyperparameters">
<h3>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this heading">#</a></h3>
<p>There are just under 20 hyperparameters, and can be split between gameplay parameters, neural network model parameters and PPO training parameters. Each value in this configuration object below can be changed, and is passed throughout training functions.</p>
<p>Initializations and hyper-parameter choices were greatly influenced by the exhaustive work in <span id="id4">[<a class="reference internal" href="intro.html#id36" title="Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in On-Policy reinforcement learning? a Large-Scale empirical study. arXiv preprint arXiv:2006.05990, June 2020. arXiv:2006.05990.">ARStanczyk+20</a>]</span> and <span id="id5">[<a class="reference internal" href="intro.html#id39" title="Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In The Eleventh International Conference on Learning Representations. 2022.">PPZP22</a>]</span> for the LSTM portion.</p>
<p>The following parameters are the ones we permuted the most</p>
<ul class="simple">
<li><p>Model</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">recurrent_hidden_size</span></code>: This refers to the LSTM hidden units</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">joint_mlp_size</span></code>: This is size of the first fully connected layer after the LSTM</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">split_mlp_size</span></code>: This is the size of the individual fully connected layers after the joint layer. One is for the policy, the other for the critic.</p></li>
</ul>
</li>
<li><p>Training</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: Number of samples we process before an update</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">epochs</span></code>: Number of times we go through all our samples</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">updates</span></code>: Number of times we fill our buffer and repeat epoch training</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buffer_games_per_update</span></code>: How many werewolf games we play per update</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>: Tegulating our networks weights based on our loss gradient</p></li>
</ul>
</li>
<li><p>Game:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">accusation_phases</span></code>: How many times agents accuse one another before a final vote phase</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config_training</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;recurrent_layers&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;recurrent_hidden_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;joint_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;split_mlp_size&quot;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;num_votes&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;approval_states&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;training&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> 
        <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
        <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="mi">501</span><span class="p">,</span> 
        <span class="s2">&quot;buffer_games_per_update&quot;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span> 
        <span class="s2">&quot;clip_range&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;value_loss_coefficient&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="s2">&quot;max_grad_norm&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> 
        <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> 
        <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span> 
        <span class="s2">&quot;adam_eps&quot;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> 
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
        <span class="s2">&quot;gae_lambda&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> 
    <span class="p">}</span>
<span class="p">}</span>


<span class="n">config_game</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;rewards&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;day&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_death&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;player_win&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;player_loss&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;self_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_villager&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;dead_wolf&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s2">&quot;no_viable_vote&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;no_sleep&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;gameplay&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;accusation_phases&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;num_agents&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;num_werewolves&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;config_game&quot;</span><span class="p">:</span> <span class="n">config_game</span><span class="p">,</span>
    <span class="s2">&quot;config_training&quot;</span><span class="p">:</span> <span class="n">config_training</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="observation-vectorization">
<span id="convert-obs"></span><h3>Observation Vectorization<a class="headerlink" href="#observation-vectorization" title="Permalink to this heading">#</a></h3>
<p>While not a hyperparameter, the way we choose to vectorize our observations before inputting them into our neural network can have a big impact on overall training. We have a util function <code class="docutils literal notranslate"><span class="pre">convert_obs</span></code> which flattens the observation object into one 1D vector, or can also one-hot encode everything that is not already represented by <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(0\)</span>. While training, we had more success when not using the one-hot encoding, but further experimentation with this vectorization and representation could be a fruitful avenue for future work.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_obs</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">voting_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    observation will have the following information</span>
<span class="sd">        day (int)</span>
<span class="sd">        phase (int) </span>
<span class="sd">        self_id (int)</span>
<span class="sd">        player_status (list) - 0/1 for alive or dead</span>
<span class="sd">        roles (list) - 0/1 for villager or werewolf</span>
<span class="sd">        votes (dict) - dict with player and associated vote</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;votes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s2">&quot;player_status&quot;</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">()</span>
    
    <span class="c1"># phase length</span>
    <span class="n">day</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">]])</span>

    <span class="c1"># we can make the phase a one hot, hardcoded 3 phases</span>
    <span class="k">if</span> <span class="n">one_hot</span><span class="p">:</span>
        <span class="n">phase</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">self_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;self_id&#39;</span><span class="p">]),</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span> <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">])</span><span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;approval&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">voting_type</span> <span class="o">==</span> <span class="s2">&quot;plurality&quot;</span><span class="p">:</span>
            <span class="n">votes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

        <span class="n">phase</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;phase&#39;</span><span class="p">]])</span>
        <span class="n">self_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;self_id&#39;</span><span class="p">]])</span>


    <span class="c1"># PLAYER STATUS (ALIVE OR DEAD)</span>
    <span class="n">player_status</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;player_status&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">player_roles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">[</span><span class="s1">&#39;roles&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">day</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">self_id</span><span class="p">,</span> <span class="n">player_status</span><span class="p">,</span> <span class="n">player_roles</span><span class="p">,</span> <span class="n">votes</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="training-time">
<span id="id6"></span><h3>Training Time<a class="headerlink" href="#training-time" title="Permalink to this heading">#</a></h3>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>p_env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = p_env.reset()
obs_size= p_env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)

start = time.time()
play_recurrent_game(p_env, random_plurality_wolf, plurality_agent, num_times=1000, hidden_state_size=128, voting_type=&quot;plurality&quot;)
end = time.time()
plurality_time = end - start


a_env = pare(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = a_env.reset()
obs_size= a_env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

approval_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 256,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 10,
                                        &quot;approval_states&quot;: 3},
                                        num_players=10,
                                        obs_size=obs_size)
start = time.time()
play_recurrent_game(a_env, random_approval_wolf, approval_agent, num_times=1000, hidden_state_size=256, voting_type=&quot;approval&quot;)
end = time.time()
approval_time = end - start

print(f&#39;It took {plurality_time:.2f} s to play 1000 plurality games&#39;)
print(f&#39;It took {approval_time:.2f} s to play 1000 approval games&#39;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>It took 20.80 s to play 1000 plurality games
It took 46.75 s to play 1000 approval games
</pre></div>
</div>
</div>
</div>
<p>We can see the amount of time taken to play 1000 games is about 20 seconds for plurality and roughly double for approval.</p>
<p>When it comes to training agents, we fill a buffer with 200 games, over  500 updates. This by itself is roughly 1 hour. Time is also spent during each epoch (of which we have 5) calculating loss, which requires passing through the model with all our data. This adds roughly another 4 hours to the run as training loops that have gone to completion with these parameters have taken roughly 5 hours.</p>
<p>We fill buffers and collect avg/win rates seperately. In the spirit of saving time, we could have just used the results from filling the buffer to get win-rate updates at every update step.</p>
</section>
<section id="training-results">
<h3>Training Results<a class="headerlink" href="#training-results" title="Permalink to this heading">#</a></h3>
<p>Below are the configurations of Plurality and Approval trained agents. We made the decision to stick with an LSTM hidden size of 128 for plurality agents due to the flattened input vector being quite a bit smaller than that of the approval agent. This assumed no one-hot encoding, as we found in our limited testing time that it did not have much of an impact.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># code to calculate mean and std dev for varying length arrays
def tolerant_mean(arrs):
    lens = [len(i) for i in arrs]
    arr = np.ma.empty((np.max(lens),len(arrs)))
    arr.mask = True
    for idx, l in enumerate(arrs):
        arr[:len(l),idx] = l
    return arr.mean(axis = -1), arr.std(axis=-1)
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mlflow_client = mlflow.tracking.MlflowClient(tracking_uri=&quot;http://mlflow:5000&quot;)

## PLURALITY 
plurality_experiment = mlflow_client.get_experiment_by_name(&quot;Plurality Training&quot;)
plurality_runs = mlflow_client.search_runs([plurality_experiment.experiment_id])

four_accusations_p = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;4_accusations&#39;&quot;)
three_accusations_p = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;3_accusations&#39;&quot;)
two_accusations_p = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;2_accusations&#39;&quot;)
one_accusations_p = mlflow_client.search_runs([plurality_experiment.experiment_id], filter_string=&quot;run_name = &#39;1_accusations&#39;&quot;)

four_accusation_p_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in four_accusations_p]
three_accusation_p_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in three_accusations_p]
two_accusation_p_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in two_accusations_p]
one_accusation_p_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in one_accusations_p]

### APPROVAL

approval_experiment = mlflow_client.get_experiment_by_name(&quot;Approval Training&quot;)
approval_runs = mlflow_client.search_runs([approval_experiment.experiment_id])

four_accusations_a = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;4_accusations&#39;&quot;)
three_accusations_a = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;3_accusations&#39;&quot;)
two_accusations_a = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;2_accusations&#39;&quot;)
one_accusations_a = mlflow_client.search_runs([approval_experiment.experiment_id], filter_string=&quot;run_name = &#39;1_accusations&#39;&quot;)

four_accusation_a_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in four_accusations_a]
three_accusation_a_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in three_accusations_a]
two_accusation_a_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in two_accusations_a]
one_accusation_a_votes = \
    [[item.value for item in mlflow_client.get_metric_history(run_id=accusation.info.run_id, key=&quot;avg_wins/100&quot;)] for accusation in one_accusations_a]

print(&quot;Done gathering mlflow win rate metrics&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done gathering mlflow win rate metrics
</pre></div>
</div>
</div>
</div>
<section id="plurality-training">
<h4>Plurality Training<a class="headerlink" href="#plurality-training" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()

tolerant_p_results = [tolerant_mean(a_votes) for a_votes in [one_accusation_p_votes, two_accusation_p_votes, three_accusation_p_votes, four_accusation_p_votes]]

for i, tolerant_result in enumerate(tolerant_p_results):
    y, std = tolerant_result
    ax.plot(np.arange(len(y))+1, y, label=i+1)
    ax.fill_between(np.arange(len(y))+1, y-std, y+std, alpha=0.1)

plt.ylabel(&quot;Win Percentage&quot;)
plt.xlabel(&quot;Collection Step&quot;)
plt.legend(title=&quot;Number of Accusation Phase(s)&quot;)
plt.suptitle(&quot;Win-Rates while Training Plurality Agents&quot;)
print(&quot;\n&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="_images/f3ca7021558e8dc6e13bf83a74c056718cc33437a31522965a17c572e6998398.png" src="_images/f3ca7021558e8dc6e13bf83a74c056718cc33437a31522965a17c572e6998398.png" />
</div>
</div>
<p>We can see that regardless of one or many accusation phases, there is no definitive trend as to which performs the best. We had an outlier with a win-rate above <span class="math notranslate nohighlight">\(40\%\)</span> with <span class="math notranslate nohighlight">\(2\)</span> accusation phases, however just based on the graph, <span class="math notranslate nohighlight">\(4\)</span> accusation phases scored higher on average. Given what we know in our <a class="reference internal" href="intro.html#p-analysis"><span class="std std-ref">plurality analysis</span></a>, it might be that more accusation phases are needed for agents that can only cast a vote for a single candidate. More time is needed to learn complex strategies for signaling intent.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>four_accusation_p_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in four_accusations_p]
three_accusation_p_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in three_accusations_p]
two_accusation_p_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in two_accusations_p]
one_accusation_p_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in one_accusations_p]

plt.boxplot([one_accusation_p_times, two_accusation_p_times, three_accusation_p_times, four_accusation_p_times],
            vert=True,  # vertical box alignment
            patch_artist=True,  # fill with color
            labels=[1,2,3,4])
plt.ylabel(&quot;Minutes&quot;)
plt.xlabel(&quot;Number of Accusation Phase(s)&quot;)
plt.suptitle(&quot;Time elapsed for PPO agent training loops&quot;)
    
print(&quot;\n&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="_images/75753e17925e086aa40dc40a73e109874f804ce12c79c067a3981bcb953b6b0b.png" src="_images/75753e17925e086aa40dc40a73e109874f804ce12c79c067a3981bcb953b6b0b.png" />
</div>
</div>
<p>Stability of training was quite poor for plurality voting, regardless of number of accusation phases.</p>
</section>
<section id="approval-training">
<h4>Approval Training<a class="headerlink" href="#approval-training" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()

tolerant_a_results = [tolerant_mean(a_votes) for a_votes in [one_accusation_a_votes, two_accusation_a_votes, three_accusation_a_votes, four_accusation_a_votes]]

for i, tolerant_result in enumerate(tolerant_a_results):
    y, std = tolerant_result
    ax.plot(np.arange(len(y))+1, y, label=i+1)
    ax.fill_between(np.arange(len(y))+1, y-std, y+std, alpha=0.1)

plt.ylabel(&quot;Win Percentage&quot;)
plt.xlabel(&quot;Collection Step&quot;)
plt.legend(title=&quot;Number of Accusation Phase(s)&quot;)
plt.suptitle(&quot;Win-Rates while Training Approval Agents&quot;)
print(&quot;\n&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="_images/292a22b8bd62ebed6e659c92176dd058bc90ce676ee5c7af4fe41461f6e6c2dd.png" src="_images/292a22b8bd62ebed6e659c92176dd058bc90ce676ee5c7af4fe41461f6e6c2dd.png" />
</div>
</div>
<p>We can see that regardless of number of accusation phases, it seems as though for the first half, agents learn roughly at the same rate, probably learning the same behavior. Afterwards, learning becomes more temperamental. A lot of runs did not make it this far either, so some data we have is only from a single run. Each accusation phase was able to reach about <span class="math notranslate nohighlight">\(.50\)</span> win-rates, and given the lack of data, we cannot definitively say one was better than the other. The analysis was done with 2 accusation phases as some of our saved agents initially came from this batch.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>four_accusation_a_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in four_accusations_a]
three_accusation_a_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in three_accusations_a]
two_accusation_a_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in two_accusations_a]
one_accusation_a_times = [(accusation.info.end_time - accusation.info.start_time)/(1000 * 60) for accusation in one_accusations_a]

plt.boxplot([one_accusation_a_times, two_accusation_a_times, three_accusation_a_times, four_accusation_a_times],
            vert=True,  # vertical box alignment
            patch_artist=True,  # fill with color
            labels=[1,2,3,4])
plt.ylabel(&quot;Minutes&quot;)
plt.xlabel(&quot;Number of Accusation Phase(s)&quot;)
plt.suptitle(&quot;Time elapsed for PPO agent training loops&quot;)
    
print(&quot;\n&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="_images/3ce3f6e034db1f353ff039ad393a791adbbdbc76a46f3fa83a17135655c8702b.png" src="_images/3ce3f6e034db1f353ff039ad393a791adbbdbc76a46f3fa83a17135655c8702b.png" />
</div>
</div>
<p>We can also see that the more accusation phases we added, the less stable the training became. This was another reason we stuck with <span class="math notranslate nohighlight">\(2\)</span> accusation phases. For the hyperparameters we had settled on, it seemed the most stable.</p>
</section>
<section id="comparing-approval-and-plurality">
<h4>Comparing Approval and Plurality<a class="headerlink" href="#comparing-approval-and-plurality" title="Permalink to this heading">#</a></h4>
<p>We can see that when comparing approval and purality training, an approval voting mechanism provided much better win-rates, and a faster and more consistent learning experience. The increased expressibility provided by approval voting gave learning agents a more consistent experience and way to signal.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots()

tolerant_results = [tolerant_mean(a_votes) for a_votes in [one_accusation_a_votes, two_accusation_a_votes, three_accusation_a_votes, four_accusation_a_votes]]


y_p, std_p = tolerant_p_results[1]
y_a, std_a = tolerant_a_results[1]

ax.plot(np.arange(len(y_p))+1, y_p, label=&quot;Plurality&quot;)
# ax.fill_between(np.arange(len(y_p))+1, y_p-std_p, y_p+std_p, alpha=0.1)

ax.plot(np.arange(len(y_a))+1, y_a, label=&quot;Approval&quot;)
# ax.fill_between(np.arange(len(y_a))+1, y_a-std_a, y_a+std_a, alpha=0.1)

plt.ylabel(&quot;Win Percentage&quot;)
plt.xlabel(&quot;Collection Step&quot;)
plt.legend(title=&quot;Voting Mechanism&quot;)
plt.suptitle(&quot;Win-Rates while Training Approval Agents&quot;)
print(&quot;\n&quot;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="_images/8914f1ae56ae20a4a58245d37802f6ae7a53696199f18ffd628ed603a0929fd3.png" src="_images/8914f1ae56ae20a4a58245d37802f6ae7a53696199f18ffd628ed603a0929fd3.png" />
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="mlflow-tooling" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://mlflow.org/">https://mlflow.org/</a></p>
</aside>
</section>
</section>
</section>
<span id="document-indicators"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
import notebooks.learning_agents.stats as indicators 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="gameplay-indicators">
<span id="gameplay-indicators-list"></span><h2>Gameplay Indicators<a class="headerlink" href="#gameplay-indicators" title="Permalink to this heading">#</a></h2>
<p>Indicators gauge certain trends and facts, and can indicate the level of something such as learned behavior or skill. Basic indicators such as win-rates, total/average score and neural network loss are almost ubiquitous to RL papers due to their simplicity and interpretability. A reader knows immediately how the agent is doing just by looking at them. In Werewolf, we consider win-rate as our most basic indicator.</p>
<p>To better undestand our learned agents behavior, we want more in-depth and descriptive indicators, that may or may not generalize well to other problems. We can categorize these as:</p>
<ul class="simple">
<li><p>Win-rate adjacent : time-to-win and time between wolf executions</p></li>
<li><p>Tie Indicators : when and what did agents do when tie’s occured</p></li>
<li><p>Targetting Indicators: Novel heuristics we designed to try and identify the reasoning behind how agents target others via their votes.</p></li>
</ul>
<p>Each indicator has a code snippet associated with it, so anyone extending or replicating this project can use them as examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The way the environment stores history is slightly different than observations. Whereas the latter stores the prior votes, env.history steps have the votes and the outcomes that occured at that particular day/phase/round.</p>
</div>
<section id="load-up-data">
<h3>Load up data<a class="headerlink" href="#load-up-data" title="Permalink to this heading">#</a></h3>
<p>We are going to use replays from our trained agents to investigate these various markers. 1000 games of each voting type will be used.</p>
<section id="plurality-agent">
<h4>Plurality Agent<a class="headerlink" href="#plurality-agent" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()

obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

untrained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128, 
                                        &quot;rec_layers&quot;: 1,
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)

trained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)
trained_plurality_agent.load_state_dict(torch.load(&quot;stored_agents/lstm_first_no_one_hot_128_128/plurality_agent_10_score_46&quot;))

# random_agent = None

trained_plurality_wins, trained_plurality_replays = play_recurrent_game(env, random_plurality_wolf, trained_plurality_agent, num_times=1000, hidden_state_size=128, voting_type=&quot;plurality&quot;)
untrained_plurality_wins, untrained_plurality_replays = play_recurrent_game(env, random_plurality_wolf, untrained_plurality_agent, num_times=1000, hidden_state_size=128, voting_type=&quot;plurality&quot;)
# random_wins, random_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, random_agent, num_times=1000, hidden_state_size=128, voting_type=&quot;plurality&quot;)

trained_plurality_villager_wins = [r for r in trained_plurality_replays if r[-1][&quot;winners&quot;] == Roles.VILLAGER]
print(f&#39;Trained villagers won {trained_plurality_wins} games&#39;)
untrained_plurality_villager_wins = [r for r in untrained_plurality_replays if r[-1][&quot;winners&quot;] == Roles.VILLAGER]
print(f&#39;Untrained villagers won {untrained_plurality_wins} games&#39;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trained villagers won 486 games
Untrained villagers won 47 games
</pre></div>
</div>
</div>
</div>
</section>
<section id="approval-agent">
<h4>Approval Agent<a class="headerlink" href="#approval-agent" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = pare(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()

obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]
observations[&#39;player_0&#39;][&#39;observation&#39;]

untrained_approval_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 256, 
                                        &quot;rec_layers&quot;: 1,
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 10,
                                        &quot;approval_states&quot;: 3},
                                        num_players=10,
                                        obs_size=obs_size)

trained_approval_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 256,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 10,
                                        &quot;approval_states&quot;: 3},
                                        num_players=10,
                                        obs_size=obs_size)
trained_approval_agent.load_state_dict(torch.load(&quot;stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_49&quot;))

# random_agent = None

trained_approval_wins, trained_approval_replays = play_recurrent_game(env, random_approval_wolf, trained_approval_agent, num_times=1000, hidden_state_size=256, voting_type=&quot;approval&quot;)
untrained_approval_wins, untrained_approval_replays = play_recurrent_game(env, random_approval_wolf, untrained_approval_agent, num_times=1000, hidden_state_size=256, voting_type=&quot;approval&quot;)
# random_wins, random_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, random_agent, num_times=1000, hidden_state_size=128, voting_type=&quot;plurality&quot;)

trained_approval_villager_wins = [r for r in trained_approval_replays if r[-1][&quot;winners&quot;] == Roles.VILLAGER]
print(f&#39;Trained villagers won {trained_approval_wins} games&#39;)
untrained_approval_villager_wins = [r for r in untrained_approval_replays if r[-1][&quot;winners&quot;] == Roles.VILLAGER]
print(f&#39;Untrained villagers won {untrained_approval_wins} games&#39;)
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trained villagers won 507 games
Untrained villagers won 62 games
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="behavioral-indicators">
<h3>Behavioral Indicators<a class="headerlink" href="#behavioral-indicators" title="Permalink to this heading">#</a></h3>
<section id="days-elapsed-before-a-villager-win">
<h4>Days elapsed before a villager win<a class="headerlink" href="#days-elapsed-before-a-villager-win" title="Permalink to this heading">#</a></h4>
<p>Looking at the average amount of days elapsed before villagers win is a metric that highlights positive learning and collaboration trends</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;Average amount of days until a win is achieved by villagers in plurality games&quot;)
print(f&#39;\t Trained villagers : {np.mean([villager_win[-1][&quot;day&quot;] for villager_win in trained_plurality_villager_wins]):2f}&#39;)
print(f&#39;\t Untrained villagers : {np.mean([villager_win[-1][&quot;day&quot;] for villager_win in untrained_plurality_villager_wins]):2f}&#39;)

print(&quot;\n&quot;)

print(&quot;Average amount of days until a win is achieved by villagers in approval games&quot;)
print(f&#39;\t Trained villagers : {np.mean([villager_win[-1][&quot;day&quot;] for villager_win in trained_approval_villager_wins]):2f}&#39;)
print(f&#39;\t Untrained villagers : {np.mean([villager_win[-1][&quot;day&quot;] for villager_win in untrained_approval_villager_wins]):2f}&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average amount of days until a win is achieved by villagers in plurality games
	 Trained villagers : 2.987368
	 Untrained villagers : 3.076923


Average amount of days until a win is achieved by villagers in approval games
	 Trained villagers : 2.970833
	 Untrained villagers : 3.376471
</pre></div>
</div>
</div>
</div>
</section>
<section id="days-between-wolf-executions">
<h4>Days between wolf executions<a class="headerlink" href="#days-between-wolf-executions" title="Permalink to this heading">#</a></h4>
<p>Looking at the distance in days between wolf executions also highlights positive trends in learning and collaboration, as the lower the number, the more likely villagers were able to confidently coordinate and identify the wolves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;Average amount of days until the next wolf was killed in plurality games for 2 wolf environments&quot;)
wolf_execution_days = \
    [indicators._when_did_wolves_get_killed(trained_villager_win) for trained_villager_win in trained_plurality_villager_wins]
wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]
print(f&#39;\tDays between wolf kills for trained agents : {np.mean(wolf_execution_duration_between):.3f}&#39;)

wolf_execution_days = \
    [indicators._when_did_wolves_get_killed(untrained_villager_win) for untrained_villager_win in untrained_plurality_villager_wins]
wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]
print(f&#39;\tDays between wolf kills for untrained agents : {np.mean(wolf_execution_duration_between):.3f}&#39;)

print(&quot;\n&quot;)

print(&quot;Average amount of days until the next wolf was killed in approval games for 2 wolf environments&quot;)
wolf_execution_days = \
    [indicators._when_did_wolves_get_killed(trained_villager_win) for trained_villager_win in trained_approval_villager_wins]
wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]
print(f&#39;\tDays between wolf kills for trained agents : {np.mean(wolf_execution_duration_between):.3f}&#39;)

wolf_execution_days = \
    [indicators._when_did_wolves_get_killed(untrained_villager_win) for untrained_villager_win in untrained_approval_villager_wins]
wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]
print(f&#39;\tDays between wolf kills for untrained agents : {np.mean(wolf_execution_duration_between):.3f}&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average amount of days until the next wolf was killed in plurality games for 2 wolf environments
	Days between wolf kills for trained agents : 1.581
	Days between wolf kills for untrained agents : 1.667


Average amount of days until the next wolf was killed in approval games for 2 wolf environments
	Days between wolf kills for trained agents : 1.448
	Days between wolf kills for untrained agents : 1.659
</pre></div>
</div>
</div>
</div>
</section>
<section id="targetting-indicators">
<h4>Targetting Indicators<a class="headerlink" href="#targetting-indicators" title="Permalink to this heading">#</a></h4>
<p>Picking the right indicators to try and describe targetting behavior is not straightforward, and differs between plurality and approval voting. Below are the ones currently chosen for both game types, along with a rendering of them across days and phases in a randomly selected game.</p>
<p>For further analysis, we also use <code class="docutils literal notranslate"><span class="pre">indicators._game_avg_records(replays,indicator_function)</span></code> to average these values across phases and days for every replay.</p>
<section id="plurality">
<h5>Plurality<a class="headerlink" href="#plurality" title="Permalink to this heading">#</a></h5>
<p>To try an make sense of targetting, we chose to look at:</p>
<ul class="simple">
<li><p><strong>Ratio of unique villager targets</strong>: How many unique players are targetted on average by villagers?</p></li>
<li><p><strong>Ratio of villagers voting for themselves</strong>: How much self targetting is occuring on average by villagers?</p></li>
<li><p><strong>Percentage of villagers targetting dead players</strong>: How many villagers are voting for dead players out of total votes cast?</p></li>
<li><p><strong>Percentage of villager votes targetting wolves and dead wolves</strong>: How many villagers are voting for werewolves out of total votes cast?</p></li>
</ul>
<p>These should be good enough to indicate cooperation as well as general role comprehension.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>record = indicators._plurality_target_indicators(trained_plurality_villager_wins[0], verbose=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Wolves : [&#39;player_0&#39;, &#39;player_3&#39;]

Day : 1 | Phase : 0 | Round : 0
Villager votes : [9, 9, 9, 3, 9, 9, 5, 2]
	 | - Ratio of unique players targetted : 0.5
	 | - 0.125 of the votes targetting wolves
	 | - 0.125 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 1 | Phase : 0 | Round : 1
Villager votes : [3, 9, 9, 7, 7, 8, 6, 7]
	 | - Ratio of unique players targetted : 0.625
	 | - 0.125 of the votes targetting wolves
	 | - 0.125 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 1 | Phase : 1 | Round : 0
Villager votes : [2, 8, 9, 8, 9, 0, 3, 8]
	 | - Ratio of unique players targetted : 0.625
	 | - 0.250 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 2 | Phase : 0 | Round : 0
Villager votes : [9, 6, 9, 8, 3, 8]
	 | - Ratio of unique players targetted : 0.6666666666666666
	 | - 0.167 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.333 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 2 | Phase : 0 | Round : 1
Villager votes : [8, 3, 3, 8, 3, 8]
	 | - Ratio of unique players targetted : 0.3333333333333333
	 | - 0.500 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 2 | Phase : 1 | Round : 0
Villager votes : [7, 3, 0, 0, 0, 0]
	 | - Ratio of unique players targetted : 0.5
	 | - 0.833 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 3 | Phase : 0 | Round : 0
Villager votes : [3, 8, 8, 0, 9]
	 | - Ratio of unique players targetted : 0.8
	 | - 0.400 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.400 share of villager votes targetting dead players
	 | - 0.200 share of villager votes targetting dead wolves

Day : 3 | Phase : 0 | Round : 1
Villager votes : [8, 7, 3, 3, 8]
	 | - Ratio of unique players targetted : 0.6
	 | - 0.400 of the votes targetting wolves
	 | - 0.0 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves

Day : 3 | Phase : 1 | Round : 0
Villager votes : [3, 3, 6, 3, 6]
	 | - Ratio of unique players targetted : 0.4
	 | - 0.600 of the votes targetting wolves
	 | - 0.2 of villagers targetting themselves
	 | - 0.000 share of villager votes targetting dead players
	 | - 0.000 share of villager votes targetting dead wolves
</pre></div>
</div>
</div>
</div>
</section>
<section id="approval">
<h5>Approval<a class="headerlink" href="#approval" title="Permalink to this heading">#</a></h5>
<p>Because of the extra dimensions and expressability of approval voting, trying to determine behavior for these agents is much harder. We thus collect many different and interelated indicators to see what combinations of them might give us the most insight. Some may seem redudant, but providing different perspectives to certain indicators will hopefully paint a more complete picture while we analyze our data.</p>
<p>The ones we are currently looking at are:</p>
<ul class="simple">
<li><p><strong>average target count</strong>: How many candidates does a single agent dissaprove of on average?</p></li>
<li><p><strong>average like count</strong>: How many candidates does a single agent like on average?</p></li>
<li><p><strong>average neutral count</strong>: How many candidates does a single agent use a neutral option for on average?</p></li>
<li><p><strong>average self target</strong>: How likely is it a single agent will target/dissaprove of themselves?</p></li>
<li><p><strong>average self like</strong>: How likely is it that a single agent will like themselves?</p></li>
<li><p><strong>percentage of wolves in top targets</strong>: How many targets are allocated towards werewolves when considering a top percentage of targets cast.</p></li>
<li><p><strong>percentage of wolves in top likes</strong>: How many likes are allocated towards werewolves when considering a top percentage of likes cast.</p></li>
<li><p><strong>percent of votes targetting dead players</strong>: How many targets are towards dead players out of all targets cast.</p></li>
<li><p><strong>percent of votes targetting dead wolves</strong>: How many targets are towards dead werewolves out of all targets cast.</p></li>
<li><p><strong>percent of votes targetting wolves that are still alive</strong>: How many targets are towards werewolves still in the game out of all targets cast.</p></li>
<li><p><strong>percent of likes for dead wolves</strong>: How many likes are towards dead werewolves out of all likes cast.</p></li>
<li><p><strong>percent of likes for wolves that are still alive</strong>: How many likes are towards werewolves still in the game out of all likes cast.</p></li>
<li><p><strong>percent of likes towards dead villagers</strong>: How many likes are towards dead villagers out of all likes cast.</p></li>
<li><p><strong>percent of likes towards villagers that are still alive</strong>: How many likes are towards villagers still in the game out of all likes cast.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>_ = indicators._approval_target_indicators(trained_approval_villager_wins[0], verbose=True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Day : 1 | Phase : 0 - Accusation Phase | Round : 0
	 | - avg targetted 3.75, liked 3.00, neutral 3.25
	 | - 0.50 share of villagers targeted themselves, and 0.38 liked themselves
	 | - 0.0 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.00
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.20)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.17)
	 | - % of likes towards dead villagers (0.00), and towards living villagers (0.83)


Day : 1 | Phase : 0 - Accusation Phase | Round : 1
	 | - avg targetted 2.62, liked 3.38, neutral 4.00
	 | - 0.12 share of villagers targeted themselves, and 0.38 liked themselves
	 | - 0.3333333333333333 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.00
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.24)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.19)
	 | - % of likes towards dead villagers (0.00), and towards living villagers (0.81)


Day : 1 | Phase : 1 - Voting Phase | Round : 0
	 | - avg targetted 3.25, liked 3.25, neutral 3.50
	 | - 0.25 share of villagers targeted themselves, and 0.25 liked themselves
	 | - 0.0 wolves targetted in top votes
	 | - 0.5 wolves liked in top likes
	 | - % of votes towards dead players (0.00
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.12)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.31)
	 | - % of likes towards dead villagers (0.00), and towards living villagers (0.69)


Day : 2 | Phase : 0 - Accusation Phase | Round : 0
	 | - avg targetted 3.33, liked 3.83, neutral 2.83
	 | - 0.33 share of villagers targeted themselves, and 0.50 liked themselves
	 | - 0.5 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.25
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.20)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.17)
	 | - % of likes towards dead villagers (0.17), and towards living villagers (0.65)


Day : 2 | Phase : 0 - Accusation Phase | Round : 1
	 | - avg targetted 3.00, liked 4.00, neutral 3.00
	 | - 0.33 share of villagers targeted themselves, and 0.50 liked themselves
	 | - 0.5 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.22
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.22)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.21)
	 | - % of likes towards dead villagers (0.17), and towards living villagers (0.62)


Day : 2 | Phase : 1 - Voting Phase | Round : 0
	 | - avg targetted 3.17, liked 3.17, neutral 3.67
	 | - 0.17 share of villagers targeted themselves, and 0.33 liked themselves
	 | - 0.5 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.32
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.37)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.11)
	 | - % of likes towards dead villagers (0.16), and towards living villagers (0.74)


Day : 3 | Phase : 0 - Accusation Phase | Round : 0
	 | - avg targetted 4.25, liked 3.25, neutral 2.50
	 | - 0.00 share of villagers targeted themselves, and 0.75 liked themselves
	 | - 0.0 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.65
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.18)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.15)
	 | - % of likes towards dead villagers (0.08), and towards living villagers (0.77)


Day : 3 | Phase : 0 - Accusation Phase | Round : 1
	 | - avg targetted 2.50, liked 3.25, neutral 4.25
	 | - 0.00 share of villagers targeted themselves, and 0.50 liked themselves
	 | - 1.0 wolves targetted in top votes
	 | - 0.5 wolves liked in top likes
	 | - % of votes towards dead players (0.30
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.50)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.23)
	 | - % of likes towards dead villagers (0.31), and towards living villagers (0.46)


Day : 3 | Phase : 1 - Voting Phase | Round : 0
	 | - avg targetted 3.25, liked 2.25, neutral 4.50
	 | - 0.00 share of villagers targeted themselves, and 0.00 liked themselves
	 | - 1.0 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.38
	 | - % of votes for dead wolves (0.00), and towards living wolves (0.46)
	 | - % of likes towards dead wolves (0.00) and towards living wolves (0.11)
	 | - % of likes towards dead villagers (0.56), and towards living villagers (0.33)


Day : 4 | Phase : 0 - Accusation Phase | Round : 0
	 | - avg targetted 3.33, liked 3.67, neutral 3.00
	 | - 0.33 share of villagers targeted themselves, and 0.67 liked themselves
	 | - 0.0 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.50
	 | - % of votes for dead wolves (0.10), and towards living wolves (0.20)
	 | - % of likes towards dead wolves (0.18) and towards living wolves (0.09)
	 | - % of likes towards dead villagers (0.36), and towards living villagers (0.36)


Day : 4 | Phase : 0 - Accusation Phase | Round : 1
	 | - avg targetted 4.00, liked 2.00, neutral 4.00
	 | - 0.00 share of villagers targeted themselves, and 0.00 liked themselves
	 | - 0.0 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.75
	 | - % of votes for dead wolves (0.17), and towards living wolves (0.17)
	 | - % of likes towards dead wolves (0.17) and towards living wolves (0.17)
	 | - % of likes towards dead villagers (0.33), and towards living villagers (0.33)


Day : 4 | Phase : 1 - Voting Phase | Round : 0
	 | - avg targetted 3.00, liked 4.33, neutral 2.67
	 | - 0.33 share of villagers targeted themselves, and 0.33 liked themselves
	 | - 0.5 wolves targetted in top votes
	 | - 0.0 wolves liked in top likes
	 | - % of votes towards dead players (0.56
	 | - % of votes for dead wolves (0.11), and towards living wolves (0.22)
	 | - % of likes towards dead wolves (0.08) and towards living wolves (0.08)
	 | - % of likes towards dead villagers (0.46), and towards living villagers (0.38)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="ties">
<h4>Ties<a class="headerlink" href="#ties" title="Permalink to this heading">#</a></h4>
<p>Ties are quite common, and could possibly be used strategically. Knowning when/if ties are occuring could possibly lead to a better understanding of agent voting patterns.</p>
<p>What we are currenly looking for is:</p>
<ul class="simple">
<li><p>What percentage of voting rounds are ties?</p></li>
<li><p>How often do ties in accusation rounds lead to ties in voting rounds?</p></li>
<li><p>If a wolf gets lucky and survives a tied voting round, how likey is it they get executed the next voting round?</p></li>
</ul>
<p>There are two functions we use to achieve this:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">indicators._game_tie_info(game_replay,</span> <span class="pre">voting_type=None)</span></code> which returns if there was a tie, if and which wolf was targetted, and if a wolf died during the phase. This is done for every day and every phase in a game</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">indicators._process_tie_info(tie_records)</span></code> takes the results above and returns:</p>
<ul>
<li><p>percentage of ties in accusation phases per game</p></li>
<li><p>percentage of ties in voting phases per game</p></li>
<li><p>likelihood of a tie in a voting phase given a tie in the prior accusation phases</p></li>
<li><p>likelihood of a wolf getting targetting in a subsequent voting round after getting lucky and surviving a tie round where they were a target</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tgps = \
    np.nanmean(np.stack([indicators._process_tie_info(indicators._game_tie_info(trained_villager_win, voting_type=&quot;plurality&quot;)) for trained_villager_win in trained_plurality_villager_wins]),axis= 0)
tgas = \
    np.nanmean(np.stack([indicators._process_tie_info(indicators._game_tie_info(trained_villager_win, voting_type=&quot;approval&quot;)) for trained_villager_win in trained_approval_villager_wins]), axis=0)

print(&quot;Plurality tie indicators&quot;)
print(f&#39;\tLikelihood of ties in accusation phases : {tgps[0]:.2f}&#39;)
print(f&#39;\tLikelihood of ties in voting phases : {tgps[1]:.2f}&#39;)
print(f&#39;\tLikelihood of a tie in a voting phase given a tie in the prior accusation phases {tgps[2]:.2f}&#39;)
print(f&#39;\tLikelihood of a wolf getting targetting in a subsequent voting round if they survived a tie : {tgps[3]:.2f}&#39;)
print(&quot;\n&quot;)
print(&quot;Approval tie indicators&quot;)
print(f&#39;\tLikelihood of ties in accusation phases : {tgas[0]:.2f}&#39;)
print(f&#39;\tLikelihood of ties in voting phases : {tgas[1]:.2f}&#39;)
print(f&#39;\tLikelihood of a tie in a voting phase given a tie in the prior accusation phases {tgas[2]:.2f}&#39;)
print(f&#39;\tLikelihood of a wolf getting targetting in a subsequent voting round if they survived a tie : {tgas[3]:.2f}&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Plurality tie indicators
	Likelihood of ties in accusation phases : 0.29
	Likelihood of ties in voting phases : 0.19
	Likelihood of a tie in a voting phase given a tie in the prior accusation phases 0.21
	Likelihood of a wolf getting targetting in a subsequent voting round if they survived a tie : 0.77


Approval tie indicators
	Likelihood of ties in accusation phases : 0.36
	Likelihood of ties in voting phases : 0.31
	Likelihood of a tie in a voting phase given a tie in the prior accusation phases 0.32
	Likelihood of a wolf getting targetting in a subsequent voting round if they survived a tie : 0.63
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<span id="document-game_visualization"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles
import random
import copy
from tqdm import tqdm
from collections import Counter
import matplotlib.pyplot as plt
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_recurrent_game
from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf
import notebooks.learning_agents.stats as indicators 
import networkx as nx 
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="replay-visualization">
<h2>Replay visualization<a class="headerlink" href="#replay-visualization" title="Permalink to this heading">#</a></h2>
<p>We have ways to print out game replays, but humans tend to identify patterns quicker when they can visualize the data.
By plotting the game in a graph format, we give the reader yet another way to consume game replays</p>
<p>First we play a couple of games</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()

obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

untrained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128, 
                                        &quot;rec_layers&quot;: 1,
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)

trained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)
trained_plurality_agent.load_state_dict(torch.load(&quot;../notebooks/stored_agents/lstm_first_no_one_hot_128_128/plurality_agent_10_score_46&quot;))

# random_agent = None

trained_plurality_wins, trained_plurality_replays = play_recurrent_game(env, random_plurality_wolf, trained_plurality_agent, num_times=10, hidden_state_size=128, voting_type=&quot;plurality&quot;)
</pre></div>
</div>
</div>
</details>
</div>
<p>Then, using the hidden functions below, we generate graphs for each phase in each day, and color nodes and edges accordingly. Purple for werewolves, Green for villagers. Red means you died this phase, Black means the agent was already dead during the phase.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def viz_replay(replay):
    #[wolf for wolf in stage[&quot;werewolves&quot;]]
    wolves = replay[0][&#39;werewolves&#39;]
    villagers = replay[0][&#39;villagers&#39;]
    winners = None

    day_info = {}
    color_map = {}
    pos = None

    vote_rounds = []
    night_rounds = []
    for i, phase in enumerate(replay):
        if phase[&quot;phase&quot;] == Phase.VOTING:
            vote_rounds.append(phase)
        if phase[&quot;phase&quot;] == Phase.NIGHT:
            night_rounds.append(phase)
        if i == 0:

            # setup the first graph for the position and color map
            G = nx.DiGraph()
            wolf_nodes = [(id, {&#39;color&#39;: &#39;purple&#39;}) for id in wolves]
            villager_nodes = [(id, {&#39;color&#39;: &#39;green&#39;}) for id in villagers]
            G.add_nodes_from(wolf_nodes)
            G.add_nodes_from(villager_nodes)

            if pos == None:
                pos = nx.spring_layout(G)
            continue
    
        if phase[&#39;day&#39;] not in day_info.keys():
            day_info[phase[&#39;day&#39;]] = []
            color_map[phase[&#39;day&#39;]] = []

            # 
        if phase[&quot;phase&quot;] == Phase.VOTING:
            if len(vote_rounds) == 1:
                dead_players = []
                # dead_wolves = []
                executed_this_round = phase[&#39;executed&#39;][0]
                killed_this_round = []
            else:
                dead_players = list((set(phase[&#39;executed&#39;]) &amp; set(vote_rounds[-2][&#39;executed&#39;])) | set(phase[&#39;killed&#39;]))
                # dead_wolves = list(set(wolves) &amp; set(dead_players))
                executed_this_round = list(set(phase[&#39;executed&#39;]) - set(vote_rounds[-2][&#39;executed&#39;]))[0]
                killed_this_round = []

        elif phase[&quot;phase&quot;] == Phase.NIGHT:
            if len(night_rounds) == 1:
                dead_players = phase[&#39;executed&#39;]
                executed_this_round = []
                killed_this_round = phase[&#39;killed&#39;]
            else:
                dead_players = list((set(phase[&#39;killed&#39;]) &amp; set(night_rounds[-2][&#39;killed&#39;])) | set(phase[&#39;executed&#39;]))
                executed_this_round = []
                killed_this_round = list(set(phase[&#39;killed&#39;]) - set(night_rounds[-2][&#39;killed&#39;]))[0]
        else:
            dead_players = list(set(phase[&#39;executed&#39;]) | set(phase[&#39;killed&#39;]))
            executed_this_round = []
            killed_this_round = []

        G = nx.DiGraph()
        wolf_nodes = [(id, {&#39;color&#39;: &#39;purple&#39;}) for id in wolves]
        villager_nodes = [(id, {&#39;color&#39;: &#39;green&#39;}) for id in villagers]
        G.add_nodes_from(wolf_nodes)
        G.add_nodes_from(villager_nodes)

        graph_color_map = []
        for node in G:
            if &#39;color&#39; in G.nodes[node]:
                if node in dead_players:
                    graph_color_map.append(&#39;black&#39;)
                elif node in executed_this_round or node in killed_this_round:
                    graph_color_map.append(&#39;red&#39;)
                else:
                    graph_color_map.append(G.nodes[node][&#39;color&#39;])
            else:
                graph_color_map.append(&#39;blue&#39;)

        # edges for plurality
        if phase[&quot;phase&quot;] == Phase.NIGHT:
            # hide villager votes
            edges = [(voter, f&#39;player_{target}&#39;) for voter, target in phase[&#39;votes&#39;].items() if target != len(wolves) + len(villagers) and voter not in villagers]
        else:
            edges = [(voter, f&#39;player_{target}&#39;) for voter, target in phase[&#39;votes&#39;].items() if target != len(wolves) + len(villagers)]
        G.add_edges_from(edges)

        day_info[phase[&#39;day&#39;]].append(G)
        color_map[phase[&#39;day&#39;]].append(graph_color_map)

    return day_info, pos, color_map
</pre></div>
</div>
</div>
</details>
</div>
<p>This loose piece of code takes in the output of our function and actually draws out the game in a matplotlib subplot graph with rows representing incrementing days, and columns representing incrementing phases. Labelling and positioning of Day labels was tailored to this particular example.</p>
<p>Reading this graph representation is similar to reading a book. You start from the top left subplot, and as you go across, you transition from accusation phases, to a voting phase, and finally the night phase where werewolves kill a villager. Being a uni-directional graph, terminating arrows indicate a targetting from an originating player.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
viz_info, pos, cmap = viz_replay(trained_plurality_replays[3])

day_num = len(viz_info.keys())
phases_per_day = max([len(val) for val in viz_info.values()])

phase_map = list(range(phases_per_day))
phase_range_accusation = phase_map[0:-2]

fig, axs = plt.subplots(day_num,phases_per_day, figsize=(15,13), sharey=True, sharex=True)

for day in range(1,day_num+1):
    # axs[day-1][0].annotate(&quot;Hello&quot;, (0.1,0.5) )
    for j, graph in enumerate(viz_info[day]):
        if day == 1:
            if j in phase_range_accusation:
                title = f&#39;Accusation Phase&#39;
            elif j == phase_map[-2]:
                title = f&#39;Voting Phase&#39;
            elif j == phase_map[-1]:
                title = f&#39;Night Phase&#39;
            axs[day-1][j].set_title(title)
        nx.draw(viz_info[day][j], pos, ax=axs[day-1][j], node_color=cmap[day][j])
    axs[day-1][0].set_ylabel(f&#39;Day {day}&#39;)

day_phase_lengths = [len(val) for val in viz_info.values()]
if day_phase_lengths[-1] &lt; day_phase_lengths[-2]:
    axs[-1][-1].axis(&#39;off&#39;)
    plt.suptitle(&quot;Villagers win!&quot;)
else:
    plt.suptitle(&quot;Wolves win!&quot;)

fig.tight_layout()


fig.text(-0.02, 0.80, &#39;Day 1&#39;)
fig.text(-0.02, 0.45, &#39;Day 2&#39;)
fig.text(-0.02, 0.19, &#39;Day 3&#39;)

plt.show()
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/8d4febd42643dfbab4f58020bab34e8c058d60f10aa54b324aad5f7e9ed31249.png" src="_images/8d4febd42643dfbab4f58020bab34e8c058d60f10aa54b324aad5f7e9ed31249.png" />
</div>
</div>
</section>
<span id="document-plurality-analysis"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%matplotlib inline
import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import plurality_env, Roles, Phase
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_static_game, play_recurrent_game
from notebooks.learning_agents.static_agents import (
    random_plurality_villager, 
    random_coordinated_plurality_villager, 
    random_agent,
    random_plurality_wolf,
    revenge_plurality_wolf,
    coordinated_revenge_plurality_wolf)
import notebooks.learning_agents.stats as indicators
import random
import copy
from matplotlib import pyplot as plt
from tqdm import tqdm
from tabulate import tabulate
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="plurality-voting">
<h2>Plurality Voting<a class="headerlink" href="#plurality-voting" title="Permalink to this heading">#</a></h2>
<p>Plurality is a voting method where each voter picks a single candidate, and the candidate with the most votes is selected. It’s simplicity has lead to wide adoption, however there are quite a few drawbacks, one of which being the limited expressibility of a voter.</p>
<p>In our Werewolf plurality implementation, an agent can only select a single target, and cannot express their beliefs towards the remaining agents. This is also the voting mechanism used in every paper involving the Werewolf game up to now, so we want to see how our trained agents compare.</p>
<section id="quick-takeaway">
<span id="p-analysis"></span><h3>Quick Takeaway<a class="headerlink" href="#quick-takeaway" title="Permalink to this heading">#</a></h3>
<p>Trained villager agents performed better than CL-Targets villagers, even when pitted against werewolf policies they were not trained against. Using the below indicators, we have a theory that these agents use accusation phases to implicitly signal intent by targetting dead villagers to then workout who they believe are actually werewolves and who are truly villagers.</p>
</section>
<section id="win-rates">
<h3>Win Rates<a class="headerlink" href="#win-rates" title="Permalink to this heading">#</a></h3>
<p>We want to see how our hand-crafted agents play against eachother, with special interest given to coordinated random villagers and wolves.</p>
<p>As expected, the coordinated random villagers and wolves performed the best out of the static policies <span id="id1">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>]</span>.
Our agent, trained against coordinated random wolves, performed better than all of our hand-crafted villager policies. They also generalized well against our other wolf policies, and actually had the highest win rates across the board.</p>
<p>Below is code to generate the following table of 1000 runs between each villager policy and each werewolf policy. We base our discussion around this markdown table.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Our trained agent, Trained-CRWolves, is an outlier that got above a <span class="math notranslate nohighlight">\(0.40\)</span> win-rate. On average, trained plurality agents did not do as well.</p>
</div>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Villager Strategy vs.</p></th>
<th class="head"><p><a class="reference internal" href="intro.html#rwolves"><span class="std std-ref">Random</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#crwolves"><span class="std std-ref">CRWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#revwolves"><span class="std std-ref">RevWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#crevwolves"><span class="std std-ref">CRevWolves</span></a></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="intro.html#r-villagers"><span class="std std-ref">Random</span></a></p></td>
<td><p>0.597</p></td>
<td><p>0.042</p></td>
<td><p>0.076</p></td>
<td><p>0.074</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="intro.html#l-villagers"><span class="std std-ref">L-Targets</span></a></p></td>
<td><p>0.705</p></td>
<td><p>0.125</p></td>
<td><p>0.192</p></td>
<td><p>0.244</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="intro.html#cl-villagers"><span class="std std-ref">CL-Targets</span></a></p></td>
<td><p>0.653</p></td>
<td><p>0.314</p></td>
<td><p>0.304</p></td>
<td><p>0.281</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="intro.html#trained-villagers"><span class="std std-ref">Trained-CRWolves</span></a></p></td>
<td><p><strong>0.83</strong></p></td>
<td><p><strong>0.473</strong></p></td>
<td><p><strong>0.45</strong></p></td>
<td><p><strong>0.503</strong></p></td>
</tr>
</tbody>
</table>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()

obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

trained_plurality_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 128,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 1,
                                        &quot;approval_states&quot;: 10},
                                        num_players=10,
                                        obs_size=obs_size)
trained_plurality_agent.load_state_dict(torch.load(&quot;../notebooks/stored_agents/lstm_first_no_one_hot_128_128/plurality_agent_10_score_46&quot;))

num_games = 1000
print(f&#39;10 players, with 2 wolves - number of games played : {num_games} \n&#39;)

rv_wins = []
rv_replays = []
for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)
    rv_wins.append(wins/float(num_games))
    rv_replays.append(replays)

rpv_wins = []
rpv_replays = []
for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_plurality_villager, num_times=num_games)
    rpv_wins.append(wins/float(num_games))
    rpv_replays.append(replays)

cpv_wins = []
cpv_replays = []
for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_coordinated_plurality_villager, num_times=num_games)
    cpv_wins.append(wins/float(num_games))
    cpv_replays.append(replays)

tpv_wins = []
tpv_replays = []
for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:
    # wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)[0]/float(num_games)
    wins, replays = play_recurrent_game(env, wolf_policy, trained_plurality_agent, num_times=num_games, hidden_state_size=128, voting_type=&quot;plurality&quot;)
    tpv_wins.append(wins/float(num_games))
    tpv_replays.append(replays)

print(tabulate([[&#39;Random&#39;, *rv_wins], 
                [&#39;L-Targets&#39;, *rpv_wins], 
                [&#39;CL-Targets&#39;, *cpv_wins], 
                [&#39;Trained-CRWolves&#39;, *tpv_wins]], 
               headers=[&quot;Villager Strategy vs&quot;, 
                        &quot;Random&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 players, with 2 wolves - number of games played : 1000 

Villager Strategy vs      Random    CRWolves    RevWolves    CRevWolves
----------------------  --------  ----------  -----------  ------------
Random                     0.594       0.044        0.076         0.033
L-Targets                  0.674       0.123        0.18          0.109
CL-Targets                 0.653       0.324        0.297         0.309
Trained-CRWolves           0.843       0.475        0.428         0.444
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tpv_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in tpv_replays]
rv_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in rv_replays]
rpv_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in rpv_replays]
cpv_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in cpv_replays]
</pre></div>
</div>
</div>
</details>
</div>
<section id="days-elapsed-before-a-villager-win">
<h4>Days elapsed before a villager win<a class="headerlink" href="#days-elapsed-before-a-villager-win" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rv_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in rv_win_replays]
rpv_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in rpv_win_replays]
cpv_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in cpv_win_replays]
tpv_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in tpv_win_replays]

print(tabulate([[&#39;Random&#39;, *rv_days], 
                [&#39;L-Targets&#39;, *rpv_days], 
                [&#39;CL-Targets&#39;, *cpv_days], 
                [&#39;Trained-CRWolves&#39;, *tpv_days]], 
               headers=[&quot;Villager Strategy vs&quot;, 
                        &quot;Random&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Villager Strategy vs      Random    CRWolves    RevWolves    CRevWolves
----------------------  --------  ----------  -----------  ------------
Random                   3.1835      3.15909      3.40789       3.30303
L-Targets                3.11424     3.52846      3.53333       3.47706
CL-Targets               3.17764     3.52778      3.50168       3.51133
Trained-CRWolves         2.77699     2.92421      3.06542       2.7973
</pre></div>
</div>
</div>
</div>
<p>We had mentioned that this days elapsed indicator is a quick litmus test for any learned improvements, and we can see that the trained agent is quicker on average across the board.</p>
</section>
<section id="days-between-wolf-executions">
<h4>Days between wolf executions<a class="headerlink" href="#days-between-wolf-executions" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tpv_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in tpv_win_replays[1:]]
rv_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in rv_win_replays[1:]]
rpv_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in rpv_win_replays[1:]]
cpv_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in cpv_win_replays[1:]]

print(tabulate([[&#39;Random&#39;, *rv_exec], 
                [&#39;L-Targets&#39;, *rpv_exec], 
                [&#39;CL-Targets&#39;, *cpv_exec], 
                [&#39;Trained-CRWolves&#39;, *tpv_exec]], 
               headers=[&quot;Villager Strategy vs&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;]))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Villager Strategy vs      CRWolves    RevWolves    CRevWolves
----------------------  ----------  -----------  ------------
Random                     1.59091      1.72368       1.66667
L-Targets                  1.72358      1.74444       1.78899
CL-Targets                 1.69136      1.67677       1.59223
Trained-CRWolves           1.56842      1.60748       1.49099
</pre></div>
</div>
</div>
</div>
<p>This number is one we also expected to be lower, but it won’t be as drastic in a 10 player game due to the shortness of games. At most we have <span class="math notranslate nohighlight">\(5\)</span> days. In addition, the lowest (and best) value we could have here is a <span class="math notranslate nohighlight">\(1\)</span>, indicating a perfect targetting of the remaining werewolf after executing the first werewolf. The closer this number is to <span class="math notranslate nohighlight">\(1\)</span>, the better the agents are at identifying duplicitous werewolf players. We can see our trained villagers have a lower day between wolf executions across the board.</p>
</section>
<section id="ties">
<h4>Ties<a class="headerlink" href="#ties" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tie_info = [np.nanmean(np.stack([indicators._process_tie_info(indicators._game_tie_info(replay, voting_type=&quot;plurality&quot;)) for replay in replays]), axis=0) for replays in tpv_win_replays[1:]]

print(tabulate([[&#39;vs. CRWolves&#39;, *tie_info[0]], 
                [&#39;vs. RevWolves&#39;, *tie_info[1]], 
                [&#39;vs. CRevWolves&#39;, *tie_info[2]]], 
               headers=[&quot;Trained-CRWolves&quot;, 
                        &quot;Tie in Accusation&quot;, 
                        &quot;Tie in Voting&quot;,
                        &quot;Tie in Voting given a tie in accusation&quot;,
                        &quot;Likelihood of wolf targetted after a tie&quot;]))

</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trained-CRWolves      Tie in Accusation    Tie in Voting    Tie in Voting given a tie in accusation    Likelihood of wolf targetted after a tie
------------------  -------------------  ---------------  -----------------------------------------  ------------------------------------------
vs. CRWolves                   0.283421         0.175789                                   0.190936                                    0.65
vs. RevWolves                  0.248345         0.23014                                    0.274828                                    0.5
vs. CRevWolves                 0.26539          0.158784                                   0.170892                                    0.723404
</pre></div>
</div>
</div>
</div>
<p>Having less ties in accusation phases can indicate more coordination, and a lower number of ties in the voting phase can indicate synthesis of information over the accusation rounds. Learning agents also have a much lower chance of having a tie in a voting round if there was a tie in the accusation round, further solidifying this notion of accusation information synthesis.</p>
<p>For follow up targetting of wolves who survived tie rounds, the higher the number, the better. Again we can see learning agents doing this at a higher frequency. It is also promising to see how well these trained villagers generalized.</p>
</section>
<section id="targetting-indicators">
<h4>Targetting Indicators<a class="headerlink" href="#targetting-indicators" title="Permalink to this heading">#</a></h4>
<p>As a reminder, our indicators are averaged and stacked, with the following indicators in order:</p>
<ul class="simple">
<li><p><em>unique targets</em></p></li>
<li><p><em>self vote</em></p></li>
<li><p><em>percentage of targets towards wolves</em></p></li>
<li><p><em>percentage of targets towards dead players</em></p></li>
<li><p><em>percentage of targets torwards dead wolves</em></p></li>
</ul>
<p>There are trends across phases in a day, and trends across days we look at. Our stacked records have a shape of <code class="docutils literal notranslate"><span class="pre">(day,</span> <span class="pre">phase,</span> <span class="pre">indicators)</span></code>, which we can slice in many directions.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>indicator_titles = [&quot;Unique Targets&quot;,
                    &quot;Likelihood of self vote&quot;,
                    &quot;Percentage of targets toward wolves&quot;,
                    &quot;Percentage of targets towards dead players&quot;,
                    &quot;Percentage of targets towards dead wolves&quot;]

# we do this against all wolf policies
tpv_avg_records = [indicators._game_avg_records(replays, indicators._plurality_target_indicators) for replays in tpv_win_replays]
tpv_stacked = [np.stack(list(avg_records.values())) for avg_records in tpv_avg_records]

rv_avg_records = [indicators._game_avg_records(replays, indicators._plurality_target_indicators) for replays in rv_win_replays]
rv_stacked = [np.stack(list(avg_records.values())) for avg_records in rv_avg_records]

# https://www.heavy.ai/blog/12-color-palettes-for-telling-better-stories-with-your-data
day_colors = [&quot;#115f9a&quot;, &quot;#22a7f0&quot;, &quot;#76c68f&quot;, &quot;#c9e52f&quot;]
#day_colors = [&quot;#1b4332&quot;, &quot;#2d6a4f&quot;, &quot;#40916c&quot;, &quot;#52b788&quot;]

def plot_indicator(stacked_info, indicator_title, indicator_id, colors):
    plt.figure(figsize=(9,4))
    x_tick_labels = [&quot;Accusation&quot;, &quot;Accusation&quot;, &quot;Voting&quot;]
    for day, color in zip(range(stacked_info.shape[0]), colors):
        plt.plot(list(range(stacked_info.shape[1])), stacked_info[:,:,indicator_id][day], linewidth=2.0, color=color, label=f&#39;Day {day+1}&#39;)
        plt.xticks([0,1,2], x_tick_labels, rotation=40)
        plt.legend()
        plt.suptitle(indicator_title)
    plt.plot()

def plot_indicator_across_other_wolves(stacked_info, indicator_title, indicator_id):
    x_tick_labels = [&quot;Accusation&quot;, &quot;Accusation&quot;, &quot;Voting&quot;]
    fig, axs = plt.subplots(1, 4, sharey=&#39;row&#39;, figsize=(15,6))

    for day in range(stacked_info[0].shape[0]):
        for name, stacked in zip([&quot;Random&quot;, &quot;CRWolves&quot;, &quot;RevWolves&quot;, &quot;CRevWolves&quot;], stacked_info):
            axs[day].plot(list(range(stacked.shape[1])), stacked[:,:,indicator_id][day], linewidth=2.0, label=name)
            axs[day].set_xticks([0,1,2], x_tick_labels, rotation=40)
            axs[day].set_xlabel(f&#39;Day {day + 1}&#39;)

    fig.suptitle(f&#39;{indicator_title}&#39;)
    fig.legend([&quot;Random&quot;, &quot;CRWolves&quot;, &quot;RevWolves&quot;, &quot;CRevWolves&quot;], loc=&quot;upper right&quot;)
    fig.tight_layout()
</pre></div>
</div>
</div>
</details>
</div>
<section id="unique-targets">
<h5>Unique Targets<a class="headerlink" href="#unique-targets" title="Permalink to this heading">#</a></h5>
<p>This is indicator is a ratio of votes towards different targets. The lower the numbers are, the more consensus there is. Unfortunately, there is a dynamic component to it, as each day there are <span class="math notranslate nohighlight">\(2\)</span> less agents. Looking at trends across phases is more indicative of consensus.  We can see that during each day, the number drops as we progress through phases, with the voting phase having the least amount of unique targets. The biggest drop can be seen on the last day, which makes sense as agents have stronger beliefs as to who the last werewolf would be.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tpv_stacked[1], indicator_titles[0], 0, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2f7901f1505b4492c5a361eb4d9bb24bd0d2cd810440a13231ea4a784eb40aba.png" src="_images/2f7901f1505b4492c5a361eb4d9bb24bd0d2cd810440a13231ea4a784eb40aba.png" />
</div>
</div>
<p>When looking at this indicator across games played between the trained agents and other wolf targets, the behavior remains very similar. The biggest difference we can make out here is that because our agents were trained against coordinated wolves, we see the largest drops in unique targets when the werewolves play a coordinated strategy such as CRWolves or CRevWolves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tpv_stacked, indicator_titles[0], 0)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3889fe7603b72b67dd87cf1650c8b29f05afb50c583a1b082ba2e5ad0ec2524b.png" src="_images/3889fe7603b72b67dd87cf1650c8b29f05afb50c583a1b082ba2e5ad0ec2524b.png" />
</div>
</div>
</section>
<section id="likelihood-of-self-voting">
<h5>Likelihood of self voting<a class="headerlink" href="#likelihood-of-self-voting" title="Permalink to this heading">#</a></h5>
<p>Self-Voting is an indicator that has a negative associated reward. Agents will always be penalized if they target themselves, and given that they can only select a single candidate for executing in plurality voting, a self-vote can be quite disasterous.</p>
<p>We can see that by the very low likelihoods, agents have learned to not do it much, and we can attribute some of this to the soft policy sampling selecting a self target every now and then, and maybe very rarely, agents may do it strategically. Because it tends to trend down throughout the day, trained agents are more comfortable doing it during accusation rounds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tpv_stacked[1], indicator_titles[1], 1, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5cb213bf608b40e63cbddcc647ee83bdd481b5a8f2b27122b6de867793e5d67c.png" src="_images/5cb213bf608b40e63cbddcc647ee83bdd481b5a8f2b27122b6de867793e5d67c.png" />
</div>
</div>
<p>Across games versus different policies, we can see similar behaviors, with random wolves causing trained agents to vote for themselves more often.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tpv_stacked, indicator_titles[1], 1)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4bf0bdaee024a2932d345cc5693ca3114005d661731f88484d78d1ac9cd1530.png" src="_images/b4bf0bdaee024a2932d345cc5693ca3114005d661731f88484d78d1ac9cd1530.png" />
</div>
</div>
</section>
<section id="percetange-of-targets-towards-wolves">
<h5>Percetange of targets towards wolves<a class="headerlink" href="#percetange-of-targets-towards-wolves" title="Permalink to this heading">#</a></h5>
<p>This indicator is straightforward and easy to interpret. The higher the number, the more a wolf is targetted. There are three takeaways here:</p>
<ol class="arabic simple">
<li><p>As agents advance through the phases, votes increasingly target wolves, highlighting some consensus amongst villagers.</p></li>
<li><p>In the last day, almost every villager targets wolves during voting.</p></li>
<li><p>When taken in tandem with unique target ratios (our first indicator), we can say that each day, during the first phase, villagers seem to spread out their votes to gather more information, and then sythesize what they see from others and focus their targets on suspected wolves.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tpv_stacked[1], indicator_titles[2], 2, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/48536d13310e87d8c58d33ad62f85b5f6bfa4cd9bc6a1ffeade77f2125333c33.png" src="_images/48536d13310e87d8c58d33ad62f85b5f6bfa4cd9bc6a1ffeade77f2125333c33.png" />
</div>
</div>
<p>We can see similar behavior against other werewolf policies, with the random wolf causing the most trouble. By the last day, when agents have the most information, they are nowhere near as confident against random wolves as opposed to all other policies.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tpv_stacked, indicator_titles[2], 2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/288840d0cd59f2dff7adc5782e4a8fb90c6e4c83765a1299493f27e669afcf80.png" src="_images/288840d0cd59f2dff7adc5782e4a8fb90c6e4c83765a1299493f27e669afcf80.png" />
</div>
</div>
</section>
<section id="percentage-of-targets-towards-dead-players">
<h5>Percentage of targets towards dead players<a class="headerlink" href="#percentage-of-targets-towards-dead-players" title="Permalink to this heading">#</a></h5>
<p>This indicator also has a negative reward associated with it. Every time an agent targets a dead player, they get a negative reward, and their vote is masked by the environment. Given that agents can only select a single candidate in plurality voting, a wasted vote has more of a negative impact towards villagers chances to win.</p>
<p>We can see that throughout the phases in a day, agents target dead players less, with the voting round having the smallest percentage consistently. Voting on accusation days are simply for information, so agents seem to be using these phases to broadcast possible intent, and making sure their target is a viable vote does not seem as important. With <span class="math notranslate nohighlight">\(2\)</span> agents dying per day, by the last day, a majority are dead, so it is likely agents have learned to target dead players as a way to signal during accusation rounds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tpv_stacked[1], indicator_titles[3], 3, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2eda9a962e715a196fc0a7f04bcd976a0ae087825a96d16e5a7ae0a9a1a77c17.png" src="_images/2eda9a962e715a196fc0a7f04bcd976a0ae087825a96d16e5a7ae0a9a1a77c17.png" />
</div>
</div>
<p>Again, we show the indicator between trained agents against different werewolf policies, and the same trend as above is seen in all games. This strengthens our belief that dead players are used as implicit signaling during accusastion phases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tpv_stacked, indicator_titles[3], 3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f68f783d13eb1ebfca3a305ef36ba294fb3f5bdcb9006ecd45d9f2aa4e2e8217.png" src="_images/f68f783d13eb1ebfca3a305ef36ba294fb3f5bdcb9006ecd45d9f2aa4e2e8217.png" />
</div>
</div>
</section>
<section id="percentage-of-targets-towards-dead-wolves">
<h5>Percentage of targets towards dead wolves<a class="headerlink" href="#percentage-of-targets-towards-dead-wolves" title="Permalink to this heading">#</a></h5>
<p>WHen taken in consideriation with targets towards dead players, we can see that most of the dead targetting are towards different villagers, not wolves. There may be more complicated indicators that can shed some more light on this connection, but we believe that this further strengthens the idea of implicit communication of roles by targetting dead villagers more than dead wolves.</p>
<p>When taken in conjunction with <em>percetange of targets towards wolves</em>, the majority of the targetting is towards suspected wolves that are still alive.</p>
<p>When looking at it upfront, this number is low to begin with, and trends down throughout phases, indicating more signaling, and less desire for trained agents to target dead players in voting rounds, as this will have more disasterous consequences than targetting during accusation phases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tpv_stacked[1], indicator_titles[4], 4, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/81ac88ee2fa992682f3d725490df2a8e9fc4de6ec538257bb326ebdce3d2820d.png" src="_images/81ac88ee2fa992682f3d725490df2a8e9fc4de6ec538257bb326ebdce3d2820d.png" />
</div>
</div>
<p>These ideas hold throughout games played against different werewolf policies, with no real standouts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tpv_stacked, indicator_titles[4], 4)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/32b972261a0b3751d1989024499588cf5f4ca825cace2ce88608ff0b96af76a0.png" src="_images/32b972261a0b3751d1989024499588cf5f4ca825cace2ce88608ff0b96af76a0.png" />
</div>
</div>
</section>
</section>
</section>
</section>
<span id="document-approval-analysis"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import torch
import sys
sys.path.append(&#39;../&#39;)
from voting_games.werewolf_env_v0 import pare, Roles, Phase
from notebooks.learning_agents.models import ActorCriticAgent
from notebooks.learning_agents.utils import play_static_game, play_recurrent_game
from notebooks.learning_agents.static_agents import (
    random_approval_villager, 
    random_coordinated_approval_villager, 
    random_agent,
    random_approval_wolf,
    revenge_approval_wolf,
    coordinated_revenge_approval_wolf,
    random_likes_approval_wolf,
    aggressive_approval_wolf,
    )
import notebooks.learning_agents.stats as indicators
import random
import copy
from matplotlib import pyplot as plt
from tqdm import tqdm
from tabulate import tabulate
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="approval-voting">
<h2>Approval Voting<a class="headerlink" href="#approval-voting" title="Permalink to this heading">#</a></h2>
<p>Approval voting <span id="id1">[<a class="reference internal" href="intro.html#id4" title="Steven J Brams and Peter C Fishburn. Approval voting. American Political Science Review, 72(3):831–847, 1978.">BF78</a>]</span> is a mechanism in which voters are not restricted to voting for just one candidate, but can vote for (approve) of as many candidates as they want. The winner (or loser in the case of werewolf) is the candidate with the most votes. As a reminder, because players are voting to eliminate another player, we have slightly modified this voting method to consider a targetting vote as a “dissaproval”, no particular vote as “neutral”, and a “like” as approval. Thus the player that receives the most “dissaprovals” will be eliminated. Likes and neutral opinions do not impact this vote, however allow for richer expression in preferences and more revealing information for other agents to synthesize.</p>
<section id="quick-takeaways">
<h3>Quick Takeaways<a class="headerlink" href="#quick-takeaways" title="Permalink to this heading">#</a></h3>
<p>Trained villager agents were able to successfully play against coordinated werewolf policies better than the other heuristic villager policies, but were stumped by werewolf policies that randomized which players received likes and neutral votes. This indicates that trained agents learned to rely on likes and neutrals, despite them not providing any direct rewards or having any impact on the executed target consensus. Furthermore, agents seemed to have learned the ordinal representation of these three voting options (dislike/target, neutral, like), with liking being a form of trust signaling: either trust in others, or trust in themselves and their votes. Given that each trained villager roughly trusted 3-4 other agents, likes could have also acted as a form of opinion vetting for villagers to trust other villagers if they shared a like in common.</p>
</section>
<section id="win-rates">
<h3>Win Rates<a class="headerlink" href="#win-rates" title="Permalink to this heading">#</a></h3>
<p>We assume that some of the findings in the plurality werewolf game <span id="id2">[<a class="reference internal" href="intro.html#id45" title="Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. Annals of Applied Probability, 18:825-846, 2006. URL: https://api.semanticscholar.org/CorpusID:14668989.">BEM06</a>]</span> will still hold for approval voting, due to the nature of calculating the consensus amongst all the targetting done. (Target with the most dislikes gets voted out).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Below is code to generate the following table of 1000 runs between each villager policy and each werewolf policy. We base our discussion around this markdown table.</p>
</div>
<p>It is clear given the table below that trained villagers learned how to maneuver using an approval mechanism to their advantage. Even against aggressive wolves, and no game breaking coordination mechanic as in CL-Targets, they had the highest win-rates most of the time.</p>
<p>CRLWolves were specifically tailored to stump our agents, and seeing that this is the wolf policy they did the poorest does confirm that trained villager agents relied on “likes” and “neutral” feelings of other agents across candidates. To a lesser degree, we see this in their poorer perfomance against Random wolves, as they must have been stumped by the random likes and neutrals, but were likely saved by the randomness of the werewolve’s targetting .</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Villager Strategy vs.</p></th>
<th class="head"><p><a class="reference internal" href="intro.html#rwolves"><span class="std std-ref">Random</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#crwolves"><span class="std std-ref">CRWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#revwolves"><span class="std std-ref">RevWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#crevwolves"><span class="std std-ref">CRevWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#crlwolves"><span class="std std-ref">CRLWolves</span></a></p></th>
<th class="head"><p><a class="reference internal" href="intro.html#aggrowolves"><span class="std std-ref">AggroWolves</span></a></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="intro.html#r-villagers"><span class="std std-ref">Random</span></a></p></td>
<td><p>0.599</p></td>
<td><p>0.065</p></td>
<td><p>0.118</p></td>
<td><p>0.137</p></td>
<td><p>0.085</p></td>
<td><p>0.002</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="intro.html#l-villagers"><span class="std std-ref">L-Targets</span></a></p></td>
<td><p>0.676</p></td>
<td><p>0.12</p></td>
<td><p>0.243</p></td>
<td><p>0.209</p></td>
<td><p>0.122</p></td>
<td><p>0.023</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="intro.html#cl-villagers"><span class="std std-ref">CL-Targets</span></a></p></td>
<td><p><strong>0.682</strong></p></td>
<td><p>0.288</p></td>
<td><p>0.302</p></td>
<td><p>0.294</p></td>
<td><p><strong>0.291</strong></p></td>
<td><p>0.311</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="intro.html#trained-villagers"><span class="std std-ref">Trained-CRWolves</span></a></p></td>
<td><p>0.567</p></td>
<td><p><strong>0.532</strong></p></td>
<td><p><strong>0.577</strong></p></td>
<td><p><strong>0.582</strong></p></td>
<td><p>0.193</p></td>
<td><p><strong>0.385</strong></p></td>
</tr>
</tbody>
</table>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>env = pare(num_agents=10, werewolves=2, num_accusations=2)
observations, _, _, _, _ = env.reset()
obs_size= env.convert_obs(observations[&#39;player_0&#39;][&#39;observation&#39;]).shape[-1]

trained_approval_agent = ActorCriticAgent({&quot;rec_hidden_size&quot;: 256,
                                        &quot;rec_layers&quot;: 1, 
                                        &quot;joint_mlp_size&quot;: 128,
                                        &quot;split_mlp_size&quot;: 128,
                                        &quot;num_votes&quot;: 10,
                                        &quot;approval_states&quot;: 3},
                                        num_players=10,
                                        obs_size=obs_size)
trained_approval_agent.load_state_dict(torch.load(&quot;../notebooks/stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_53.pth&quot;))

num_games = 1000
print(f&#39;10 players, with 2 wolves - number of games played : {num_games} \n&#39;)

rv_wins = []
rv_replays = []
for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)
    rv_wins.append(wins/float(num_games))
    rv_replays.append(replays)

rav_wins = []
rav_replays = []
for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_approval_villager, num_times=num_games)
    rav_wins.append(wins/float(num_games))
    rav_replays.append(replays)

cav_wins = []
cav_replays = []
for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:
    wins, replays = play_static_game(env, wolf_policy, random_coordinated_approval_villager, num_times=num_games)
    cav_wins.append(wins/float(num_games))
    cav_replays.append(replays)

tav_wins = []
tav_replays = []
for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:
    wins, replays = play_recurrent_game(env, wolf_policy, trained_approval_agent, num_times=num_games, hidden_state_size=256, voting_type=&quot;approval&quot;)
    tav_wins.append(wins/float(num_games))
    tav_replays.append(replays)

print(tabulate([[&#39;Random&#39;, *rv_wins], 
                [&#39;L-Targets&#39;, *rav_wins], 
                [&#39;CL-Targets&#39;, *cav_wins], 
                [&#39;Trained-CRWolves&#39;, *tav_wins]], 
               headers=[&quot;Villager Strategy&quot;, 
                        &quot;RWolves&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;,
                        &quot;CRLWolves&quot;,
                        &quot;AggroWolves&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10 players, with 2 wolves - number of games played : 1000 

Villager Strategy      RWolves    CRWolves    RevWolves    CRevWolves    CRLWolves    AggroWolves
-------------------  ---------  ----------  -----------  ------------  -----------  -------------
Random                   0.603       0.062        0.088         0.07         0.086          0.011
L-Targets                0.681       0.119        0.175         0.116        0.116          0.026
CL-Targets               0.674       0.303        0.295         0.283        0.299          0.302
Trained-CRWolves         0.595       0.508        0.578         0.515        0.151          0.381
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tav_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in tav_replays]
rv_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in rv_replays]
rav_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in rav_replays]
cav_win_replays = [[r for r in replay if r[-1][&quot;winners&quot;] == Roles.VILLAGER] for replay in cav_replays]
</pre></div>
</div>
</div>
</div>
<section id="days-elapsed-before-a-villager-win">
<h4>Days elapsed before a villager win<a class="headerlink" href="#days-elapsed-before-a-villager-win" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>rv_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in rv_win_replays]
rav_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in rav_win_replays]
cav_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in cav_win_replays]
tav_days = [np.mean([villager_win[-1][&quot;day&quot;] for villager_win in replays]) for replays in tav_win_replays]

print(tabulate([[&#39;Random&#39;, *rv_days], 
                [&#39;L-Targets&#39;, *rav_days], 
                [&#39;CL-Targets&#39;, *cav_days], 
                [&#39;Trained-CRWolves&#39;, *tav_days]], 
               headers=[&quot;Villager Strategy&quot;, 
                        &quot;RWolves&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;,
                        &quot;CRLWolves&quot;,
                        &quot;AggroWolves&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Villager Strategy      RWolves    CRWolves    RevWolves    CRevWolves    CRLWolves    AggroWolves
-------------------  ---------  ----------  -----------  ------------  -----------  -------------
Random                 3.13856     3.52308      3.39831       3.43066      3.37647        2
L-Targets              3.17012     3.53333      3.53086       3.58373      3.46721        3.73913
CL-Targets             3.08504     3.46181      3.49338       3.4898       3.5189         3.47588
Trained-CRWolves       3.16931     3.21805      3.24783       3.26117      3.37306        3.26494
</pre></div>
</div>
</div>
</div>
<p>Days elapsed is a good litmus of whether or not a learning agent has learned to coordinate and finish games quicker. We can see that our  trained villagers were faster against CRWolves (which they were trained against), as well as against RevWolves, CRevWolves, CRLWolves, and AggroWolves. Random villagers only won a single game or two against the AggroWolves, making that data point very insignificant.</p>
</section>
<section id="days-between-wolf-executions">
<h4>Days between wolf executions<a class="headerlink" href="#days-between-wolf-executions" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># skipping random wolves because they may kill eachother during the night
tav_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in tav_win_replays[1:]]
rv_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in rv_win_replays[1:]]
rav_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in rav_win_replays[1:]]
cav_exec = [np.mean([b-a for a,b in [indicators._when_did_wolves_get_killed(vwin) for vwin in replays]]) for replays in cav_win_replays[1:]]

print(tabulate([[&#39;Random&#39;, *rv_exec], 
                [&#39;L-Targets&#39;, *rav_exec], 
                [&#39;CL-Targets&#39;, *cav_exec], 
                [&#39;Trained-CRWolves&#39;, *tav_exec]], 
               headers=[&quot;Villager Strategy&quot;, 
                        &quot;CRWolves&quot;, 
                        &quot;RevWolves&quot;,
                        &quot;CRevWolves&quot;,
                        &quot;CRLWolves&quot;,
                        &quot;AggroWolves&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Villager Strategy      CRWolves    RevWolves    CRevWolves    CRLWolves    AggroWolves
-------------------  ----------  -----------  ------------  -----------  -------------
Random                  1.64516      1.69318       1.65714      1.61628        1.81818
L-Targets               1.59664      1.66857       1.77586      1.67241        1.73077
CL-Targets              1.69307      1.72881       1.66784      1.60535        1.61921
Trained-CRWolves        1.51378      1.48097       1.4932       1.60265        1.41995
</pre></div>
</div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>RWolves were skipped simply because they would kill themselves sometimes at night, and this indicator relies on wolves being explicitly executed by villager consensus</p>
</div>
<p>This is yet another litmus test for coordination and learning, as we expect this number to be lower the more coordinated the trained agents are, and indeed it is. Interestingly, we also see that against AggroWolves, our trained villagers had the lowest values, indicating that the aggressive wolves were executed one after another the most often. This makes sense as aggressive wolves were targetting everyone, and would be the easiest to spot, however our trained agents were not trained against AggroWolves, so this finding promotes the idea that our trained agents generalized to somewhat consistent patterns.</p>
</section>
<section id="ties">
<h4>Ties<a class="headerlink" href="#ties" title="Permalink to this heading">#</a></h4>
<p>When it comes to ties, We can see that against CRWolves, ties in voting phases are lower than in accusation phases. If there was a tie in a prior accusation round, we can see that our trained agents only tie in subsequent voting rounds roughly a third of the time for CRWolves, RevWolves and CRevWolves, while having a much higher chance to tie against CRLWolves that have successfully tricked them, and against AggroWolves where there are a lot more targetting votes altogether.</p>
<p>Sometimes a wolf is one of the targets in a tie, and they may end up getting lucky. Our last column looks at the rates at which this wolf gets targetted in a subsequent voting round, and we can see that it is quite high, especially so for CLRWolves. Although they struggled against CLRWolves, when the trained villagers were sure of a target, they did not take any chances after a tie.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tie_info = [np.nanmean(np.stack([indicators._process_tie_info(indicators._game_tie_info(replay, voting_type=&quot;approval&quot;)) for replay in replays]), axis=0) for replays in tav_win_replays[1:]]
print(tabulate([[&#39;vs. CRWolves&#39;, *tie_info[0]], 
                [&#39;vs. RevWolves&#39;, *tie_info[1]], 
                [&#39;vs. CRevWolves&#39;, *tie_info[2]], 
                [&#39;vs. CRLWolves&#39;, *tie_info[3]],
                [&#39;vs. AggroWolves&#39;, *tie_info[4]]], 
               headers=[&quot;Trained-CRWolves&quot;, 
                        &quot;Tie in Accusation&quot;, 
                        &quot;Tie in Voting&quot;,
                        &quot;Tie in Voting given a  tie in accusation&quot;,
                        &quot;Likelihood of wolf targetted after a tie&quot;]))
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trained-CRWolves      Tie in Accusation    Tie in Voting    Tie in Voting given a  tie in accusation    Likelihood of wolf targetted after a tie
------------------  -------------------  ---------------  ------------------------------------------  ------------------------------------------
vs. CRWolves                   0.361959         0.330545                                    0.330735                                    0.649485
vs. RevWolvess                 0.383506         0.308968                                    0.303485                                    0.615789
vs. CRevWolves                 0.367476         0.315696                                    0.316737                                    0.52439
vs. CRLWolves                  0.35734          0.41777                                     0.420775                                    0.764706
vs. AggroWolves                0.394466         0.385389                                    0.406364                                    0.603175
</pre></div>
</div>
</div>
</div>
</section>
<section id="targetting-indicators">
<h4>Targetting Indicators<a class="headerlink" href="#targetting-indicators" title="Permalink to this heading">#</a></h4>
<p>Our indicators are averaged out and stacked across phases and days, and as a reminder, here they are:</p>
<ul class="simple">
<li><p>average target count</p></li>
<li><p>average like count</p></li>
<li><p>average neutral count</p></li>
<li><p>average self target</p></li>
<li><p>average self like</p></li>
<li><p>percentage of wolves in top targets</p></li>
<li><p>percentage of wolves in top likes</p></li>
<li><p>percent of votes targetting dead players</p></li>
<li><p>percent of votes targetting wolves by tracking:</p>
<ul>
<li><p>percent of votes targetting dead wolves</p></li>
<li><p>percent of votes targetting wolves that are still alive</p></li>
</ul>
</li>
<li><p>percent of likes towards villagers by tracking:</p>
<ul>
<li><p>percent of likes towards dead villagers</p></li>
<li><p>perceent of likes towards villagers that are still alive</p></li>
</ul>
</li>
<li><p>percent of likes towards wolves by tracking:</p>
<ul>
<li><p>percent of likes for dead wolves</p></li>
<li><p>percent of likes for wolves that are still alive</p></li>
</ul>
</li>
</ul>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>indicator_titles = [&quot;Avg Target Count&quot;,
                    &quot;Avg Like Count&quot;,
                    &quot;Avg Neutral Count&quot;,
                    &quot;Likelihood of Self Target&quot;,
                    &quot;Likelihood of Self Like&quot;,
                    &quot;Percent of Wolves in Top Targets&quot;,
                    &quot;Percent of Wolves in Top Likes&quot;,
                    &quot;Percent of votes Targetting Dead Players&quot;,
                    &quot;Percent of votes Targetting Dead Wolves&quot;,
                    &quot;Percent of votes Targetting Living Wolves&quot;,
                    &quot;Percent of Likes for Living Villagers&quot;,
                    &quot;Percent of Likes for Dead Villagers&quot;,
                    &quot;Percent of Likes for Dead Wolves&quot;,
                    &quot;Percent of Likes for Living Wolves&quot;,
                    ]

tav_avg_records = [indicators._game_avg_records(replays, indicators._approval_target_indicators) for replays in tav_win_replays]
tav_stacked = [np.stack(list(avg_records.values())) for avg_records in tav_avg_records]

rv_avg_records = [indicators._game_avg_records(replays, indicators._approval_target_indicators) for replays in rv_win_replays]
rv_stacked = [np.stack(list(avg_records.values())) for avg_records in rv_avg_records]

# https://www.heavy.ai/blog/12-color-palettes-for-telling-better-stories-with-your-data
day_colors = [&quot;#115f9a&quot;, &quot;#22a7f0&quot;, &quot;#76c68f&quot;, &quot;#c9e52f&quot;]

def plot_indicator(stacked_info, indicator_title, indicator_id, colors):
    plt.figure(figsize=(9,4))
    x_tick_labels = [&quot;Accusation&quot;, &quot;Accusation&quot;, &quot;Voting&quot;]
    for day, color in zip(range(stacked_info.shape[0]), colors):
        plt.plot(list(range(stacked_info.shape[1])), stacked_info[:,:,indicator_id][day], linewidth=2.0, color=color, label=f&#39;Day {day+1}&#39;)
        plt.xticks([0,1,2], x_tick_labels, rotation=40)
        plt.legend()
        plt.suptitle(indicator_title)
    plt.plot()


def plot_indicator_across_other_wolves(stacked_info, indicator_title, indicator_id):
    x_tick_labels = [&quot;Accusation&quot;, &quot;Accusation&quot;, &quot;Voting&quot;]
    fig, axs = plt.subplots(1, 4, sharey=&#39;row&#39;, figsize=(15,6))

    for day in range(stacked_info[0].shape[0]):
        for name, stacked in zip([&quot;Random&quot;, &quot;CRWolves&quot;, &quot;RevWolves&quot;, &quot;CRevWolves&quot;, &quot;CRLWolves&quot;, &quot;AggroWolves&quot;], stacked_info):
            axs[day].plot(list(range(stacked.shape[1])), stacked[:,:,indicator_id][day], linewidth=2.0, label=name)
            axs[day].set_xticks([0,1,2], x_tick_labels, rotation=40)
            axs[day].set_xlabel(f&#39;Day {day + 1}&#39;)

    fig.suptitle(f&#39;{indicator_title}&#39;)
    fig.legend([&quot;Random&quot;, &quot;CRWolves&quot;, &quot;RevWolves&quot;, &quot;CRevWolves&quot;, &quot;CRLWolves&quot;, &quot;AggroWolves&quot;], loc=&quot;upper right&quot;)
    fig.tight_layout()
</pre></div>
</div>
</div>
</details>
</div>
<section id="average-target-count">
<h5>Average Target Count<a class="headerlink" href="#average-target-count" title="Permalink to this heading">#</a></h5>
<p>Each agent is able to target everyone, so this indicator looks at the average amount of targets trained agents selected. Seeing this number go down as the day progresses is a good indicator that agents feel more sure about their targets. The more pronounced the slope, the more certainty the agents have.</p>
<p>An important note here is that every round we have <span class="math notranslate nohighlight">\(2\)</span> less players, so even with dead players, these agents are targetting roughly a third of total players.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[0], 0, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b5a27e890d753d4c64f965e5d01bd1356e4787defb7f4e01a6ec4896e1b5bd46.png" src="_images/b5a27e890d753d4c64f965e5d01bd1356e4787defb7f4e01a6ec4896e1b5bd46.png" />
</div>
</div>
<p>The same general behavior is exhibited regardless of the werewolves strategies. The increase on day <span class="math notranslate nohighlight">\(3\)</span> against CRLWolves we believe can be attributed to the confusion around those werewolves usage of random likes and neutrals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[0], 0)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9ff5099a1797f58d9686a5f4dce892a4eda60893f66a747915b891c71dc8d0a6.png" src="_images/9ff5099a1797f58d9686a5f4dce892a4eda60893f66a747915b891c71dc8d0a6.png" />
</div>
</div>
</section>
<section id="average-like-count">
<h5>Average Like Count<a class="headerlink" href="#average-like-count" title="Permalink to this heading">#</a></h5>
<p>Liking another agent does not provide agents with direct rewards, and is not mechanically relevant to elimination, so whereas humans would have used this indicator to highlight players we believe are more trustworthy, our PPO trained agents may use it in a very different way.</p>
<p>In contrast to targetting, agents seem to use more likes as the day progresses, possibly mirroring how a human would “like” other trusted humans.
This idea of trust is further reinforced in later indicators, where agents like who they believe are villagers, regardless of whether they are alive or dead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[1], 1, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ae1a34da43e303e43dec8939919d8990903bbe657c27b3c8ec2699a7e64022a9.png" src="_images/ae1a34da43e303e43dec8939919d8990903bbe657c27b3c8ec2699a7e64022a9.png" />
</div>
</div>
<p>This observed increase is also seen when playing other agents, with less likes towards CRLWolves and AggroWolves. We believe this strengthens the idea of trained agents liking other trusted players.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[1], 1)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/904cb2144a1649213545b5d710561293fc840f09bfc07e5e63ccd405327d5c53.png" src="_images/904cb2144a1649213545b5d710561293fc840f09bfc07e5e63ccd405327d5c53.png" />
</div>
</div>
</section>
<section id="average-neutral-count">
<h5>Average Neutral Count<a class="headerlink" href="#average-neutral-count" title="Permalink to this heading">#</a></h5>
<p>The third option an agent has in approval voting is to remain neutral towards another agent. This indicator tracks the average amount of neutrals an agent will use.</p>
<p>Much like “likes”, a neutral vote does not provide agents with direct rewards, and is not mechanically relevant to elimination, however the usage of neutral appears to be much more chaotic.</p>
<p>We believe that because targetting and liking are at the start and the end of our classification outputs, agents actually learned this ordinal representation, and maybe neutrals are a transient spot for other agents being shuffled between targets and likes, leading to chaotic looking graphs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[2], 2, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3c4a739a2e00160bc0d178fe25f7a515dd332b3af19600360def567c76d604ea.png" src="_images/3c4a739a2e00160bc0d178fe25f7a515dd332b3af19600360def567c76d604ea.png" />
</div>
</div>
<p>This chaotic usage is also seen in games against other policies. We had initially expectd that CRLWolves and Random wolves would have similar patterns, but even these differ quite a bit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[2], 2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a46cf7de043ed39b6a9f03faccba076a40dcaa2035fe9ccb52f644ebfbc3ae98.png" src="_images/a46cf7de043ed39b6a9f03faccba076a40dcaa2035fe9ccb52f644ebfbc3ae98.png" />
</div>
</div>
</section>
<section id="likelihood-of-self-targetting">
<h5>Likelihood of Self Targetting<a class="headerlink" href="#likelihood-of-self-targetting" title="Permalink to this heading">#</a></h5>
<p>This indicator tracks an agent’s propensity to target themselves, and seeing a downward trend indicates two things:</p>
<ol class="arabic simple">
<li><p>Early targetting may be used as a type of signal</p></li>
<li><p>Less targetting during voting rounds means the agent has learned it is not a good idea to self-target during an elimination round</p></li>
</ol>
<p>Targetting themselves during accusation rounds does not result in negative rewards, only if they do it during voting rounds. In addition, the surplus of targetting in approval voting leads to self-targetting not being as dangerous as in plurality.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[3], 3, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ba1db131eb006f006fb7d142bdc251c8ecf07d538f5c2ed64433e3611458b93f.png" src="_images/ba1db131eb006f006fb7d142bdc251c8ecf07d538f5c2ed64433e3611458b93f.png" />
</div>
</div>
<p>The same trend is exhibited with various werewolf policies, however AggroWolves stand out because it seems agents were able to realize it was more dangerous to self-target against these opponents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[3], 3)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2cd735b6efc04000a21e26650354e61a81ca6de3195d5e06da5acf64cecc5a87.png" src="_images/2cd735b6efc04000a21e26650354e61a81ca6de3195d5e06da5acf64cecc5a87.png" />
</div>
</div>
</section>
<section id="likelihood-of-self-liking">
<h5>Likelihood of Self Liking<a class="headerlink" href="#likelihood-of-self-liking" title="Permalink to this heading">#</a></h5>
<p>This indicator tracks an agent’s propensity to like themselves. Interestingly, agents are less likely to use it on themselves early on in the day. There could be many different reasons for this, however two stand out:</p>
<ol class="arabic simple">
<li><p>CRWolves always “like” other wolves, so maybe this is a behavior they want to do sparingly to not attract suspicion.</p></li>
<li><p>It could be how sure they are of the targetting profile they have chosen</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[4], 4, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/31119e698a9122c173e405a4f3056cd5b2fd1ecec26a762d5bd778aed74a099e.png" src="_images/31119e698a9122c173e405a4f3056cd5b2fd1ecec26a762d5bd778aed74a099e.png" />
</div>
</div>
<p>Seeing results against other werewolf policies solidifies this possibly being an assurance indicator, because against Random wolves and more prominently against CRWolves, this number dips the most before they make a more confident vote for the day</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[4], 4)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fcaf7d08001ed54f9a191022b5d5d578bfeabf5d2cd9ff7c78772c24f08deda3.png" src="_images/fcaf7d08001ed54f9a191022b5d5d578bfeabf5d2cd9ff7c78772c24f08deda3.png" />
</div>
</div>
</section>
<section id="percent-of-wolves-in-top-targets">
<h5>Percent of Wolves in Top Targets<a class="headerlink" href="#percent-of-wolves-in-top-targets" title="Permalink to this heading">#</a></h5>
<p>A dynamic <em>Top Targets</em> makes this indicator a little less compelling however it is easily interpretable. Top targets make the majority of targetting votes, and if wolves are a greater part of this, it means agents are able to better identify them.</p>
<p>Not only do they go up during phases in a day, each subsequent voting day has a higher share of top targets allocated to wolves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[5], 5, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fd8ffef9dcc84178ae17bbfee4c365e96545f5ee82d9be8b5b71b8675d8d308b.png" src="_images/fd8ffef9dcc84178ae17bbfee4c365e96545f5ee82d9be8b5b71b8675d8d308b.png" />
</div>
</div>
<p>Against different policies, there is quite a stark contrast between policies with random likes/neutrals (Random, CRLWolves) and those without. Truly random werewolves are barely in the majority, whereas CRLWolves barely make it to half of top targetting by the last day. This is in contrast with AggroWolves, which are the easiest to pick out, and make up a majority of the top votes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[5], 5)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f1d7326ad56d6891adacde652a159c389705488a1a0c9ce590437229d31bada8.png" src="_images/f1d7326ad56d6891adacde652a159c389705488a1a0c9ce590437229d31bada8.png" />
</div>
</div>
</section>
<section id="percent-of-wolves-in-top-likes">
<h5>Percent of Wolves in Top Likes<a class="headerlink" href="#percent-of-wolves-in-top-likes" title="Permalink to this heading">#</a></h5>
<p>This indicator strongly implies that trained agents will use likes to indicate trust in other agents, and as werewolves are suspected, confidence in them falls throughout the day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[6], 6, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/df9c1c8001fcec2d88d31c3b2f9afbb1112eb45538f9ce8c247426676c98e593.png" src="_images/df9c1c8001fcec2d88d31c3b2f9afbb1112eb45538f9ce8c247426676c98e593.png" />
</div>
</div>
<p>Much like how CRLWolves and Random wolves posed a problem to trained agents that could be seen in <em>werewolves in top targets</em>, we can see a parallel effect where wolves are also trusted more often against these policies, leading to more mistakes and lower win-rates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[6], 6)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/22e384c4dc28cefd4d4d7e84b80c0ff14532b9cf3d95a8df37459bf05a06fbc6.png" src="_images/22e384c4dc28cefd4d4d7e84b80c0ff14532b9cf3d95a8df37459bf05a06fbc6.png" />
</div>
</div>
</section>
<section id="percent-of-votes-targetting-dead-players">
<h5>Percent of Votes Targetting Dead Players<a class="headerlink" href="#percent-of-votes-targetting-dead-players" title="Permalink to this heading">#</a></h5>
<p>Targetting dead players only results in a minor negative reward, and is masked while determining group consensus so it poses minor risk to the trained agent.</p>
<p>There are also more dead players every day, so seeing this number go up is not a surprise. Much like in plurality, targets towards dead players could be one of the signals villagers use amongst themselves to try to identify eachother.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[7], 7, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e8fe75e32106c20915d8b4d5982d07fcbc136a5b282c13751d06146b6ed6bbb6.png" src="_images/e8fe75e32106c20915d8b4d5982d07fcbc136a5b282c13751d06146b6ed6bbb6.png" />
</div>
</div>
<p>Against CRWolves and Random wolves, this number seems to be the highest, and against AggroWolves, the lowest. It could be that less signaling needs to occur when werewolves are easier to spot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[7], 7)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1df75df2404e108cf6598e00cc19467395fa50b39044b276b5763937279a5b4b.png" src="_images/1df75df2404e108cf6598e00cc19467395fa50b39044b276b5763937279a5b4b.png" />
</div>
</div>
</section>
<section id="percent-of-votes-targetting-dead-wolves">
<h5>Percent of Votes Targetting Dead Wolves<a class="headerlink" href="#percent-of-votes-targetting-dead-wolves" title="Permalink to this heading">#</a></h5>
<p>On the heels of targets towars dead players, this indicator focuses soley on targets towards dead wolves. It is a small percentage out of the total targets towards dead players, and does not change much throughout the day other than increasing each day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[8], 8, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/89e3ce59d52340172986b597151f7d2d51923ffba4b0f454238dd49cdcd81f8a.png" src="_images/89e3ce59d52340172986b597151f7d2d51923ffba4b0f454238dd49cdcd81f8a.png" />
</div>
</div>
<p>Werewolves randomly assigning likes and neutrals seem to pose problems for our trained wolves, so comparing them with the other policies is usally a good idea. In this case it would seem as though trained agents target dead CRLWolves and Random werewolves less.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[8], 8)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0e4fdb415b721252ad35722f8ee6057891610cb03d0f896018c7013fc0e32377.png" src="_images/0e4fdb415b721252ad35722f8ee6057891610cb03d0f896018c7013fc0e32377.png" />
</div>
</div>
</section>
<section id="percent-of-votes-targetting-living-wolves">
<h5>Percent of Votes Targetting Living Wolves<a class="headerlink" href="#percent-of-votes-targetting-living-wolves" title="Permalink to this heading">#</a></h5>
<p>A positive trend throughout the day and culminating with the highest amount during votes indicate that werewolves are being identified and properly targetted by trained agents. Why each day’s first accusatsion doesn’t target them must be a strategy trained agents employ to signal to other trained agents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[9], 9, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/718f0651b546f4efc55fdb702f5695fa2d2780aa9c05fb876e028ab25fb5446e.png" src="_images/718f0651b546f4efc55fdb702f5695fa2d2780aa9c05fb876e028ab25fb5446e.png" />
</div>
</div>
<p>We can see that AggroWolves, which are the easiest to identify, are targetted more than any other werewolf policies, even moreso than the ones (CRWolves) the agents were trained against. In contrast, Random Wolves and CRLWolves are targetted the least, as these confuse our trained agents the most.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[9], 9)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/765c0c3acc1382034718f2f8fb8467249ecc707bdf48e9b158be739a4ed27db9.png" src="_images/765c0c3acc1382034718f2f8fb8467249ecc707bdf48e9b158be739a4ed27db9.png" />
</div>
</div>
</section>
<section id="percent-of-likes-towards-living-villagers">
<h5>Percent of Likes Towards Living Villagers<a class="headerlink" href="#percent-of-likes-towards-living-villagers" title="Permalink to this heading">#</a></h5>
<p>This is an interesting indicator, because it shows that likes towards actual villagers do not change much throughout a day, and decreases each day. This decrease is proportional to the increase in likes for dead villagers, indicating that agents will continue to like dead players they considered true villagers, possibly to signal their knowledge to others.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[10], 10, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c0d5cc264ecefeb7816bb13caf794587ba084bed7c5bb2d0f8ff2abb60f92fb4.png" src="_images/c0d5cc264ecefeb7816bb13caf794587ba084bed7c5bb2d0f8ff2abb60f92fb4.png" />
</div>
</div>
<p>The same trend can be seen against all other werewolf policies, with no big differences for us to focus on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[10], 10)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d9ab1789558d7b4a8b8b8f63ea6d42d24dd5641cd856dceea9caf2e6f2cd8c3a.png" src="_images/d9ab1789558d7b4a8b8b8f63ea6d42d24dd5641cd856dceea9caf2e6f2cd8c3a.png" />
</div>
</div>
</section>
<section id="percent-of-likes-for-dead-villagers">
<h5>Percent of Likes for Dead Villagers<a class="headerlink" href="#percent-of-likes-for-dead-villagers" title="Permalink to this heading">#</a></h5>
<p>As villagers die, trained agents still use a “like” on them to likely signal their knowledge and beliefs in regards to the roles of other agents. It was good to have this indicator and the likes towards living villagers to be able to observe and more confidently classify this behavior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[11], 11, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1fb77066d9fe52a5a7d9c84025d71f05a883e024a2553633e8c0efc1f26bfe8a.png" src="_images/1fb77066d9fe52a5a7d9c84025d71f05a883e024a2553633e8c0efc1f26bfe8a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[11], 11)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ac9196598bef921401aec9f30e3d9c595cf76fdbcb810e1b0e8160348b7c7f74.png" src="_images/ac9196598bef921401aec9f30e3d9c595cf76fdbcb810e1b0e8160348b7c7f74.png" />
</div>
</div>
</section>
<section id="percent-of-likes-for-dead-wolves">
<h5>Percent of Likes for Dead Wolves<a class="headerlink" href="#percent-of-likes-for-dead-wolves" title="Permalink to this heading">#</a></h5>
<p>In contrast to likes towards villagers, likes towards wolves are low, and especially so towards dead wolves. This indicator solidifies our belief that likes have been learned to represent trust regardless of whether a player is alive or dead, and such low numbers for dead wolves indicate this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[12], 12, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7b0aca77c6ee8cb69c8933f5ef583cbb6d4450c08f7fdd41881b5382ee76873d.png" src="_images/7b0aca77c6ee8cb69c8933f5ef583cbb6d4450c08f7fdd41881b5382ee76873d.png" />
</div>
</div>
<p>AggroWolves being the lowest so consistently across days highlight how likelier they are to be identified by trained agents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[12], 12)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6c005abaa8ce9430c8232986fe229c04d62db07b6180c99de612c7950ea72a43.png" src="_images/6c005abaa8ce9430c8232986fe229c04d62db07b6180c99de612c7950ea72a43.png" />
</div>
</div>
</section>
<section id="percent-of-likes-towards-living-wolves">
<h5>Percent of Likes Towards Living Wolves<a class="headerlink" href="#percent-of-likes-towards-living-wolves" title="Permalink to this heading">#</a></h5>
<p>This indicator shows two very critical things:</p>
<ol class="arabic simple">
<li><p>Wolves are not as liked as villagers, strengthening the case that likes are used for trust</p></li>
<li><p>Likes towards werewolves decrease both throughout and across days, with almost no misplaced trust in any wolves by the end of the game.</p></li>
</ol>
<p>Now using this information, we can say that likes for dead wolves don’t change as much because trained agents do not bother with them after they are dead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator(tav_stacked[1], indicator_titles[13], 13, day_colors)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4d7ba45a3678ea60b8a5462ae036bb561d619aa61f32ad93d59bc0bec6191b33.png" src="_images/4d7ba45a3678ea60b8a5462ae036bb561d619aa61f32ad93d59bc0bec6191b33.png" />
</div>
</div>
<p>These two points come across quite clearly against different werewolf policies, with our usual tricky Random and CRLWolves having the most trust, and AggroWolves having the least.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_indicator_across_other_wolves(tav_stacked, indicator_titles[13], 13)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e321c99385b1b9c28f3ee0eb95fcd7e392ebf4808e550c625c7fa36eb077ab4d.png" src="_images/e321c99385b1b9c28f3ee0eb95fcd7e392ebf4808e550c625c7fa36eb077ab4d.png" />
</div>
</div>
</section>
</section>
</section>
</section>
<span id="document-dicsussion"></span><div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">voting_games.werewolf_env_v0</span> <span class="kn">import</span> <span class="n">pare</span><span class="p">,</span> <span class="n">plurality_env</span><span class="p">,</span> <span class="n">Roles</span><span class="p">,</span> <span class="n">Phase</span>
<span class="kn">from</span> <span class="nn">notebooks.learning_agents.models</span> <span class="kn">import</span> <span class="n">ActorCriticAgent</span>
<span class="kn">from</span> <span class="nn">notebooks.learning_agents.utils</span> <span class="kn">import</span> <span class="n">play_static_game</span><span class="p">,</span> <span class="n">play_recurrent_game</span>
<span class="kn">from</span> <span class="nn">notebooks.learning_agents.static_agents</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">random_plurality_wolf</span><span class="p">,</span> 
    <span class="n">random_approval_wolf</span><span class="p">,</span>
    <span class="p">)</span>
<span class="kn">import</span> <span class="nn">notebooks.learning_agents.stats</span> <span class="k">as</span> <span class="nn">indicators</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="discussion">
<h2>Discussion<a class="headerlink" href="#discussion" title="Permalink to this heading">#</a></h2>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h3>
<section id="inconsistent-training">
<h4>Inconsistent Training<a class="headerlink" href="#inconsistent-training" title="Permalink to this heading">#</a></h4>
<p>Reinforcement learning is a hard problem, and MARL even more so given its dynamic nature. To train our agents, we used a policy gradient method (PPO) that consistently improved our agents, but almost always had some type of divergence or weight collapse. In Sutton and Barto<a class="footnote-reference brackets" href="#sutton-barto-book" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, they refer to a combination of issues -<strong>deadly triad</strong>- that when used together, lead to divergence more often than not. These issues are:</p>
<ul class="simple">
<li><p>Using non-linear function approximations. <em>Our neural network</em></p></li>
<li><p>Bootstraping our estimated values. <em>Our policy is changing constantly, along with our value approximation</em></p></li>
<li><p>Off-Policy Training. <em>The one thing we do not do in this case!</em></p></li>
</ul>
<p>The difficulty in calculating good estimates of the policy gradient is compounded by the stochasticity of our MARL environment <span id="id2">[<a class="reference internal" href="intro.html#id5" title="Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In International Conference on Machine Learning. 2002. URL: https://api.semanticscholar.org/CorpusID:31442909.">KL02</a>]</span>. If we get a couple unfortunate episodes with bad estimates, our parameters may go in a poor direction leading to policy collapse and a long to possibly never recovery time. Multiple such events can be seen during our training. Empirically, when training our agents for approximately 500 update steps, they run to completion only roughly <span class="math notranslate nohighlight">\(10\%\)</span> of the time. A collapse of our weights is almost always the termination factor of training. Despite this we do get decent results in the meantime, and would like to make training more consistent.</p>
<p>Some ideas we believe may help and are worth trying:</p>
<ul class="simple">
<li><p>Using decaying rates for our learning rate</p></li>
<li><p>Use running norms for observations and rewards</p></li>
<li><p>futher explore gradient clipping</p></li>
<li><p>any divisions should also add an <span class="math notranslate nohighlight">\(\epsilon\)</span> of <span class="math notranslate nohighlight">\(1e-5\)</span> to begin with</p></li>
<li><p>clamp logarithmic values between <code class="docutils literal notranslate"><span class="pre">-np.log(1e-5)</span> <span class="pre">,</span> <span class="pre">np.log(1e-5))</span></code> to begin with</p></li>
<li><p>split critic and actor networks</p>
<ul>
<li><p>have a higher learning rate for the critic</p></li>
</ul>
</li>
<li><p>vary replay buffer sizes</p></li>
<li><p>vary batch sizes</p></li>
<li><p>change optimizer</p></li>
<li><p>simplify model structure</p></li>
</ul>
<p>These have been collected from many blog posts, reddit posts, and work done in <span id="id3">[<a class="reference internal" href="intro.html#id36" title="Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in On-Policy reinforcement learning? a Large-Scale empirical study. arXiv preprint arXiv:2006.05990, June 2020. arXiv:2006.05990.">ARStanczyk+20</a>, <a class="reference internal" href="intro.html#id44" title="Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In AAAI Conference on Artificial Intelligence. 2017. URL: https://api.semanticscholar.org/CorpusID:4674781.">HIB+17</a>]</span>. We leave this exploration to future work, as it is not the direct scope of the project, but would help with consistency.</p>
</section>
<section id="training-time">
<h4>Training Time<a class="headerlink" href="#training-time" title="Permalink to this heading">#</a></h4>
<p>Playing an approval game <a class="reference internal" href="intro.html#training-time"><span class="std std-ref">took roughly twice as long as a</span></a> a plurality game. If agent training went to completion (which only occured roughly <span class="math notranslate nohighlight">\(10\%\)</span> of the time), approval would go for roughly <span class="math notranslate nohighlight">\(5\)</span> hours, whereas plurality took <span class="math notranslate nohighlight">\(2.5\)</span>. Training plurality agents also collapsed much quicker than training approval agents. While it was not immediately clear why this would be, we believe it might be caused by our observation representation: some integers represent ordinal relationships while others do not, and this may cause problems for our model. Any learning happening was likely in spite of this possibly poor representation choice. All the training was done on a CPU (i7-9700k, 32G of RAM), as there were issues with passing the GPU through the WSL (Windows subsystem for Linux) and docker containers on our machine.</p>
</section>
</section>
<section id="agents-learning-approval-and-plurality-mechanics">
<h3>Agents learning Approval and Plurality Mechanics<a class="headerlink" href="#agents-learning-approval-and-plurality-mechanics" title="Permalink to this heading">#</a></h3>
<p>We were able to train both approval and plurality based agents to perform better than random policy villagers, even ones that coordinated in what would be a game breaking way. Despite the challenges with the PPO training itself, it was clear that agents trained in the approval environment were consistently reaching higher average win-rates.</p>
<p>When it came down to what behaviors they seemed to have learnt, and how they went about executing them in-game, the expressability of our enhanced approval mechanism allowed agents to openly share their beliefs throughout accusations, while plurality agents had to figure out how to signal intent in more complicated ways during accusation phases. We found plurality agents that performed the best seemed to use their accusation votes in a more communicative way, learning to overcome the reward penalty as there was no mechanical one during accusation phases. It is probably why higher win-rates were more challenging for plurality agents to learn. They had to figure out how to superimpose intent and also synthesize it from others. On the other hand, trained approval agents consistently learned the ordinality of dissaprovals, neutrals and approvals, along with using approvals to indicate trust and to some extend, trust others with whom they shared a trusted agent.</p>
<p>Both plurality and approval indicators when viewed holistically provided strong evidence for the behaviors we identified, and while analyzing them, we realized that being able to view dynamics of changing votes would have presented even more compelling proof. We leave creating indicators for changes between approval, neutral and dissaproval to future work.</p>
<p>Experimentally, it was found that in approval voting scenarios, most voters will pick a small amount of candidates <span id="id4">[<a class="reference internal" href="intro.html#id41" title="Jean-François Laslier and M Remzi Sanver. Handbook on approval voting. Springer Science &amp; Business Media, 2010.">LS10</a>, <a class="reference internal" href="intro.html#id42" title="Jean-François Laslier and Karine Van der Straeten. Une expérience de vote par assentiment lors de l’élection présidentielle française de 2002. Revue française de science politique, 54(1):99–130, 2004.">LVdS04</a>]</span> relative to the full candidate list<a class="footnote-reference brackets" href="#approval-voting-avg-targets" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. This was empirically observed in our werewolf game, albeit our claim here is not truly tested. If this number changed at a proportional rate to the amount of players, we could make it a stronger claim.</p>
<section id="reward-shaping">
<h4>Reward shaping<a class="headerlink" href="#reward-shaping" title="Permalink to this heading">#</a></h4>
<p>We based our reward function and values on prior work in werewolf, however shaping behavior through rewards is challenging and not something prior works considered. For approval agents, we had no direct rewards for liking or feeling neutral about other agents, and there was no mechanical incentive either, however they learned to use them to implicitly communicate in a way that was interpretable to us. Two ideas branch out from this finding:</p>
<ol class="arabic simple">
<li><p>One would be to have a derived reward function using Inverse Reinforcement Learning (IRL) by using human replays. Other than specific communication rounds before villager or werewolf voting rounds, no other work has implemented interative voting phases, so we would have to get humans to play and generate these replays for us.</p></li>
<li><p>Adding more complex rewarding logic to see if we can force certain behavior to be learned.</p></li>
</ol>
<p>In plurality voting, the couple of trained agents that were able to achieve higher win-rates did so by discovering an ability to superimpose intent in their targetting. Our indicators highlighted targetting dead players during accusation as a possible way to signal something about themselves or their intent. This is more complex behavior, and they were likely able to learn this due to the fact that accusation phases did not penalize or reward targetting dead players in any way. By being heavy handed with penalties, this behavior may never have been discovered.</p>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="sutton-barto-book" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a></p>
</aside>
<aside class="footnote brackets" id="approval-voting-avg-targets" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://electionscience.org/commentary-analysis/super-tuesday-deep-voting-methods-dive/">https://electionscience.org/commentary-analysis/super-tuesday-deep-voting-methods-dive/</a></p>
</aside>
</section>
</section>
</section>
<span id="document-conclusion-future"></span><section class="tex2jax_ignore mathjax_ignore" id="conclusion-and-future-directions">
<h2>Conclusion and Future Directions<a class="headerlink" href="#conclusion-and-future-directions" title="Permalink to this heading">#</a></h2>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h3>
<p>Hidden Role games allow for rich interactions between competing groups of players with varying access to information. This uneven playing field leads to uneasy cooperatiion and perverse attempts at deception and guile. The identification of traitors and allies by the uninformed majority player group has so far focused on their communication, overlooking the powerful contribution voting mechanisms play in revealing the true intentions of their voters. Our approval voting mechanism is a novel twist to the voting mechanics of werewolf, and hidden role games in general. We were able train agents to play successfully, and in tandem with our heuristic indicators, were able to classify their voting behaviors.</p>
<p>In a recent case for approval voting <span id="id1">[<a class="reference internal" href="intro.html#id43" title="Aaron Hamlin and Whitney Hua. The case for approval voting. Constitutional Political Economy, 34:335 - 345, 2022. URL: https://api.semanticscholar.org/CorpusID:254913046.">HH22</a>]</span>, albeit in an elector setting, the arguments were multi-faceted. It is simplistic enough to supercede plurality, with almost no additional mental load on voters, while also doing a better job at selecting a strong winner and has been proven to be able to select Condorcet winners. It also has a higher accuracy relative to honest assessments of voters. Our project extends this claim to hidden role games, and gives a less informed majority a better chance against an informed antagonistic minority group.</p>
<p>Given our findings and our easy to use and up to date environment, we hope future researchers can extend or incorporate ideas found here in their own work.</p>
</section>
<section id="future-directions">
<h3>Future Directions<a class="headerlink" href="#future-directions" title="Permalink to this heading">#</a></h3>
<p>There are many different avenues that can be taken but not to limited extending the environment with different voting mechanisms, exploring and creating more indicators and focusing on the neural network model architecture and associated hyperparameter choices.
We list out a couple of ideas in no particular order:</p>
<ul class="simple">
<li><p>What are optimal strategies in these self-playing scenarios where wolves and villagers learn to fool and indentify themselves? What voting mechanisms make this easier for villagers and which ones give werewolves an even greater advantage?</p></li>
<li><p>How do we improve our observation and state space to produce more robust and consistent training?</p></li>
<li><p>Make the environment work well with a training framework such as Tianshou<a class="footnote-reference brackets" href="#tianshou" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> so more MARL algorithms can be employed and tested.</p></li>
<li><p>Add roles that have access to more information, or have different actions available to them.</p></li>
<li><p>Extend the game to allow explicit communication.</p>
<ul>
<li><p>How would this impact the use of signaling in voting? Do they work in tandem, or are there novel interactions to bypass detection?</p></li>
</ul>
</li>
<li><p>How do these findings scale with larger game sizes?</p></li>
<li><p>Mix different villager and werewolf policies in a game, and see how trained agents perform. Also train agents in these mixed settings.</p></li>
<li><p>Adding more indicators</p>
<ul>
<li><p>Measuring commitment. how likely a villager is to stay with a vote throughout phases.</p></li>
<li><p>Dynamics of how a vote transitions from approval to neutral to dissaproval.</p></li>
</ul>
</li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="tianshou" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p><a class="github reference external" href="https://github.com/thu-ml/tianshou">thu-ml/tianshou</a></p>
</aside>
</section>
</section>
<span id="document-bibliography"></span><section class="tex2jax_ignore mathjax_ignore" id="bibliography">
<h2>Bibliography<a class="headerlink" href="#bibliography" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id1">
<div class="citation" id="id17" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>AGP17<span class="fn-bracket">]</span></span>
<p>Stéphane Airiau, Umberto Grandi, and Filipo Studzinski Perotto. Learning agents for iterative voting. In <em>Algorithmic Decision Theory</em>, 139–152. Springer International Publishing, 2017.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ARStanczyk+20<span class="fn-bracket">]</span></span>
<p>Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters in On-Policy reinforcement learning? a Large-Scale empirical study. <em>arXiv preprint arXiv:2006.05990</em>, June 2020. <a class="reference external" href="https://arxiv.org/abs/2006.05990">arXiv:2006.05990</a>.</p>
</div>
<div class="citation" id="id49" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>ARK15<span class="fn-bracket">]</span></span>
<p>Amos Azaria, Ariella Richardson, and Sarit Kraus. An agent for deception detection in discussion based environments. In <em>Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</em>, 218–227. 2015.</p>
</div>
<div class="citation" id="id4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BF78<span class="fn-bracket">]</span></span>
<p>Steven J Brams and Peter C Fishburn. Approval voting. <em>American Political Science Review</em>, 72(3):831–847, 1978.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BGI21<span class="fn-bracket">]</span></span>
<p>Nicolo' Brandizzi, Davide Grossi, and Luca Iocchi. Rlupus: cooperation through emergent communication in the werewolf social deduction game. <em>ArXiv</em>, 2021.</p>
</div>
<div class="citation" id="id45" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BEM06<span class="fn-bracket">]</span></span>
<p>Mark Braverman, Omid Etesami, and Elchanan Mossel. Mafia: a theoretical study of players and coalitions in a partial information environment. <em>Annals of Applied Probability</em>, 18:825–846, 2006. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:14668989">https://api.semanticscholar.org/CorpusID:14668989</a>.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>BPSzepesvaryTasnadi22<span class="fn-bracket">]</span></span>
<p>Dávid Burka, Clemens Puppe, László Szepesváry, and Attila Tasnádi. Voting: a machine learning approach. <em>European Journal of Operational Research</em>, 299(3):1003–1017, 2022.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CYS22<span class="fn-bracket">]</span></span>
<p>Siqi Chen, Yang Yang, and Ran Su. Deep reinforcement learning with emergent communication for coalitional negotiation games. <em>Math. Biosci. Eng</em>, 19(5):4592–4609, 2022.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>CB22<span class="fn-bracket">]</span></span>
<p>Christopher T Conner and Nicholas M Baxter. Are you a werewolf? teaching symbolic interaction theory through game play. <em>Teach. Sociol.</em>, 50(1):17–27, January 2022.</p>
</div>
<div class="citation" id="id20" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DHB+20<span class="fn-bracket">]</span></span>
<p>Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R. McKee, Joel Z. Leibo, K. Larson, and Thore Graepel. Open problems in cooperative ai. <em>ArXiv</em>, 2020. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:229220772">https://api.semanticscholar.org/CorpusID:229220772</a>.</p>
</div>
<div class="citation" id="id8" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>dFatimaL17<span class="fn-bracket">]</span></span>
<p>Maria de Fátima Loureiro. A natural language capable agent to play a werewolf or mafia game. Master's thesis, University of Lisbon, 2017. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:216077572">https://api.semanticscholar.org/CorpusID:216077572</a>.</p>
</div>
<div class="citation" id="id16" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Dod19<span class="fn-bracket">]</span></span>
<p>A Zorica Dodevska. Computational social choice and challenges of voting in multi-agent systems. <em>Tehnika</em>, 2019.</p>
</div>
<div class="citation" id="id40" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EM18<span class="fn-bracket">]</span></span>
<p>Markus Eger and Chris Martens. Keeping the story straight: a comparison of commitment strategies for a social deduction game. <em>AIIDE</em>, 14(1):24–30, September 2018.</p>
</div>
<div class="citation" id="id3" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FFA+18<span class="fn-bracket">]</span></span>
<p>Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In <em>Proceedings of the AAAI conference on artificial intelligence</em>, volume 32. 2018.</p>
</div>
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HMI19<span class="fn-bracket">]</span></span>
<p>Makoto Hagiwara, Ahmed Moustafa, and Takayuki Ito. Using q-learning and estimation of role in werewolf game. In <em>Proceedings of the Annual Conference of JSAI 33rd (2019)</em>, 2O5E303–2O5E303. The Japanese Society for Artificial Intelligence, 2019.</p>
</div>
<div class="citation" id="id43" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HH22<span class="fn-bracket">]</span></span>
<p>Aaron Hamlin and Whitney Hua. The case for approval voting. <em>Constitutional Political Economy</em>, 34:335 – 345, 2022. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:254913046">https://api.semanticscholar.org/CorpusID:254913046</a>.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HIB+17<span class="fn-bracket">]</span></span>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In <em>AAAI Conference on Artificial Intelligence</em>. 2017. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:4674781">https://api.semanticscholar.org/CorpusID:4674781</a>.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HIT+16<span class="fn-bracket">]</span></span>
<p>Yuya Hirata, Michimasa Inaba, Kenichi Takahashi, Fujio Toriumi, Hirotaka Osawa, Daisuke Katagami, and Kousuke Shinoda. Werewolf game modeling using action probabilities based on play log analysis. In <em>Computers and Games</em>, 103–114. Springer International Publishing, 2016.</p>
</div>
<div class="citation" id="id7" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HS97<span class="fn-bracket">]</span></span>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural computation</em>, 9(8):1735–1780, 1997.</p>
</div>
<div class="citation" id="id47" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>IZD22<span class="fn-bracket">]</span></span>
<p>Samee Ibraheem, Gaoyue Zhou, and John DeNero. Putting the con in context: identifying deceptive actors in the game of mafia. 2022. <a class="reference external" href="https://arxiv.org/abs/2207.02253">arXiv:2207.02253</a>.</p>
</div>
<div class="citation" id="id5" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KL02<span class="fn-bracket">]</span></span>
<p>Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning. In <em>International Conference on Machine Learning</em>. 2002. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:31442909">https://api.semanticscholar.org/CorpusID:31442909</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KTI+14<span class="fn-bracket">]</span></span>
<p>Daisuke Katagami, Shono Takaku, Michimasa Inaba, Hirotaka Osawa, Kosuke Shinoda, Junji Nishino, and Fujio Toriumi. Investigation of the effects of nonverbal information on werewolf. In <em>2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)</em>, 982–987. IEEE, 2014.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KOT+19<span class="fn-bracket">]</span></span>
<p>Shoma Kato, Tomoya Okumura, Itsuki Toda, Takanori Fukui, Kazunori Iwata, and Nobuhiro Ito. Consideration of the essential topics for role estimation for aiwolf. In <em>2019 6th International Conference on Computational Science/Intelligence and Applied Informatics (CSII)</em>, volume, 72–77. 2019. <a class="reference external" href="https://doi.org/10.1109/CSII.2019.00020">doi:10.1109/CSII.2019.00020</a>.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KDuenezGuzmanM+22<span class="fn-bracket">]</span></span>
<p>Kavya Kopparapu, Edgar A. Duéñez-Guzmán, Jayd Matyas, Alexander Sasha Vezhnevets, John P. Agapiou, Kevin R. McKee, Richard Everett, Janusz Marecki, Joel Z. Leibo, and Thore Graepel. Hidden agenda: a social deduction game with diverse learned equilibria. <em>ArXiv</em>, 2022. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:245769696">https://api.semanticscholar.org/CorpusID:245769696</a>.</p>
</div>
<div class="citation" id="id41" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LS10<span class="fn-bracket">]</span></span>
<p>Jean-François Laslier and M Remzi Sanver. <em>Handbook on approval voting</em>. Springer Science &amp; Business Media, 2010.</p>
</div>
<div class="citation" id="id42" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LVdS04<span class="fn-bracket">]</span></span>
<p>Jean-François Laslier and Karine Van der Straeten. Une expérience de vote par assentiment lors de l’élection présidentielle française de 2002. <em>Revue française de science politique</em>, 54(1):99–130, 2004.</p>
</div>
<div class="citation" id="id15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LGleauMLR20<span class="fn-bracket">]</span></span>
<p>Tangui Le Gléau, Xavier Marjou, Tayeb Lemlouma, and Benoit Radier. Multi-agents ultimatum game with reinforcement learning. In <em>Highlights in Practical Applications of Agents, Multi-Agent Systems, and Trust-worthiness. The PAAMS Collection</em>, 267–278. Springer International Publishing, 2020.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LSCN22<span class="fn-bracket">]</span></span>
<p>Olaf Lipinski, Adam J. Sobey, F. Cerutti, and T. Norman. E mergent p assword s ignalling in the g ame of w erewolf. In <em>EmeCom Workshop, ICLR 2022</em>. 2022. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:248983549">https://api.semanticscholar.org/CorpusID:248983549</a>.</p>
</div>
<div class="citation" id="id14" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MOI20<span class="fn-bracket">]</span></span>
<p>Natsuki Matsunami, Shun Okuhara, and Takayuki Ito. Agents that learn to vote for a joint action through Multi-Agent reinforcement learning. In <em>2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI)</em>, 832–833. September 2020.</p>
</div>
<div class="citation" id="id48" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Migdal10<span class="fn-bracket">]</span></span>
<p>Piotr Migdał. A mathematical model of the mafia game. <em>arXiv preprint arXiv:1009.1031</em>, 2010.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NO16<span class="fn-bracket">]</span></span>
<p>Ema Nishizaki and Tomonobu Ozaki. Behavior analysis of executed and attacked players in werewolf game by ilp. In <em>ILP (Short Papers)</em>, 48–53. 2016.</p>
</div>
<div class="citation" id="id18" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PFV07<span class="fn-bracket">]</span></span>
<p>Ioannis Partalas, Ioannis Feneris, and Ioannis Vlahavas. Multi-agent reinforcement learning using strategies and voting. In <em>19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)</em>, volume 2, 318–324. October 2007.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PPZP22<span class="fn-bracket">]</span></span>
<p>Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In <em>The Eleventh International Conference on Learning Representations</em>. 2022.</p>
</div>
<div class="citation" id="id12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>PPZP23<span class="fn-bracket">]</span></span>
<p>Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory gym: partially observable challenges to memory-based agents. In <em>International Conference on Learning Representations</em>. 2023. URL: <a class="reference external" href="https://openreview.net/forum?id=jHc8dCx6DDr">https://openreview.net/forum?id=jHc8dCx6DDr</a>.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Rei20<span class="fn-bracket">]</span></span>
<p>Jackson T Reinhardt. Competing in a complex hidden role game with information set monte carlo tree search. <em>ArXiv</em>, 2020. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:218628845">https://api.semanticscholar.org/CorpusID:218628845</a>.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>RKKI22<span class="fn-bracket">]</span></span>
<p>Hong Ri, Xiaohan Kang, Mohd Nor Akmal Khalid, and Hiroyuki Iida. The dynamics of minority versus majority behaviors: a case study of the mafia game. <em>Information</em>, 13(3):134, March 2022.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SML+15<span class="fn-bracket">]</span></span>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. <em>arXiv preprint arXiv:1506.02438</em>, 2015.</p>
</div>
<div class="citation" id="id37" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SWD+17<span class="fn-bracket">]</span></span>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. <em>arXiv preprint arXiv:1707.06347</em>, 2017.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SKWPT19<span class="fn-bracket">]</span></span>
<p>Jack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend and foe in multi-agent games. <em>Adv. Neural Inf. Process. Syst.</em>, 2019.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SK21<span class="fn-bracket">]</span></span>
<p>Yingxue Sun and Tomoyuki Kaneko. Prediction of werewolf players by sentiment analysis of game dialogue in japanese. In <em>Proc. of the 26th Game Programming Workshop 2021</em>, 186–191. 2021.</p>
</div>
<div class="citation" id="id2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TBG+21<span class="fn-bracket">]</span></span>
<p>J Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth Hari, Ryan Sullivan, Luis S Santos, Clemens Dieffendahl, Caroline Horsch, Rodrigo Perez-Vicente, and others. Pettingzoo: gym for multi-agent reinforcement learning. <em>Advances in Neural Information Processing Systems</em>, 34:15032–15043, 2021.</p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TOI+17<span class="fn-bracket">]</span></span>
<p>Fujio Toriumi, Hirotaka Osawa, Michimasa Inaba, Daisuke Katagami, Kosuke Shinoda, and Hitoshi Matsubara. Ai wolf contest—development of game ai using collective intelligence—. In <em>Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5</em>, 101–115. Springer, 2017.</p>
</div>
<div class="citation" id="id6" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TTK+23<span class="fn-bracket">]</span></span>
<p>Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium. March 2023. URL: <a class="reference external" href="https://zenodo.org/record/8127025">https://zenodo.org/record/8127025</a> (visited on 2023-07-08), <a class="reference external" href="https://doi.org/10.5281/zenodo.8127026">doi:10.5281/zenodo.8127026</a>.</p>
</div>
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Vel21<span class="fn-bracket">]</span></span>
<p>Georgi Ventsislavov Velikov. <em>RLereWolf–Reinforcement Learning Agent Development Framework For The Social Deduction Game Werewolf</em>. PhD thesis, University of Aberdeen, 2021.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>WK18<span class="fn-bracket">]</span></span>
<p>Tianhe Wang and Tomoyuki Kaneko. Application of deep reinforcement learning in werewolf game agents. In <em>2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)</em>, 28–33. November 2018.</p>
</div>
<div class="citation" id="id13" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>XDW+20<span class="fn-bracket">]</span></span>
<p>Yue Xu, Zengde Deng, Mengdi Wang, Wenjun Xu, Anthony Man-Cho So, and Shuguang Cui. Voting-based multiagent reinforcement learning for intelligent iot. <em>IEEE Internet of Things Journal</em>, 8:2681–2693, 2020. URL: <a class="reference external" href="https://api.semanticscholar.org/CorpusID:212954448">https://api.semanticscholar.org/CorpusID:212954448</a>.</p>
</div>
<div class="citation" id="id46" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Yao08<span class="fn-bracket">]</span></span>
<p>Erlin Yao. A theoretical study of mafia games. <em>arXiv preprint arXiv:0804.0071</em>, 2008.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Zha21<span class="fn-bracket">]</span></span>
<p>Shengjing Zhang. Designing aiwolf agents by rule-based algorithm and by deep q-learning. <em>SCSE Student Reports (FYP/IA/PA/PI)</em>, 2021.</p>
</div>
</div>
</div>
</section>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game-description">Werewolf - The Game</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-literature-review">Literature Review</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game-implementation">Werewolf - The Implementation</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-terminology">Terminology</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-agent-implementation">Agent Implementation</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-training_agent_info">Training Agents</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-indicators">Gameplay Indicators</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-game_visualization">Replay visualization</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-plurality-analysis">Plurality Voting</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-approval-analysis">Approval Voting</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-dicsussion">Discussion</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-conclusion-future">Conclusion and Future Directions</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-bibliography">Bibliography</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By George Savin
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>