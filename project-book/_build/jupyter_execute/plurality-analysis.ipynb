{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_plurality_villager, \n",
    "    random_coordinated_plurality_villager, \n",
    "    random_agent,\n",
    "    random_plurality_wolf,\n",
    "    revenge_plurality_wolf,\n",
    "    coordinated_revenge_plurality_wolf)\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plurality Voting\n",
    "\n",
    "Plurality is a voting method where each voter picks a single candidate, and the candidate with the most votes is selected. It's simplicity has lead to wide adoption, however there are quite a few drawbacks, one of which being the limited expressibility of a voter.\n",
    "\n",
    "In our Werewolf plurality implementation, an agent can only select a single target, and cannot express their beliefs towards the remaining agents. This is also the voting mechanism used in every paper involving the Werewolf game up to now, so we want to see how our trained agents compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rates\n",
    "\n",
    "We want to see how our hand-crafted agents play against eachother, with special interest given to coordinated random villagers and wolves. \n",
    "\n",
    "As expected, the coordinated random villagers and wolves performed the best out of the static policies {cite}`braverman2008mafia`.\n",
    "Our agent, trained against coordinated random wolves, performed better than all of our hand-crafted villager policies. They also generalized well against our other wolf policies, and actually had the highest win rates across the board.\n",
    "\n",
    "Below is a generated table of 1000 runs between each villager policy and each werewolf policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 players, with 2 wolves - number of games played : 1000 \n",
      "\n",
      "Villager Strategy                        Totally Random Wolves    Coordinated Random Wolves    Revenge Wolves    Coordinated Revenge Wolves\n",
      "-------------------------------------  -----------------------  ---------------------------  ----------------  ----------------------------\n",
      "Totally Random                                           0.597                        0.042             0.076                         0.074\n",
      "Random Targetting of living villagers                    0.705                        0.125             0.192                         0.244\n",
      "Coorindated random targetting                            0.653                        0.314             0.304                         0.281\n",
      "Trained villagers                                        0.83                         0.473             0.45                          0.503\n"
     ]
    }
   ],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "trained_plurality_agent = ActorCriticAgent({\"rec_hidden_size\": 128,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 1,\n",
    "                                        \"approval_states\": 10},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_plurality_agent.load_state_dict(torch.load(\"../notebooks/stored_agents/lstm_first_no_one_hot_128_128/plurality_agent_10_score_46\"))\n",
    "\n",
    "num_games = 1000\n",
    "print(f'10 players, with 2 wolves - number of games played : {num_games} \\n')\n",
    "\n",
    "rv_wins = []\n",
    "rv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)\n",
    "    rv_wins.append(wins/float(num_games))\n",
    "    rv_replays.append(replays)\n",
    "\n",
    "rpv_wins = []\n",
    "rpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_plurality_villager, num_times=num_games)\n",
    "    rpv_wins.append(wins/float(num_games))\n",
    "    rpv_replays.append(replays)\n",
    "\n",
    "cpv_wins = []\n",
    "cpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_coordinated_plurality_villager, num_times=num_games)\n",
    "    cpv_wins.append(wins/float(num_games))\n",
    "    cpv_replays.append(replays)\n",
    "\n",
    "tpv_wins = []\n",
    "tpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    # wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)[0]/float(num_games)\n",
    "    wins, replays = play_recurrent_game(env, wolf_policy, trained_plurality_agent, num_times=num_games, hidden_state_size=128, voting_type=\"plurality\")\n",
    "    tpv_wins.append(wins/float(num_games))\n",
    "    tpv_replays.append(replays)\n",
    "\n",
    "print(tabulate([['Totally Random', *rv_wins], \n",
    "                ['Random Targetting of living villagers', *rpv_wins], \n",
    "                ['Coorindated random targetting', *cpv_wins], \n",
    "                ['Trained villagers', *tpv_wins]], \n",
    "               headers=[\"Villager Strategy\", \n",
    "                        \"Totally Random Wolves\", \n",
    "                        \"Coordinated Random Wolves\", \n",
    "                        \"Revenge Wolves\",\n",
    "                        \"Coordinated Revenge Wolves\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}