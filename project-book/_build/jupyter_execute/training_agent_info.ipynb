{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_recurrent_game, convert_obs\n",
    "from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents \n",
    "\n",
    "Because we used PPO to train our actor-critic truncated BPTT LSTM, we modified our code to closely resemble the work done in {cite}`pleines2022memory`.\n",
    "The steps we follow are very closely related to our [PPO Algorithm](ppo-alg-header).\n",
    "\n",
    "We first create a trainer class that stores our config and uses it to set up our buffer, our central villager policy and our optimizer.\n",
    "\n",
    "```python\n",
    "class PPOTrainer:\n",
    "    def __init__(self, env, config:dict, wolf_policy, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\"), voting_type:str=None) -> None:\n",
    "```\n",
    "\n",
    "We then call our train function which sets up the main loop consisting of gatheric metrics periodically, filling up our buffer, calculating our loss and updating our neural network.\n",
    "\n",
    "```python\n",
    "def train(self, voting_type=None, save_threshold=50.0):\n",
    "\n",
    "    # TODO: put mlflow logging into a callback\n",
    "    mlflow.log_params(self.config[\"config_training\"][\"training\"])\n",
    "    mlflow.log_params(self.config[\"config_training\"][\"model\"])\n",
    "    mlflow.log_params(self.config[\"config_game\"]['gameplay'])\n",
    "\n",
    "    loop = tqdm(range(self.config[\"config_training\"][\"training\"][\"updates\"]), position=0)\n",
    "\n",
    "    # if the average wins when we do periodic checks of the models scoring is above the save threshold, we save or overwrite the model\n",
    "    # model_save_threshold = 50.0\n",
    "    for tid, _ in enumerate(loop):\n",
    "\n",
    "        if tid % 10 == 0:\n",
    "            # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "            loop.set_description(\"Playing games and averaging score\")\n",
    "            wins = []\n",
    "            score_gathering = tqdm(range(10), position=1, leave=False)\n",
    "            replays = []\n",
    "            for _ in score_gathering:\n",
    "                game_wins, game_replays = play_recurrent_game(self.env, \n",
    "                                                self.wolf_policy, \n",
    "                                                self.agent, \n",
    "                                                num_times=100,\n",
    "                                                hidden_state_size=self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"],\n",
    "                                                voting_type=voting_type)\n",
    "                wins.append(game_wins)\n",
    "                replays.append(game_replays)\n",
    "                score_gathering.set_description(f'Avg wins with current policy : {np.mean(wins)}')\n",
    "\n",
    "            # flatten all of our replays and get our current stats\n",
    "            mlflow.log_metrics(aggregate_stats_from_replays(\n",
    "                [replay for game_replays in replays for replay in game_replays], voting_type=voting_type))\n",
    "\n",
    "            mlflow.log_metric(\"avg_wins/100\", np.mean(wins))\n",
    "            if np.mean(wins) > save_threshold:\n",
    "                save_threshold = int(np.mean(wins))\n",
    "                mlflow.pytorch.log_state_dict(self.agent.state_dict(), artifact_path='checkpoint')\n",
    "                # torch.save(self.agent.state_dict(), f'{voting_type}_agent_{self.config[\"config_game\"][\"gameplay\"][\"num_agents\"]}_score_{save_threshold}')\n",
    "\n",
    "\n",
    "        loop.set_description(\"Filling buffer\")\n",
    "        # fill buffer\n",
    "        # _scaled_rewards\n",
    "        buff = fill_recurrent_buffer_scaled_rewards(self.buffer, \n",
    "                                        self.env,\n",
    "                                        self.config[\"config_training\"],\n",
    "                                        self.wolf_policy, \n",
    "                                        self.agent,\n",
    "                                        voting_type=voting_type)\n",
    "\n",
    "        # train info will hold our metrics\n",
    "        train_info = []\n",
    "        # TODO List how many items we are training on\n",
    "        loop.set_description(f'Epoch Training on {self.buffer.games} games')\n",
    "        for _ in range(self.config['config_training'][\"training\"]['epochs']):\n",
    "            # run through batches and train network\n",
    "            for batch in buff.get_minibatch_generator(self.config['config_training'][\"training\"]['batch_size']):\n",
    "                train_info.append(calc_minibatch_loss(self.agent, \n",
    "                                                        batch, \n",
    "                                                        clip_range=self.config['config_training'][\"training\"]['clip_range'], \n",
    "                                                        beta=self.config['config_training'][\"training\"]['beta'], \n",
    "                                                        v_loss_coef=self.config['config_training'][\"training\"]['value_loss_coefficient'],\n",
    "                                                        grad_norm=self.config['config_training'][\"training\"]['max_grad_norm'],\n",
    "                                                        optimizer=self.optimizer))\n",
    "\n",
    "        train_stats = np.mean(train_info, axis=0)\n",
    "        mlflow.log_metric(\"policy loss\", train_stats[0])\n",
    "        mlflow.log_metric(\"value loss\", train_stats[1])\n",
    "        mlflow.log_metric(\"total loss\", train_stats[2])\n",
    "        mlflow.log_metric(\"entropy loss\", train_stats[3])\n",
    "```\n",
    "\n",
    "We calculate the loss per mini-batch, which is also a hypeparameter choice. This works for both approval and plurality models due to the policy outputs being in an array. We return and average out the policy, value, entropy and total losses. \n",
    "\n",
    "```python\n",
    "def calc_minibatch_loss(agent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, grad_norm: float, optimizer):\n",
    "    policies, values, _ = agent(samples['observations'], (samples['hxs'].detach(), samples['cxs'].detach()))\n",
    "    \n",
    "    log_probs, entropies = [], []\n",
    "    for i, policy_head in enumerate(policies):\n",
    "        # append the log_probs for 1 -> n other agent opinions\n",
    "        log_probs.append(policy_head.log_prob(samples['actions'][:,i]))\n",
    "        entropies.append(policy_head.entropy())\n",
    "    log_probs = torch.stack(log_probs, dim=1)\n",
    "    entropies = torch.stack(entropies, dim=1).sum(1).reshape(-1)\n",
    "    \n",
    "    ratio = torch.exp(log_probs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "\n",
    "    # need to repeat for amount of shape of policies (this way we know how many policy heads we need to watch out for)\n",
    "    norm_advantage = norm_advantage.unsqueeze(1).repeat(1, agent.num_votes)\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    return [policy_loss.cpu().data.numpy(),     # policy loss\n",
    "            vf_loss.cpu().data.numpy(),         # value loss\n",
    "            loss.cpu().data.numpy(),            # total loss\n",
    "            entropy_loss.cpu().data.numpy()]    # entropy loss\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mlflow-info)=\n",
    "## MLflow\n",
    "\n",
    "MLflow[^mlflow-tooling] is a powerful and popular MLOps open source tool for end-to-end machine learning lifecycle management. We compared this platform with others and simpler alternatives that just forcused on parameter and metric tracking, however due to the adoption of MLflow in industry, we felt it would be a good addition to the project as a sort of tutorial for any other researchers extending this work.\n",
    "\n",
    "For every training loop, we track all the associated hyperparameters of the experiment, we also track the following metrics:\n",
    "- Average villager wins periodically\n",
    "- Our Policy, Value, Entropy and total loss associated with the PPO algorithm\n",
    "- [Various indicators](gameplay-indicators-list) we try to use to understand the trained agent behaviors\n",
    "\n",
    "\n",
    "If the average villager wins passes a threshold, the model dict is stored for future usage, and this new rate is set as the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "There are just under 20 hyperparameters, and can be split between gameplay parameters, neural network model parameters and PPO training parameters. Each value in this configuration object below can be changed, and is passed throughout training functions.\n",
    "\n",
    "Initializations and hyper-parameter choices were greatly influenced by the exhaustive work in {cite}`Andrychowicz2020-fs` and {cite}`pleines2022memory` for the LSTM portion.\n",
    "\n",
    "```python\n",
    "config_training = {\n",
    "    \"model\": {\n",
    "        \"recurrent_layers\": 1,\n",
    "        \"recurrent_hidden_size\": 256, \n",
    "        \"joint_mlp_size\": 128,\n",
    "        \"split_mlp_size\": 128,\n",
    "        \"num_votes\": 10,\n",
    "        \"approval_states\": 3,\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"batch_size\": 256, \n",
    "        \"epochs\": 5, \n",
    "        \"updates\": 501, \n",
    "        \"buffer_games_per_update\": 500, \n",
    "        \"clip_range\": 0.1, \n",
    "        \"value_loss_coefficient\": 0.1, \n",
    "        \"max_grad_norm\": 0.5, \n",
    "        \"beta\": 0.01, \n",
    "        \"learning_rate\": 0.0001, \n",
    "        \"adam_eps\": 1e-5, \n",
    "        \"gamma\": 0.99, \n",
    "        \"gae_lambda\": 0.95, \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "config_game = {\n",
    "    \"rewards\": {\n",
    "        \"day\": -1,\n",
    "        \"player_death\": -1,\n",
    "        \"player_win\": 10,\n",
    "        \"player_loss\": -5,\n",
    "        \"self_vote\": -1,\n",
    "        \"dead_villager\": -1,\n",
    "        \"dead_vote\": -1,\n",
    "        \"dead_wolf\": 5,\n",
    "        \"no_viable_vote\": -1,\n",
    "        \"no_sleep\": -1,\n",
    "    },\n",
    "    \"gameplay\": {\n",
    "        \"accusation_phases\": 3,\n",
    "        \"num_agents\": 10,\n",
    "        \"num_werewolves\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"config_game\": config_game,\n",
    "    \"config_training\": config_training,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^mlflow-tooling]:https://mlflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}