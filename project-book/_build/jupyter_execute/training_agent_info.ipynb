{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_recurrent_game, convert_obs\n",
    "from notebooks.learning_agents.static_agents import random_approval_wolf, random_plurality_wolf\n",
    "import notebooks.learning_agents.stats as indicators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "There are just under 20 hyperparameters, and can be split between gameplay parameters, neural network model parameters and PPO training parameters. Each value in this configuration object below can be changed, and is passed throughout training functions.\n",
    "\n",
    "INitializations and hyper-parameter choices were greatly influenced by the exhaustive work in {cite}`Andrychowicz2020-fs` and {cite}`pleines2022memory` for the LSTM portion.\n",
    "\n",
    "\n",
    "```python\n",
    "config_training = {\n",
    "    \"model\": {\n",
    "        \"recurrent_layers\": 1,\n",
    "        \"recurrent_hidden_size\": 256, \n",
    "        \"joint_mlp_size\": 128,\n",
    "        \"split_mlp_size\": 128,\n",
    "        \"num_votes\": 10,\n",
    "        \"approval_states\": 3,\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"batch_size\": 256, \n",
    "        \"epochs\": 5, \n",
    "        \"updates\": 501, \n",
    "        \"buffer_games_per_update\": 500, \n",
    "        \"clip_range\": 0.1, \n",
    "        \"value_loss_coefficient\": 0.1, \n",
    "        \"max_grad_norm\": 0.5, \n",
    "        \"beta\": 0.01, \n",
    "        \"learning_rate\": 0.0001, \n",
    "        \"adam_eps\": 1e-5, \n",
    "        \"gamma\": 0.99, \n",
    "        \"gae_lambda\": 0.95, \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "config_game = {\n",
    "    \"rewards\": {\n",
    "        \"day\": -1,\n",
    "        \"player_death\": -1,\n",
    "        \"player_win\": 10,\n",
    "        \"player_loss\": -5,\n",
    "        \"self_vote\": -1,\n",
    "        \"dead_villager\": -1,\n",
    "        \"dead_vote\": -1,\n",
    "        \"dead_wolf\": 5,\n",
    "        \"no_viable_vote\": -1,\n",
    "        \"no_sleep\": -1,\n",
    "    },\n",
    "    \"gameplay\": {\n",
    "        \"accusation_phases\": 3,\n",
    "        \"num_agents\": 10,\n",
    "        \"num_werewolves\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"config_game\": config_game,\n",
    "    \"config_training\": config_training,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training w/ PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}