{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_approval_villager, \n",
    "    random_coordinated_approval_villager, \n",
    "    random_agent,\n",
    "    random_approval_wolf,\n",
    "    revenge_approval_wolf,\n",
    "    coordinated_revenge_approval_wolf,\n",
    "    random_likes_approval_wolf,\n",
    "    aggressive_approval_wolf,\n",
    "    )\n",
    "import notebooks.learning_agents.stats as indicators\n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approval Voting\n",
    "\n",
    "Approval voting is a mechanism in which voters can select as many candidates from a list of all possible candidates to approve of. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rates\n",
    "\n",
    "We assume that some of the findings in the plurality werewolf game {cite}`braverman2008mafia` will still hold for approval voting, due to the nature of calculating the consensus amongst all the targetting done. (Target with the most dislikes gets voted out).\n",
    "\n",
    "\n",
    "Villager Strategy vs. | [RWolves](rawolves) | [CRWolves](crawolves) | [RevWolves](revawolves) | [CRevWolves](crevawolves) | [CRLWolves](crlawolves) | [AggroWolves](aggrowolves) |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "Totally Random | | | | | | |\n",
    "Random Targetting of living villagers |  |  |  |  | | |\n",
    "Coorindated random targetting |  | |  |  | | |\n",
    "Trained villagers |  | |  | | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 players, with 2 wolves - number of games played : 1000 \n",
      "\n",
      "Villager Strategy      TRWolves    CRWolves    RevWolves    CRevWolves    CRLWolves    AgroWolves\n",
      "-------------------  ----------  ----------  -----------  ------------  -----------  ------------\n",
      "Random                    0.609       0.075        0.137         0.145        0.084         0.005\n",
      "L-Targets                 0.672       0.119        0.213         0.184        0.116         0.018\n",
      "CL-Targets                0.625       0.294        0.314         0.299        0.301         0.304\n",
      "Trained-CRWolves          0.578       0.48         0.583         0.595        0.168\n"
     ]
    }
   ],
   "source": [
    "env = pare(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "trained_approval_agent = ActorCriticAgent({\"rec_hidden_size\": 256,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 10,\n",
    "                                        \"approval_states\": 3},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_approval_agent.load_state_dict(torch.load(\"../notebooks/stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_49\"))\n",
    "\n",
    "num_games = 1000\n",
    "print(f'10 players, with 2 wolves - number of games played : {num_games} \\n')\n",
    "\n",
    "rv_wins = []\n",
    "rv_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)\n",
    "    rv_wins.append(wins/float(num_games))\n",
    "    rv_replays.append(replays)\n",
    "\n",
    "rav_wins = []\n",
    "rav_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_approval_villager, num_times=num_games)\n",
    "    rav_wins.append(wins/float(num_games))\n",
    "    rav_replays.append(replays)\n",
    "\n",
    "cav_wins = []\n",
    "cav_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_coordinated_approval_villager, num_times=num_games)\n",
    "    cav_wins.append(wins/float(num_games))\n",
    "    cav_replays.append(replays)\n",
    "\n",
    "tav_wins = []\n",
    "tav_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf, aggressive_approval_wolf]:\n",
    "    wins, replays = play_recurrent_game(env, wolf_policy, trained_approval_agent, num_times=num_games, hidden_state_size=256, voting_type=\"approval\")\n",
    "    tav_wins.append(wins/float(num_games))\n",
    "    tav_replays.append(replays)\n",
    "\n",
    "print(tabulate([['Random', *rv_wins], \n",
    "                ['L-Targets', *rav_wins], \n",
    "                ['CL-Targets', *cav_wins], \n",
    "                ['Trained-CRWolves', *tav_wins]], \n",
    "               headers=[\"Villager Strategy\", \n",
    "                        \"TRWolves\", \n",
    "                        \"CRWolves\", \n",
    "                        \"RevWolves\",\n",
    "                        \"CRevWolves\",\n",
    "                        \"CRLWolves\",\n",
    "                        \"AggroWolves\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days elapsed before a villager win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days between wolf executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targetting Indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}