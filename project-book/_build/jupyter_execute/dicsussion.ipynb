{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, plurality_env, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_plurality_wolf, \n",
    "    random_approval_wolf,\n",
    "    )\n",
    "import notebooks.learning_agents.stats as indicators\n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training \n",
    "### Inconsistent Training\n",
    "\n",
    "Reinforcement learning is a hard problem, and MARL even more so given its dynamic nature. To train our agents, we use a policy gradient method (PPO) that consistently improves our agents, but almost always has some type of divergence or weight collapse. In Sutton and Barto **REF**, they refer to a combination of issues -**deadly triad**- that when used together, lead to divergence more often than not. These issues are :\n",
    "- Using non-linear function approximations. _Our neural network_\n",
    "- Bootstraping our estimated values. _Our policy is changing constantly, along with our value approximation_\n",
    "- Off-Policy Training. _The one thing we do not do in this case!_\n",
    "\n",
    "Another thing to note is the difficulty to calcuate good estimates of the policy gradient (cite Kakade Langfor 2002) given the stochasticity of our MARL environment, and if we get a couple unfortunate episodes with bad estiamtes, our parameters may change in a poor direction leading to policy collapse and a long to possibly never recovery time. We do see in our training that there are multiple such events, with and without future recovery.\n",
    "\n",
    "Empirically, our agents can train for around 500 update steps around $10\\%$ of the time, meaning a collapse of our weights is almost always the termination factor of training. Despite this we do get decent results in the meantime, and would like to make training more consistent and reduce this collapse.\n",
    "\n",
    "Some ideas we believe may help and are worth trying:\n",
    "- Using decaying rates for our learning rate\n",
    "- Use running norms for observations and rewards\n",
    "- futher explore gradient clipping\n",
    "- any divisions should also add an $\\epsilon$ of $1e-5$ to begin with\n",
    "- clamp logarithmic values between `-np.log(1e-5) , np.log(1e-5))` to begin with\n",
    "- split critic and actor networks\n",
    "    - have a higher learning rate for the critic\n",
    "- vary replay buffer sizes\n",
    "- vary batch sizes\n",
    "- change optimizer\n",
    "- simplify model strcuture\n",
    "\n",
    "These have been collected from many blog posts, reddit posts, and work done in {cite}`Andrychowicz2020-fs`. We leave this exploration to future work, as it is not the direct scope of the project, but would help with consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Times\n",
    "\n",
    "One game of plurality takes :\n",
    "One game of approval takes :\n",
    "\n",
    "We "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approval vs Plurality\n",
    "\n",
    "Approval Voting allowed for agents to express dislikes as well as likes against all other agents in the game. This increased expressibility led to harder to interpret behavior, although trends were clearly seen. Training approval agents was more consistent than plurality agents, and they achieved higher win-rates much quicker. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}