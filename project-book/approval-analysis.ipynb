{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_approval_villager, \n",
    "    random_coordinated_approval_villager, \n",
    "    random_agent,\n",
    "    random_approval_wolf,\n",
    "    revenge_approval_wolf,\n",
    "    coordinated_revenge_approval_wolf,\n",
    "    random_likes_approval_wolf)\n",
    "import notebooks.learning_agents.stats as indicators\n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approval Voting\n",
    "\n",
    "Approval voting is a mechanism in which voters can select as many candidates from a list of all possible candidates to approve of. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rates\n",
    "\n",
    "We assume that some of the findings in the plurality werewolf game {cite}`braverman2008mafia` will still hold for approval voting, due to the nature of calculating the consensus amongst all the targetting done. (Target with the most dislikes gets voted out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = pare(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "trained_approval_agent = ActorCriticAgent({\"rec_hidden_size\": 256,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 10,\n",
    "                                        \"approval_states\": 3},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_approval_agent.load_state_dict(torch.load(\"../notebooks/stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_49\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 1000\n",
    "print(f'10 players, with 2 wolves - number of games played : {num_games} \\n')\n",
    "\n",
    "# random agent\n",
    "# random approval wolf \n",
    "# revenge approval wolf\n",
    "# coordinated revenge wolf\n",
    "# # sprinkle in a wolf that does random likes to see how well the learning agents do against this\n",
    "# aggressive wolf\n",
    "\n",
    "rv_wins = []\n",
    "rv_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)\n",
    "    rv_wins.append(wins/float(num_games))\n",
    "    rv_replays.append(replays)\n",
    "\n",
    "rav_wins = []\n",
    "rav_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_approval_villager, num_times=num_games)\n",
    "    rav_wins.append(wins/float(num_games))\n",
    "    rav_replays.append(replays)\n",
    "\n",
    "cav_wins = []\n",
    "cav_replays = []\n",
    "for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_coordinated_approval_villager, num_times=num_games)\n",
    "    cav_wins.append(wins/float(num_games))\n",
    "    cav_replays.append(replays)\n",
    "\n",
    "# tav_wins = []\n",
    "# tav_replays = []\n",
    "# for wolf_policy in [random_agent, random_approval_wolf, revenge_approval_wolf, coordinated_revenge_approval_wolf, random_likes_approval_wolf]:\n",
    "#     # wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)[0]/float(num_games)\n",
    "#     wins, replays = play_recurrent_game(env, wolf_policy, trained_approval_agent, num_times=num_games, hidden_state_size=256, voting_type=\"approval\")\n",
    "#     tav_wins.append(wins/float(num_games))\n",
    "#     tav_replays.append(replays)\n",
    "\n",
    "print(tabulate([['Totally Random', *rv_wins], \n",
    "                ['Random Targetting of living villagers', *rav_wins], \n",
    "                ['Coorindated random targetting', *cav_wins]], \n",
    "                # ['Trained villagers', *tav_wins]], \n",
    "               headers=[\"Villager Strategy\", \n",
    "                        \"Totally Random Wolves\", \n",
    "                        \"Coordinated Random Wolves\", \n",
    "                        \"Revenge Wolves\",\n",
    "                        \"Coordinated Revenge Wolves\",\n",
    "                        \"Coordinated Random Wolves Random Likes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m observations, _, _, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m obs_size\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mconvert_obs(observations[\u001b[39m'\u001b[39m\u001b[39mplayer_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m play_static_game(env, revenge_approval_wolf, random_agent, num_times\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/workspaces/voting-games/project-book/../notebooks/learning_agents/utils.py:154\u001b[0m, in \u001b[0;36mplay_static_game\u001b[0;34m(env, wolf_policy, villager_policy, num_times)\u001b[0m\n\u001b[1;32m    151\u001b[0m     wolf_action \u001b[39m=\u001b[39m wolf_policy(env, wolf, action\u001b[39m=\u001b[39mwolf_action)\n\u001b[1;32m    152\u001b[0m     actions[wolf] \u001b[39m=\u001b[39m wolf_action\n\u001b[0;32m--> 154\u001b[0m observations, rewards, terminations, truncations, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m env\u001b[39m.\u001b[39mworld_state[\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m Phase\u001b[39m.\u001b[39mNIGHT:\n\u001b[1;32m    157\u001b[0m     wolf_action \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/voting-games/project-book/../voting_games/env/parallel_werewolf_env_approval.py:213\u001b[0m, in \u001b[0;36mraw_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m# if its nighttime, we will add 0 votes for all agents. All asleep\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# if its nighttime, villagers do not see votes\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld_state[\u001b[39m'\u001b[39m\u001b[39mvotes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(actions)\n\u001b[0;32m--> 213\u001b[0m target, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_player_to_be_killed(actions)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld_state[\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m Phase\u001b[39m.\u001b[39mACCUSATION:\n\u001b[1;32m    216\u001b[0m     \n\u001b[1;32m    217\u001b[0m     \u001b[39m# add target to the dead agents\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdead_agents\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplayer_\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/voting-games/project-book/../voting_games/env/parallel_werewolf_env_approval.py:125\u001b[0m, in \u001b[0;36mraw_env._get_player_to_be_killed\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m player, action \u001b[39min\u001b[39;00m actions\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    123\u001b[0m     pid \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(player\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mfor\u001b[39;00m i, opinion \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(action):\n\u001b[1;32m    126\u001b[0m         \n\u001b[1;32m    127\u001b[0m         \u001b[39m# self_vote\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m pid \u001b[39mand\u001b[39;00m opinion \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m             infos[player][\u001b[39m\"\u001b[39m\u001b[39mself_vote\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def random_likes_approval_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        # just randomly assign a like or a neutral\n",
    "        for i in range(len(action)):\n",
    "            if action[i] != -1:\n",
    "                action[i] = random.randint(0,1)\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [random.randint(0,1) for _ in range(len(env.possible_agents))]\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "play_static_game(env, revenge_approval_wolf, random_agent, num_times=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days elapsed before a villager win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days between wolf executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targetting Indicators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
