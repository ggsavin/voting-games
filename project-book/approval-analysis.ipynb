{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_approval_villager, \n",
    "    random_coordinated_approval_villager, \n",
    "    random_agent,\n",
    "    random_approval_wolf,\n",
    "    revenge_approval_wolf,\n",
    "    coordinated_revenge_approval_wolf)\n",
    "import notebooks.learning_agents.stats as indicators\n",
    "import random\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approval Voting\n",
    "\n",
    "Approval voting is a mechanism in which voters can select as many candidates from a list of all possible candidates to approve of. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Rates\n",
    "\n",
    "We assume that some of the findings in the plurality werewolf game {cite}`braverman2008mafia` will still hold for approval voting, due to the nature of calculating the consensus amongst all the targetting done. (Target with the most dislikes gets voted out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "trained_approval_agent = ActorCriticAgent({\"rec_hidden_size\": 256,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 10,\n",
    "                                        \"approval_states\": 3},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_approval_agent.load_state_dict(torch.load(\"stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_49\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 1000\n",
    "print(f'10 players, with 2 wolves - number of games played : {num_games} \\n')\n",
    "\n",
    "rv_wins = []\n",
    "rv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)\n",
    "    rv_wins.append(wins/float(num_games))\n",
    "    rv_replays.append(replays)\n",
    "\n",
    "rpv_wins = []\n",
    "rpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_plurality_villager, num_times=num_games)\n",
    "    rpv_wins.append(wins/float(num_games))\n",
    "    rpv_replays.append(replays)\n",
    "\n",
    "cpv_wins = []\n",
    "cpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    wins, replays = play_static_game(env, wolf_policy, random_coordinated_plurality_villager, num_times=num_games)\n",
    "    cpv_wins.append(wins/float(num_games))\n",
    "    cpv_replays.append(replays)\n",
    "\n",
    "tpv_wins = []\n",
    "tpv_replays = []\n",
    "for wolf_policy in [random_agent, random_plurality_wolf, revenge_plurality_wolf, coordinated_revenge_plurality_wolf]:\n",
    "    # wins, replays = play_static_game(env, wolf_policy, random_agent, num_times=num_games)[0]/float(num_games)\n",
    "    wins, replays = play_recurrent_game(env, wolf_policy, trained_plurality_agent, num_times=num_games, hidden_state_size=128, voting_type=\"plurality\")\n",
    "    tpv_wins.append(wins/float(num_games))\n",
    "    tpv_replays.append(replays)\n",
    "\n",
    "print(tabulate([['Totally Random', *rv_wins], \n",
    "                ['Random Targetting of living villagers', *rpv_wins], \n",
    "                ['Coorindated random targetting', *cpv_wins], \n",
    "                ['Trained villagers', *tpv_wins]], \n",
    "               headers=[\"Villager Strategy\", \n",
    "                        \"Totally Random Wolves\", \n",
    "                        \"Coordinated Random Wolves\", \n",
    "                        \"Revenge Wolves\",\n",
    "                        \"Coordinated Revenge Wolves\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days elapsed before a villager win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days between wolf executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targetting Indicators"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
