---
---

% Petting zoo 
@article{terry2021pettingzoo,
  title={Pettingzoo: Gym for multi-agent reinforcement learning},
  author={Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15032--15043},
  year={2021}
}

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

% Gymnasium
@misc{towers_gymnasium_2023,
        title = {Gymnasium},
        url = {https://zenodo.org/record/8127025},
        abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
        urldate = {2023-07-08},
        publisher = {Zenodo},
        author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and Cola, Gianluca de and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
        month = mar,
        year = {2023},
        doi = {10.5281/zenodo.8127026},
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{Brandizzi2021RLupusCT,
  title={RLupus: Cooperation through emergent communication in The Werewolf social deduction game},
  author={Nicolo' Brandizzi and Davide Grossi and Luca Iocchi},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.05018}
}

# follow up to RL-Lupus
@inproceedings{Lipinski2022EMP,
  title={E MERGENT P ASSWORD S IGNALLING IN THE G AME OF W EREWOLF},
  author={Olaf Lipinski and Adam J. Sobey and F. Cerutti and T. Norman},
  year={2022}
}

# AI Wolf comeptition
@inproceedings{toriumi2017ai,
  title={AI wolf contest—development of game AI using collective intelligence—},
  author={Toriumi, Fujio and Osawa, Hirotaka and Inaba, Michimasa and Katagami, Daisuke and Shinoda, Kosuke and Matsubara, Hitoshi},
  booktitle={Computer Games: 5th Workshop on Computer Games, CGW 2016, and 5th Workshop on General Intelligence in Game-Playing Agents, GIGA 2016, Held in Conjunction with the 25th International Conference on Artificial Intelligence, IJCAI 2016, New York, USA, July 9-10, 2016, Revised Selected Papers 5},
  pages={101--115},
  year={2017},
  organization={Springer}
}

@inproceedings{
  pleines2023memory,
  title={Memory Gym: Partially Observable Challenges to Memory-Based Agents},
  author={Marco Pleines and Matthias Pallasch and Frank Zimmer and Mike Preuss},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=jHc8dCx6DDr}
}

@ARTICLE{Xu2019-vy,
  title={Voting-Based Multiagent Reinforcement Learning for Intelligent IoT},
  author={Yue Xu and Zengde Deng and Mengdi Wang and Wenjun Xu and Anthony Man-Cho So and Shuguang Cui},
  journal={IEEE Internet of Things Journal},
  year={2020},
  volume={8},
  pages={2681-2693},
  url={https://api.semanticscholar.org/CorpusID:212954448}
}


@INPROCEEDINGS{Matsunami2020-wt,
  title     = "Agents that Learn to Vote for a Joint Action Through
               {Multi-Agent} Reinforcement Learning",
  booktitle = "2020 9th International Congress on Advanced Applied Informatics
               ({IIAI-AAI})",
  author    = "Matsunami, Natsuki and Okuhara, Shun and Ito, Takayuki",
  pages     = "832--833",
  month     =  sep,
  year      =  {2020},
  keywords  = "Reinforcement learning;Task analysis;Informatics;Multi-agent
               Reinforcement Learning;Groves - Clarke pivotal mechanism;Voting"
}


@INPROCEEDINGS{Le_Gleau2020-ye,
  title     = "Multi-agents Ultimatum Game with Reinforcement Learning",
  booktitle = "Highlights in Practical Applications of Agents, {Multi-Agent}
               Systems, and Trust-worthiness. The {PAAMS} Collection",
  author    = "Le Gl{\'e}au, Tangui and Marjou, Xavier and Lemlouma, Tayeb and
               Radier, Benoit",
  publisher = "Springer International Publishing",
  pages     = "267--278",
  year      =  {2020}
}

@article{Dodevska2019ComputationalSC,
  title={Computational Social Choice and challenges of voting in multi-agent systems},
  author={A Zorica Dodevska},
  journal={Tehnika},
  year={2019}
}


@INPROCEEDINGS{Airiau2017-ny,
  title     = "Learning Agents for Iterative Voting",
  booktitle = "Algorithmic Decision Theory",
  author    = "Airiau, St{\'e}phane and Grandi, Umberto and Perotto, Filipo
               Studzinski",
  publisher = "Springer International Publishing",
  pages     = "139--152",
  year      =  2017
}


@INPROCEEDINGS{Partalas2007-sz,
  title     = "Multi-agent Reinforcement Learning Using Strategies and Voting",
  booktitle = "19th {IEEE} International Conference on Tools with Artificial
               {Intelligence(ICTAI} 2007)",
  author    = "Partalas, Ioannis and Feneris, Ioannis and Vlahavas, Ioannis",
  volume    =  2,
  pages     = "318--324",
  month     =  oct,
  year      =  2007,
  keywords  = "Learning;Voting;Artificial intelligence;Informatics;Robustness"
}



@ARTICLE{Kopparapu2022-bm,
  title={Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria},
  author={Kavya Kopparapu and Edgar A. Du{\'e}{\~n}ez-Guzm{\'a}n and Jayd Matyas and Alexander Sasha Vezhnevets and John P. Agapiou and Kevin R. McKee and Richard Everett and Janusz Marecki and Joel Z. Leibo and Thore Graepel},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.01816},
  url={https://api.semanticscholar.org/CorpusID:245769696}
}


@ARTICLE{Dafoe2020-ds,
  title={Open Problems in Cooperative AI},
  author={Allan Dafoe and Edward Hughes and Yoram Bachrach and Tantum Collins and Kevin R. McKee and Joel Z. Leibo and K. Larson and Thore Graepel},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.08630},
  url={https://api.semanticscholar.org/CorpusID:229220772}
}



@PHDTHESIS{Velikov2021-vt,
  title  = "{RLereWolf--Reinforcement} Learning Agent Development Framework For
            The Social Deduction Game Werewolf",
  author = "Velikov, Georgi Ventsislavov",
  year   =  2021,
  school = "University of Aberdeen"
}

@inproceedings{sun2021prediction,
  title={Prediction of Werewolf Players by Sentiment Analysis of Game Dialogue in Japanese},
  author={Sun, Yingxue and Kaneko, Tomoyuki},
  booktitle={Proc. of the 26th Game Programming Workshop 2021},
  pages={186--191},
  year={2021}
}

@INPROCEEDINGS{Wang2018-yn,
  title     = "Application of Deep Reinforcement Learning in Werewolf Game
               Agents",
  booktitle = "2018 Conference on Technologies and Applications of Artificial
               Intelligence ({TAAI})",
  author    = "Wang, Tianhe and Kaneko, Tomoyuki",
  abstract  = "Werewolf, also known as Mafia, is a kind of game with imperfect
               information. Werewolf game agents must cope with two kinds of
               problems, ``decision on who to trust or to kill'', and
               ``decision on information exchange''. In this paper, we focus on
               the first problem. We apply techniques in Deep Q Network in
               building werewolf agents. We also improve representation of
               states and actions based on existing agents trained by Q
               learning method. Our proposed agents were compared with existing
               agents trained by Q learning method and with existing agents
               submitted to the AIWolf Contest, the most famous werewolf game
               agents contest in Japan. For every role, we prepared four agents
               with proposed method and investigated average win ratio of four
               agents in our experiments. Experimental results showed that when
               agents learned and played with the same group of players, our
               proposed agents have better player performances than existing
               agents trained by Q learning method and a part of agents
               submitted to the AIWolf Contest. We obtained promising results
               by using reinforcement learning method to solve ``decision on
               who to trust or to kill'' problem without using heuristic
               methods.",
  pages     = "28--33",
  month     =  nov,
  year      =  2018,
  keywords  = "Games;Buildings;Information exchange;Learning
               systems;Protocols;Estimation;werewolf;game;deep reinforcement
               learning;agent"
}


@article{chen2022deep,
  title={Deep reinforcement learning with emergent communication for coalitional negotiation games},
  author={Chen, Siqi and Yang, Yang and Su, Ran},
  journal={Math. Biosci. Eng},
  volume={19},
  number={5},
  pages={4592--4609},
  year={2022}
}

@article{burka2022voting,
  title={Voting: a machine learning approach},
  author={Burka, D{\'a}vid and Puppe, Clemens and Szepesv{\'a}ry, L{\'a}szl{\'o} and Tasn{\'a}di, Attila},
  journal={European Journal of Operational Research},
  volume={299},
  number={3},
  pages={1003--1017},
  year={2022},
  publisher={Elsevier}
}

@ARTICLE{Conner2022-iu,
  title     = "Are You a Werewolf? Teaching Symbolic Interaction Theory through
               Game Play",
  author    = "Conner, Christopher T and Baxter, Nicholas M",
  journal   = "Teach. Sociol.",
  publisher = "SAGE Publications Inc",
  volume    =  50,
  number    =  1,
  pages     = "17--27",
  month     =  jan,
  year      =  2022
}

@article{zhang2021designing,
  title={Designing AIWolf agents by rule-based algorithm and by deep Q-learning},
  author={Zhang, Shengjing},
  year={2021},
  journal={SCSE Student Reports (FYP/IA/PA/PI)},
  publisher={Nanyang Technological University}
}

@INPROCEEDINGS{8916754,
  author={Kato, Shoma and Okumura, Tomoya and Toda, Itsuki and Fukui, Takanori and Iwata, Kazunori and Ito, Nobuhiro},
  booktitle={2019 6th International Conference on Computational Science/Intelligence and Applied Informatics (CSII)}, 
  title={Consideration of the Essential Topics for Role Estimation for AIWolf}, 
  year={2019},
  volume={},
  number={},
  pages={72-77},
  doi={10.1109/CSII.2019.00020}}

@INPROCEEDINGS{Hirata2016-bu,
  title     = "Werewolf Game Modeling Using Action Probabilities Based on Play
               Log Analysis",
  booktitle = "Computers and Games",
  author    = "Hirata, Yuya and Inaba, Michimasa and Takahashi, Kenichi and
               Toriumi, Fujio and Osawa, Hirotaka and Katagami, Daisuke and
               Shinoda, Kousuke",
  abstract  = "In this study, we construct a non-human agent that can play the
               werewolf game (i.e., AI wolf) with aims of creating more
               advanced intelligence and acquire more advanced communication
               skills for AI-based systems. We therefore constructed a
               behavioral model using information regarding human players and
               the decisions made by such players; all such information was
               obtained from play logs of the werewolf game. To confirm our
               model, we conducted simulation experiments of the werewolf game
               using an agent based on our proposed behavioral model, as well
               as a random agent for comparison. Consequently, we obtained an
               81.55\% coincidence ratio of agent behavior versus human
               behavior.",
  publisher = "Springer International Publishing",
  pages     = "103--114",
  year      =  2016
}


@inproceedings{hagiwara2019using,
  title={Using q-learning and estimation of role in werewolf game},
  author={Hagiwara, Makoto and Moustafa, Ahmed and Ito, Takayuki},
  booktitle={Proceedings of the Annual Conference of JSAI 33rd (2019)},
  pages={2O5E303--2O5E303},
  year={2019},
  organization={The Japanese Society for Artificial Intelligence}
}

@inproceedings{nishizaki2016behavior,
  title={Behavior Analysis of Executed and Attacked Players in Werewolf Game by ILP.},
  author={Nishizaki, Ema and Ozaki, Tomonobu},
  booktitle={ILP (Short Papers)},
  pages={48--53},
  year={2016}
}

@inproceedings{katagami2014investigation,
  title={Investigation of the effects of nonverbal information on werewolf},
  author={Katagami, Daisuke and Takaku, Shono and Inaba, Michimasa and Osawa, Hirotaka and Shinoda, Kosuke and Nishino, Junji and Toriumi, Fujio},
  booktitle={2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
  pages={982--987},
  year={2014},
  organization={IEEE}
}

@ARTICLE{Ri2022-ih,
  title     = "The Dynamics of Minority versus Majority Behaviors: A Case Study
               of the Mafia Game",
  author    = "Ri, Hong and Kang, Xiaohan and Khalid, Mohd Nor Akmal and Iida,
               Hiroyuki",
  journal   = "Information",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  13,
  number    =  3,
  pages     = "134",
  month     =  mar,
  year      =  2022,
  language  = "en"
}


@ARTICLE{Serrino2019-ge,
  title    = "Finding friend and foe in multi-agent games",
  author   = "Serrino, Jack and Kleiman-Weiner, Max and Parkes, David C and
              Tenenbaum, Josh",
  journal  = "Adv. Neural Inf. Process. Syst.",
  volume   =  32,
  year     =  2019
}


@ARTICLE{Reinhardt2020-bs,
  title={Competing in a Complex Hidden Role Game with Information Set Monte Carlo Tree Search},
  author={Jackson T Reinhardt},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.07156},
  url={https://api.semanticscholar.org/CorpusID:218628845}
}



@ARTICLE{Andrychowicz2020-fs,
  title         = "What Matters In {On-Policy} Reinforcement Learning? A
                   {Large-Scale} Empirical Study",
  author        = "Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk,
                   Piotr and Orsini, Manu and Girgin, Sertan and Marinier,
                   Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and
                   Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain
                   and Bachem, Olivier",
  abstract      = "In recent years, on-policy reinforcement learning (RL) has
                   been successfully applied to many different continuous
                   control tasks. While RL algorithms are often conceptually
                   simple, their state-of-the-art implementations take numerous
                   low- and high-level design decisions that strongly affect
                   the performance of the resulting agents. Those choices are
                   usually not extensively discussed in the literature, leading
                   to discrepancy between published descriptions of algorithms
                   and their implementations. This makes it hard to attribute
                   progress in RL and slows down overall progress
                   [Engstrom'20]. As a step towards filling that gap, we
                   implement >50 such ``choices'' in a unified on-policy RL
                   framework, allowing us to investigate their impact in a
                   large-scale empirical study. We train over 250'000 agents in
                   five continuous control environments of different complexity
                   and provide insights and practical recommendations for
                   on-policy training of RL agents.",
  journal={arXiv preprint arXiv:2006.05990},
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2006.05990"
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@inproceedings{pleines2022memory,
  title={Memory Gym: Partially Observable Challenges to Memory-Based Agents},
  author={Pleines, Marco and Pallasch, Matthias and Zimmer, Frank and Preuss, Mike},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@ARTICLE{Eger2018-hx,
  title    = "Keeping the Story Straight: A Comparison of Commitment Strategies
              for a Social Deduction Game",
  author   = "Eger, Markus and Martens, Chris",
  abstract = "Social deduction games present a unique challenge for AI agents,
              because communication plays a central role in most of them, and
              deception plays a key role in game play. To be successful in such
              games, players need to come up with convincing stories, but also
              discern the truth of statements of other players and adapt to the
              information learned from them. In this paper we present an
              approach for virtual agents that have to determine how long to
              stick to their story in the light of information obtained from
              other players. We apply this approach to a particular social
              deduction game, One Night Ultimate Werewolf, and demonstrate the
              effect of different levels of commitment to an agent's story.",
  journal  = "AIIDE",
  volume   =  14,
  number   =  1,
  pages    = "24--30",
  month    =  sep,
  year     =  2018,
  keywords = "game play; epistemic logic; Werewolf; Mafia; intentionality;
              commitment; planning",
  language = "en"
}


@article{braverman2008mafia,
  title={Mafia: A theoretical study of players and coalitions in a partial information environment},
  author={Mark Braverman and Omid Etesami and Elchanan Mossel},
  journal={Annals of Applied Probability},
  year={2006},
  volume={18},
  pages={825-846},
  url={https://api.semanticscholar.org/CorpusID:14668989}
}

@article{yao2008theoretical,
  title={A theoretical study of mafia games},
  author={Yao, Erlin},
  journal={arXiv preprint arXiv:0804.0071},
  year={2008}
}

@misc{ibraheem2022putting,
      title={Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia}, 
      author={Samee Ibraheem and Gaoyue Zhou and John DeNero},
      year={2022},
      eprint={2207.02253},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{migdal2010mathematical,
  title={A mathematical model of the Mafia game},
  author={Migda{\l}, Piotr},
  journal={arXiv preprint arXiv:1009.1031},
  year={2010}
}

@inproceedings{azaria2015agent,
  title={An agent for deception detection in discussion based environments},
  author={Azaria, Amos and Richardson, Ariella and Kraus, Sarit},
  booktitle={Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
  pages={218--227},
  year={2015}
}
---
Example Citations
---
