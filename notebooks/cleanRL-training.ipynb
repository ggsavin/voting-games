{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CleanRL PPO to try and train Villagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import raw_env\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly based on [this](https://pettingzoo.farama.org/tutorials/cleanrl/implementing_PPO/), and the [following blogpost](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/). Another PZ implementation referenced is [here](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_pettingzoo_ma_atari.py)\n",
    "\n",
    "One more link I plan on reading, at least for PPO is [here](https://towardsdatascience.com/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_spaces[\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mn\n\u001b[1;32m      2\u001b[0m observation_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_spaces[\u001b[39m'\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = env.observation_spaces['player_1']['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_spaces['player_1']['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('action_mask',\n",
       "              array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                      True])),\n",
       "             ('observation',\n",
       "              OrderedDict([('day', 1),\n",
       "                           ('phase', 2),\n",
       "                           ('player_status',\n",
       "                            array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                                    True])),\n",
       "                           ('roles', array([1, 1, 0, 1, 0, 0, 1, 1, 1, 1])),\n",
       "                           ('votes',\n",
       "                            array([10.892954 ,  0.8731968,  8.033873 ,  2.277371 ,  1.8526876,\n",
       "                                   10.99731  ,  7.982339 ,  5.003656 ,  2.5943236,  2.754004 ],\n",
       "                                  dtype=float32))]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spaces['player_1'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([1, 1, array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True]), array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0]), array([10.48173  ,  2.2281246,  2.331817 ,  4.617806 ,  5.0304337,\n",
       "        1.4088393,  2.493021 , 10.6729   ,  5.5346136,  6.636181 ],\n",
       "      dtype=float32)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spaces['player_1'].sample()['observation'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_obs(observation):\n",
    "    return  np.asarray([observation['day']] + \\\n",
    "            [observation['phase']] + \\\n",
    "            [int(status) for status in observation['player_status']] + \\\n",
    "            [role for role in observation['roles']] + \\\n",
    "            [vote for vote in observation['votes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, num_actions, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, num_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGORITHM PARAMETERS\n",
    "# TODO: What is really necessary here?\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "batch_size = 32 #\n",
    "stack_size = 4 #\n",
    "frame_size = (64, 64) #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 2 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Env Setup\n",
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "# env.reset()\n",
    "num_agents = 10\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "# for agents in \n",
    "# # Algorithm Logic : Episode Storage\n",
    "\n",
    "# # rb = rollback\n",
    "# end_step = 0\n",
    "# total_episodic_return = 0\n",
    "# rb_obs = \n",
    "# rb_actions =\n",
    "# rb_logprobs = \n",
    "# rb_rewards = \n",
    "# rb_terms = \n",
    "# rb_values =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    available_actions = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(available_actions, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    action = random.choice(legal_actions)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1\n",
      "T: 8 - Rewards : -25 \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for dimension 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(records[\u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m]))):\n\u001b[1;32m     54\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mT: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m - Rewards : \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtensor(records[\u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m])[t]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mT: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m - Rewards : \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39;49mtensor(records[\u001b[39m\"\u001b[39;49m\u001b[39mrewards\u001b[39;49m\u001b[39m\"\u001b[39;49m])[t\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[39m# not using terms, as these are episodic\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39m#delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m     \u001b[39m#advantages[t] = delta + gamma * gamma * advantages[t+1]\u001b[39;00m\n\u001b[1;32m     61\u001b[0m returns \u001b[39m=\u001b[39m advantages \u001b[39m+\u001b[39m records[\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for dimension 0 with size 9"
     ]
    }
   ],
   "source": [
    "# Training Logic\n",
    "total_episodes = 1\n",
    "for episode in range(total_episodes):\n",
    "    with torch.no_grad():\n",
    "        env.reset()\n",
    "\n",
    "        # magent_list = {agent: [] for agent in env.agents}\n",
    "        magent_list = {agent : [] for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "        # print(magent_list.keys())\n",
    "        for magent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "            if sum(observation['observation']['roles']):\n",
    "                # TODO: find a cleaner way to identify a wolf\n",
    "                action = random_policy(observation, magent) if not termination or truncation else None\n",
    "            else:\n",
    "                obs = torch.Tensor(flat_obs(observation['observation']))\n",
    "                if not termination or truncation:\n",
    "                    action, logprobs, _, value = agent.get_action_and_value(obs)\n",
    "                else:\n",
    "                    action = None\n",
    "\n",
    "                magent_list[magent].append({\n",
    "                    \"obs\": obs, \n",
    "                    \"action\": action,\n",
    "                    \"prev_reward\": reward,\n",
    "                    \"logprobs\": logprobs,\n",
    "                    \"value\": value\n",
    "                    })\n",
    "                \n",
    "            env.step(action)\n",
    "        \n",
    "        # take the sequential observations of each agent, and store them appropriately\n",
    "        magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': []} for agent in magent_list}\n",
    "        for key, value in magent_list.items():\n",
    "            # print(f'-- {key} --')\n",
    "            for s1, s2 in zip(value, value[1:]):\n",
    "                magent_obs[key]['obs'].append(s1['obs'])\n",
    "                magent_obs[key]['rewards'].append(s2['prev_reward'])\n",
    "                magent_obs[key]['actions'].append(s1['action'])\n",
    "                magent_obs[key]['logprobs'].append(s1['logprobs'])\n",
    "                magent_obs[key]['values'].append(s1['value'])\n",
    "\n",
    "    # We will do this for each agent in the episode\n",
    "    # essentially we are calculating advantages and returns\n",
    "    with torch.no_grad():\n",
    "        for player, records in magent_obs.items():\n",
    "            print(f'{player}')\n",
    "            # print(f'{records}')\n",
    "            advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "            for t in reversed(range(len(records['obs']))):\n",
    "                print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                # not using terms, as these are episodic\n",
    "                #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "\n",
    "                #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "            \n",
    "            returns = advantages + records['values']\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magent_obs['player_4']['actions'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obs': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'action': tensor(4),\n",
       "  'prev_reward': 0,\n",
       "  'logprobs': tensor(-2.3014),\n",
       "  'value': tensor([0.8297])},\n",
       " {'obs': tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 6., 4., 7., 3., 1., 1., 7., 1., 4., 2.]),\n",
       "  'action': tensor(0),\n",
       "  'prev_reward': 0,\n",
       "  'logprobs': tensor(-2.2969),\n",
       "  'value': tensor([1.4532])},\n",
       " {'obs': tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 3., 0., 3., 1., 6., 5., 7., 2., 7., 2.]),\n",
       "  'action': tensor(5),\n",
       "  'prev_reward': -1,\n",
       "  'logprobs': tensor(-2.3052),\n",
       "  'value': tensor([0.6734])},\n",
       " {'obs': tensor([1., 2., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'action': tensor(2),\n",
       "  'prev_reward': -1,\n",
       "  'logprobs': tensor(-2.3045),\n",
       "  'value': tensor([0.2058])},\n",
       " {'obs': tensor([2., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 2., 9., 0., 3., 6., 0., 6., 2., 2.]),\n",
       "  'action': tensor(3),\n",
       "  'prev_reward': 0,\n",
       "  'logprobs': tensor(-2.2929),\n",
       "  'value': tensor([0.7987])},\n",
       " {'obs': tensor([3., 2., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'action': None,\n",
       "  'prev_reward': -36,\n",
       "  'logprobs': tensor(-2.3134),\n",
       "  'value': tensor([0.6216])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magent_list['player_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
