{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approval Voting\n",
    "\n",
    "Blurb on approval voting goes here...\n",
    "\n",
    "We want to see how an static agents vs static wolves fare, before training our PPO agents to hopefully learn to do better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Baselines\n",
    "\n",
    "To properly asses our agents, we need baselines. For this purpose we have totally random villagers and semi-random villagers that will only vote for agents that are currently alive.\n",
    "\n",
    "As for wolves, we have the following behaviors:\n",
    "- random wolves that coordinate and each target one villager while approve themselves. The remaining villagers get neutral rankings\n",
    "- hyper aggressive wolves that simply disapprove of every single non-wolf player\n",
    "- random wolves that do whatever\n",
    "- revenge wolves that coordinate and target a villager that targetted them. Choose a revenge target randomly each round"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approval voting is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     88\u001b[0m         game_replays\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39mdeepcopy(env\u001b[39m.\u001b[39mhistory))\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m villager_wins, game_replays\n\u001b[0;32m---> 92\u001b[0m env \u001b[39m=\u001b[39m pare(num_agents\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, werewolves\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_accusations\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pare' is not defined"
     ]
    }
   ],
   "source": [
    "def random_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def aggressive_wolf(env, agent, action=None):\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "    action = [-1] * len(env.possible_agents)\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    # TODO:\n",
    "    return None\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent, action=None):\n",
    "   return env.action_space(agent).sample().tolist()\n",
    "\n",
    "\n",
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100):\n",
    "\n",
    "    villager_wins = 0\n",
    "    game_replays = []\n",
    "    for _ in range(num_times):\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        game_replays.append(copy.deepcopy(env.history))\n",
    "\n",
    "    return villager_wins, game_replays\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2, num_accusations=1)\n",
    "env.reset()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")\n",
    "\n",
    "env = pare(num_agents=15, werewolves=3, num_accusations=1)\n",
    "env.reset()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)[0]}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)[0]}')\n",
    "print(\"------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the most realistic scenario, random coordinated wolves vs. single target random villagers (that do not target dead players) we see villagers winning under 13% of the time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained/Training agents \n",
    "\n",
    "Agents were trained using an LSTM to have a history of moves to hopefully elucidate who the wolves are. There were very many hyperparameters to choose from with the overarching goal being the impact gameplay settings such as number of accusation rounds would have on the learning and the agents ability to win and to implicitly communicate between eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
