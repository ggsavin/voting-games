{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import parrallel_raw_env, Roles\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, approval_states, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, approval_states), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        # could call the network each time, with a different integer for each player?  get approvals that way\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    player_status = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(player_status, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    player = random.choice(legal_actions)\n",
    "\n",
    "    action = [0] * len(action_mask)\n",
    "    action[me] = 1\n",
    "    action[player] = -1\n",
    "    return action\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == -1 and i == me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "    choice = [-1] * len(observation['action_mask'])\n",
    "\n",
    "    choice[me] = 1\n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1]\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected =  random.choice(villagers_alive)\n",
    "    choice = [0] * len(observation['observation']['player_status'])\n",
    "    \n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    choice[player_selected] = -1\n",
    "\n",
    "    return action if action != None else choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 439.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.16\n",
      "Wolf wins : 943\n",
      "Villager wins: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 430.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.04\n",
      "Wolf wins : 997\n",
      "Villager wins: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 643\n",
      "Villager wins: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = parrallel_raw_env(num_agents=10, werewolves=1)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on approval\n",
    "\n",
    "Because we need to generate approvals for every other agent, we will have to call the neural network n-1 times. We should look at batching this, as well as generating the proper observation\n",
    "\n",
    "maybe we have to call the model n times, but then add the loss together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
