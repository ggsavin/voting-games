{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import parrallel_raw_env, Roles\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, approval_states, num_players, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, approval_states), std=0.01),\n",
    "        )\n",
    "\n",
    "        self.num_players = num_players\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        # TODO: We need torch.mean because PPO will use value, and we have a bunch here. \n",
    "        #       Do we need to change PPO here?\n",
    "        return torch.mean(self.critic(torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])))\n",
    "    \n",
    "    # only doing this for the PPO batched call so I don't need extra logic in the regular get action and value\n",
    "    def get_batched_action_and_value(self, x, actions=None):\n",
    "\n",
    "        if actions is None:\n",
    "            raise ValueError(\"We need batched actions here\")\n",
    "\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        critics = []\n",
    "        for current_obs, action in zip(x, actions):\n",
    "            updated_obs = torch.stack([torch.cat((torch.tensor([i]), current_obs)) for i in range(self.num_players)])\n",
    "\n",
    "            logits = self.actor(updated_obs)\n",
    "            probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "            \n",
    "            # update our return tensors\n",
    "            log_probs.append(torch.sum(probs.log_prob(action)))\n",
    "            entropies.append(torch.prod(probs.entropy()))\n",
    "            critics.append(torch.mean(self.critic(updated_obs)))\n",
    "            \n",
    "        return actions, torch.stack(log_probs), torch.stack(entropies), torch.stack(critics)\n",
    "\n",
    "    def convert_actions_to_approvals(self, actions):\n",
    "        return [-1 if a == 2 else a.item() for a in actions]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        # could call the network each time, with a different integer for each player?  get approvals that way\n",
    "        # x is the flattened observation. we should go ahead and run each of the player_ids appended to full obs to get multiple classifications\n",
    "        # how  to handle entropy here? maybe we multiply all the probs, and then calculate the overall entropy\n",
    "        # self.critic needs to be changed too, to return an array\n",
    "\n",
    "        # option to have critic/actors for every single player?\n",
    "\n",
    "        # option to also delevt n-1 * n-2 for -1s on the wolf\n",
    "        \n",
    "        # get logits for every single player in the game.\n",
    "        x = torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        # we multiply the entropy, and we add the log_probs together\n",
    "        # TODO: multiple values for critic. should I average?\n",
    "        return action, torch.sum(probs.log_prob(action)), torch.prod(probs.entropy()), torch.mean(self.critic(x))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    player_status = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(player_status, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    player = random.choice(legal_actions)\n",
    "\n",
    "    action = [0] * len(action_mask)\n",
    "    action[me] = 1\n",
    "    action[player] = -1\n",
    "    return action\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == -1 and i == me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "    choice = [-1] * len(observation['action_mask'])\n",
    "\n",
    "    choice[me] = 1\n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1]\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected =  random.choice(villagers_alive)\n",
    "    choice = [0] * len(observation['observation']['player_status'])\n",
    "    \n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    choice[player_selected] = -1\n",
    "\n",
    "    return action if action != None else choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:06<00:00, 151.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 9341\n",
      "Villager wins: 659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 10000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 430.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.04\n",
      "Wolf wins : 997\n",
      "Villager wins: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 643\n",
      "Villager wins: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = parrallel_raw_env(num_agents=10, werewolves=1)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_return_stats(env, wolf_policy, agent_policy, num_games=1000):\n",
    "    wolf_wins = 0\n",
    "    villager_wins = 0\n",
    "\n",
    "    game_replays = []\n",
    "    for _ in tqdm(range(num_games)):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : {\"self_votes\": 0, \"dead_votes\": 0, \"lasted_for\": 0} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = wolf_policy(observation, None, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    # action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = agent_policy.get_action_and_value(obs)\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : maybe make these callbacks?\n",
    "                    # if action == observation['observation']['self_id']:\n",
    "                    #     magent_list[magent]['self_votes'] += 1\n",
    "                \n",
    "                    # if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                    #     magent_list[magent]['dead_votes'] += 1\n",
    "                        \n",
    "                env.step(action)\n",
    "            \n",
    "        game_replays.append(env.history)\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    return wolf_wins, villager_wins, game_replays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on approval\n",
    "\n",
    "Because we need to generate approvals for every other agent, we will have to call the neural network n-1 times. We should look at batching this, as well as generating the proper observation\n",
    "\n",
    "maybe we have to call the model n times, but then add the loss together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 2, Villager wins : 0\n"
     ]
    }
   ],
   "source": [
    "def play_and_return_stats(env, wolf_policy, agent_policy, num_games=1000):\n",
    "    wolf_wins = 0\n",
    "    villager_wins = 0\n",
    "\n",
    "    game_replays = []\n",
    "    for _ in range(num_games):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : {\"self_votes\": 0, \"dead_votes\": 0, \"lasted_for\": 0} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = wolf_policy(observation, None, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    # action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = agent_policy.get_action_and_value(obs)\n",
    "                        action = action.tolist()\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : maybe make these callbacks?\n",
    "                    if action == observation['observation']['self_id']:\n",
    "                        magent_list[magent]['self_votes'] += 1\n",
    "                \n",
    "                    if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                        magent_list[magent]['dead_votes'] += 1\n",
    "                        \n",
    "                env.step(action)\n",
    "            \n",
    "        game_replays.append(env.history)\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    return wolf_wins, villager_wins, game_replays\n",
    "\n",
    "\n",
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "# ppo_agent = torch.load(\"ppo_agent\")\n",
    "\n",
    "wwins, vwins, game_replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=2)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [2:06:11<00:00,  3.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.19\n",
      "Wolf wins : 27777\n",
      "Villager wins: 2223\n",
      "Avg amount of self votes a game across villagers: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 32 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 30000 #\n",
    "update_epochs = 3 #\n",
    "\n",
    "# stats to keep track of for custom metrics\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "parallel_ppo_agent = Agent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(ppo_agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Approval Training Random Wolf behavior'):\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            self_votes = 0\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : [] for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    game_action = random_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = game_action\n",
    "                else:\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = parallel_ppo_agent.get_action_and_value(obs)\n",
    "                        game_action = parallel_ppo_agent.convert_actions_to_approvals(action)\n",
    "                    else:\n",
    "                        action = None\n",
    "                        game_action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : THIS only works if you have a -1 approval for your own ID\n",
    "                    if action != None and action[observation['observation']['self_id']] == -1:\n",
    "                        self_votes += 1\n",
    "                \n",
    "                    # TODO : Do not care about dead votes yet\n",
    "                    # if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                    #     dead_votes += 1\n",
    "\n",
    "                    magent_list[magent].append({\n",
    "                        \"obs\": obs, \n",
    "                        \"action\": action,\n",
    "                        \"prev_reward\": reward,\n",
    "                        \"logprobs\": logprobs,\n",
    "                        \"term\": termination,\n",
    "                        \"value\": value\n",
    "                        })\n",
    "\n",
    "                env.step(game_action)\n",
    "            \n",
    "            # take the sequential observations of each agent, and store them appropriately\n",
    "            magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': [], 'terms': []} for agent in magent_list}\n",
    "            for key, value in magent_list.items():\n",
    "                # print(f'-- {key} --')\n",
    "                for s1, s2 in zip(value, value[1:]):\n",
    "                    magent_obs[key]['obs'].append(s1['obs'])\n",
    "                    magent_obs[key]['rewards'].append(s2['prev_reward'])\n",
    "                    magent_obs[key]['actions'].append(s1['action'])\n",
    "                    magent_obs[key]['logprobs'].append(s1['logprobs'])\n",
    "                    magent_obs[key]['values'].append(s1['value'])\n",
    "                    magent_obs[key]['terms'].append(s2['term'])\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        self_voting.append(self_votes)\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "        avg_game_length += (day * 1.0)/total_episodes\n",
    "        # END OF POST GAME STATS #\n",
    "\n",
    "        mlflow.log_metric(\"avg_game_len\", f'{avg_game_length:.2f}')\n",
    "        mlflow.log_metric(\"avg_self_votes\", f'{sum(self_voting)/len(self_voting)}')\n",
    "        mlflow.log_metric(\"wolf wins\", wolf_wins)\n",
    "        mlflow.log_metric(\"villager wins\", villager_wins)\n",
    "        if episode % 50 == 0:\n",
    "            #wwins, vwins = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=100)\n",
    "            #mlflow.log_metric(\"wwins\", wwins)\n",
    "            #mlflow.log_metric(\"vwins\", vwins)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "\n",
    "        # We will do this for each agent in the episode\n",
    "        # essentially we are calculating advantages and returns\n",
    "        with torch.no_grad():\n",
    "            for player, records in magent_obs.items():\n",
    "                # print(f'{records}')\n",
    "                advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "\n",
    "                for t in reversed(range(len(records['obs']))):\n",
    "                    # print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                    # not using terms, as these are episodic\n",
    "\n",
    "                    ## this was the last one. We are not using any terminal states in a good way\n",
    "\n",
    "                    if t == len(records['obs']) - 1:\n",
    "                        #print(f'T: {t} - Rewards at end : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions at end : {torch.tensor(records[\"actions\"])[t]} ')\n",
    "                        delta = records[\"rewards\"][t] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta\n",
    "                    else:\n",
    "                        #print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions : {torch.tensor(records[\"actions\"])[t]} ')                    \n",
    "                        delta = records[\"rewards\"][t] + gamma * records[\"values\"][t+1] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta + gamma * gamma * advantages[t+1]\n",
    "\n",
    "                    #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "                magent_obs[player][\"advantages\"] = advantages\n",
    "                magent_obs[player][\"returns\"] = advantages + torch.tensor(records[\"values\"])\n",
    "                    #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "        \n",
    "\n",
    "        # new logic, maybe we do this after a couple of games, so we get more data overall?\n",
    "        \n",
    "\n",
    "        # optimize the policy and the value network now\n",
    "        # we can take all our observations now and flatten them into one bigger list of individual transitions\n",
    "        # TODO: could make this setting into a single loop, but maybe this is clearer. ALso could make all these tensors earlier\n",
    "        b_observations = torch.cat([torch.stack(item['obs']) for item in magent_obs.values()])\n",
    "        b_logprobs = torch.cat([torch.stack(item['logprobs']) for item in magent_obs.values()])\n",
    "        b_actions = torch.cat([torch.stack(item['actions']) for item in magent_obs.values()])\n",
    "        b_returns = torch.cat([item['returns'] for item in magent_obs.values()])\n",
    "        b_values = torch.cat([torch.stack(item['values']) for item in magent_obs.values()])\n",
    "        b_advantages =  torch.cat([item['advantages'] for item in magent_obs.values()])\n",
    "\n",
    "\n",
    "\n",
    "        # b_index stands for batch index\n",
    "        b_index = np.arange(len(b_observations))\n",
    "        clip_fracs = []\n",
    "        for epoch in range(update_epochs):\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_observations), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                # TODO: batched actions, How to handle batched observations and acctions properly in the agent\n",
    "                #       Maybe a different \n",
    "\n",
    "                # newlogprob needs to return a list of logprobs\n",
    "                _, newlogprob, entropy, value = parallel_ppo_agent.get_batched_action_and_value(\n",
    "                    b_observations[batch_index], b_actions[batch_index])\n",
    "                \n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "                \n",
    "                # normalizing advantages\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = advantages.float()\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # policy loss\n",
    "                pg_loss1 = -advantages * ratio\n",
    "                pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # could move them from GPU here\n",
    "        y_pred, y_true = b_values.numpy(), b_returns.numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        \n",
    "        # if episode % 20 == 0:\n",
    "            # print(f\"Training episode {episode}\")\n",
    "            # #print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "            # #print(f\"Episode Length: {end_step}\")\n",
    "            # print(\"\")\n",
    "            # print(f\"Value Loss: {v_loss.item()}\")\n",
    "            # print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "            # print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "            # print(f\"Approx KL: {approx_kl.item()}\")\n",
    "            # print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "            # print(f\"Explained Variance: {explained_var.item()}\")\n",
    "            # print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "            # also check some stats and try to log these\n",
    "\n",
    "    # At the end, print some stuff here for overall stats\n",
    "\n",
    "    print(f'Average game length = {avg_game_length:.2f}')\n",
    "    print(f'Wolf wins : {wolf_wins}')\n",
    "    print(f'Villager wins: {villager_wins}')\n",
    "    print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "\n",
    "torch.save(parallel_ppo_agent, \"long_approval_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:07<00:00, 16.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 8689, Villager wins : 1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "# num_agents = 10\n",
    "# observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "# ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "ppo_agent = torch.load(\"approval_agent\")\n",
    "\n",
    "wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=10000)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the random wolf and random villager 100 times for 1000 games and average the villager wins.\n",
    "We will do the same for the approval agent, and the long_approval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:36<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 65.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "\n",
    "    villager_wins = 0\n",
    "    wolf_wins = 0\n",
    "    for _ in range(num_games):\n",
    "        env.reset()\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        \n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            day = observation['observation']['day']\n",
    "            phase = observation['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "            if role == Roles.WEREWOLF:\n",
    "                action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                wolf_brain['action'] = action\n",
    "            else:\n",
    "                action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        # get some stats\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    villager_win_list.append(villager_wins)    \n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')\n",
    "# print(f'Average game length = {avg_game_length:.2f}')\n",
    "# print(f'Wolf wins : {wolf_wins}')\n",
    "# print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 65.04\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.81it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.32it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.29it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.13it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.19it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.24it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.84it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 14.93it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.82it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.79it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 14.97it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.30it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.97it/s]\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.62it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.05it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.32it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.38it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.37it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.23it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.27it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.33it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.36it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.98it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.69it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.52it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.49it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.24it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.62it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.00it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.36it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.44it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.30it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.73it/s]\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.57it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.13it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.41it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.04it/s]\n",
      "100%|██████████| 1000/1000 [01:17<00:00, 12.92it/s]\n",
      "100%|██████████| 1000/1000 [01:18<00:00, 12.81it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.42it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.93it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.94it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.74it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.98it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.75it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.87it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.04it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.84it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.56it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.57it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.30it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.39it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.58it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.47it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.37it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.50it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.26it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.47it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.50it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.62it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.60it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 100/100 [1:44:44<00:00, 62.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 132.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "ppo_agent = torch.load(\"approval_agent\")\n",
    "\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "    wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=num_games)\n",
    "    villager_win_list.append(vwins)\n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 132.29\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:02<00:00, 15.93it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.01it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.21it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.19it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.08it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.12it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.14it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.14it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.12it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.98it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.23it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.08it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.08it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.14it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.21it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.99it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.99it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.01it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.23it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.01it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.19it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.16it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.19it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.99it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.16it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.08it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.29it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.25it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.19it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.12it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.27it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.21it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.21it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.28it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.25it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.15it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.21it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.02it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.24it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.01it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.18it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.26it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.07it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.24it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.29it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.17it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.10it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.16it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.19it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.13it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.16it/s]\n",
      "100%|██████████| 100/100 [1:43:19<00:00, 61.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 129.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "ppo_agent = torch.load(\"long_approval_agent\")\n",
    "\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "    wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=num_games)\n",
    "    villager_win_list.append(vwins)\n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 129.95\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 3,\n",
       " 'phase': 2,\n",
       " 'alive': ['player_2', 'player_4', 'player_6', 'player_8'],\n",
       " 'killed': ['player_7', 'player_0', 'player_3'],\n",
       " 'executed': ['player_1', 'player_9', 'player_5'],\n",
       " 'werewolves': ['player_4', 'player_6'],\n",
       " 'villagers': ['player_0',\n",
       "  'player_1',\n",
       "  'player_2',\n",
       "  'player_3',\n",
       "  'player_5',\n",
       "  'player_7',\n",
       "  'player_8',\n",
       "  'player_9'],\n",
       " 'votes': {'player_4': [0, 0, 0, -1, 1, 0, 1, 0, 0, 0],\n",
       "  'player_6': [0, 0, 0, -1, 1, 0, 1, 0, 0, 0]},\n",
       " 'winners': <Roles.WEREWOLF: 1>}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "villager_wins = [r for r in replay if r[-1][\"winners\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, values = zip(*Counter([item for sublist in [villager_win[-1][\"werewolves\"] for villager_win in villager_wins] for item in sublist]).items())\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes + width * 0.5, labels)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_2', 'player_4']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villager_wins[0][-1][\"werewolves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
