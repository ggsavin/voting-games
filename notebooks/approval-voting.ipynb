{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, Roles, Phase\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approval Voting\n",
    "\n",
    "Blurb on approval voting goes here...\n",
    "\n",
    "We want to see how an static agents vs static wolves fare, before training our PPO agents to hopefully learn to do better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Baselines\n",
    "\n",
    "To properly asses our agents, we need baselines. For this purpose we have totally random villagers and semi-random villagers that will only vote for agents that are currently alive.\n",
    "\n",
    "As for wolves, we have the following behaviors:\n",
    "- random wolves that coordinate and each target one villager while approve themselves. The remaining villagers get neutral rankings\n",
    "- hyper aggressive wolves that simply disapprove of every single non-wolf player\n",
    "- random wolves that do whatever\n",
    "- revenge wolves that coordinate and target a villager that targetted them. Choose a revenge target randomly each round"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approval voting is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Werewolf game with 10 agents, 2 werewolves, and 1 accusation step\n",
      "Wolf Strategy         Random Alive Target Villager    Random Villager\n",
      "------------------  ------------------------------  -----------------\n",
      "Coordinated Wolves                           0.136              0.077\n",
      "Aggressive Wolves                            0.029              0.006\n",
      "Random Wolves                                0.667              0.62\n",
      "\n",
      "Werewolf game with 15 agents, 3 werewolves, and 1 accusation step\n",
      "Wolf Strategy         Random Alive Target Villager    Random Villager\n",
      "------------------  ------------------------------  -----------------\n",
      "Coordinated Wolves                           0.033              0.03\n",
      "Aggressive Wolves                            0.003              0\n",
      "Random Wolves                                0.703              0.601\n"
     ]
    }
   ],
   "source": [
    "def random_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def aggressive_wolf(env, agent, action=None):\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "    action = [-1] * len(env.possible_agents)\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def revenge_wolf(env, agent, action = None, coordinated=False):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    # TODO:\n",
    "    return None\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent, action=None):\n",
    "   return env.action_space(agent).sample().tolist()\n",
    "\n",
    "\n",
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100):\n",
    "\n",
    "    villager_wins = 0\n",
    "    game_replays = []\n",
    "    for _ in range(num_times):\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        game_replays.append(copy.deepcopy(env.history))\n",
    "\n",
    "    return villager_wins, game_replays\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2, num_accusations=1)\n",
    "env.reset()\n",
    "\n",
    "coordinated_wolves = []\n",
    "coordinated_wolves.append(play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "coordinated_wolves.append(play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "aggressive_wolves = []\n",
    "aggressive_wolves.append(play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "aggressive_wolves.append(play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "random_wolves = []\n",
    "random_wolves.append(play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "random_wolves.append(play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "print(\"Werewolf game with 10 agents, 2 werewolves, and 1 accusation step\")\n",
    "print(tabulate([['Coordinated Wolves', *coordinated_wolves], ['Aggressive Wolves', *aggressive_wolves], ['Random Wolves', *random_wolves]], \n",
    "               headers=[\"Wolf Strategy\", \"Random Alive Target Villager\", \"Random Villager\"]))\n",
    "\n",
    "\n",
    "env = pare(num_agents=15, werewolves=3, num_accusations=1)\n",
    "env.reset()\n",
    "\n",
    "coordinated_wolves = []\n",
    "coordinated_wolves.append(play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "coordinated_wolves.append(play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "aggressive_wolves = []\n",
    "aggressive_wolves.append(play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "aggressive_wolves.append(play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "random_wolves = []\n",
    "random_wolves.append(play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)[0]/1000.0)\n",
    "random_wolves.append(play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)[0]/1000.0)\n",
    "\n",
    "print(\"\\nWerewolf game with 15 agents, 3 werewolves, and 1 accusation step\")\n",
    "print(tabulate([['Coordinated Wolves', *coordinated_wolves], ['Aggressive Wolves', *aggressive_wolves], ['Random Wolves', *random_wolves]], \n",
    "               headers=[\"Wolf Strategy\", \"Random Alive Target Villager\", \"Random Villager\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the most realistic scenario, random coordinated wolves vs. single target random villagers (that do not target dead players) we see villagers winning under 13% of the time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained/Training agents \n",
    "\n",
    "Agents were trained using an LSTM to have a history of moves to hopefully elucidate who the wolves are. There were very many hyperparameters to choose from with the overarching goal being the impact gameplay settings such as number of accusation rounds would have on the learning and the agents ability to win and to implicitly communicate between eachother.\n",
    "\n",
    "We want to also see what impacts different gameplay settings may have on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
