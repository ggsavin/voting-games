{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import parrallel_raw_env, Roles\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, approval_states, num_players, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256, approval_states), std=0.01),\n",
    "        )\n",
    "\n",
    "        self.num_players = num_players\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        # TODO: We need torch.mean because PPO will use value, and we have a bunch here. \n",
    "        #       Do we need to change PPO here?\n",
    "        return torch.mean(self.critic(torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])))\n",
    "    \n",
    "    # only doing this for the PPO batched call so I don't need extra logic in the regular get action and value\n",
    "    def get_batched_action_and_value(self, x, actions=None):\n",
    "\n",
    "        if actions is None:\n",
    "            raise ValueError(\"We need batched actions here\")\n",
    "\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        critics = []\n",
    "        for current_obs, action in zip(x, actions):\n",
    "            updated_obs = torch.stack([torch.cat((torch.tensor([i]), current_obs)) for i in range(self.num_players)])\n",
    "\n",
    "            logits = self.actor(updated_obs)\n",
    "            probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "            \n",
    "            # update our return tensors\n",
    "            log_probs.append(torch.sum(probs.log_prob(action)))\n",
    "            entropies.append(torch.prod(probs.entropy()))\n",
    "            critics.append(torch.mean(self.critic(updated_obs)))\n",
    "            \n",
    "        return actions, torch.stack(log_probs), torch.stack(entropies), torch.stack(critics)\n",
    "\n",
    "    def convert_actions_to_approvals(self, actions):\n",
    "        return [-1 if a == 2 else a.item() for a in actions]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        # could call the network each time, with a different integer for each player?  get approvals that way\n",
    "        # x is the flattened observation. we should go ahead and run each of the player_ids appended to full obs to get multiple classifications\n",
    "        # how  to handle entropy here? maybe we multiply all the probs, and then calculate the overall entropy\n",
    "        # self.critic needs to be changed too, to return an array\n",
    "\n",
    "        # option to have critic/actors for every single player?\n",
    "\n",
    "        # option to also delevt n-1 * n-2 for -1s on the wolf\n",
    "        \n",
    "        # get logits for every single player in the game.\n",
    "        x = torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        # we multiply the entropy, and we add the log_probs together\n",
    "        # TODO: multiple values for critic. should I average?\n",
    "        return action, torch.sum(probs.log_prob(action)), torch.prod(probs.entropy()), torch.mean(self.critic(x))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    player_status = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(player_status, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    player = random.choice(legal_actions)\n",
    "\n",
    "    action = [0] * len(action_mask)\n",
    "    action[me] = 1\n",
    "    action[player] = -1\n",
    "    return action\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == -1 and i == me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "    choice = [-1] * len(observation['action_mask'])\n",
    "\n",
    "    choice[me] = 1\n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1]\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected =  random.choice(villagers_alive)\n",
    "    choice = [0] * len(observation['observation']['player_status'])\n",
    "    \n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    choice[player_selected] = -1\n",
    "\n",
    "    return action if action != None else choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:34<00:00, 292.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 9268\n",
      "Villager wins: 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 10000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 430.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.04\n",
      "Wolf wins : 997\n",
      "Villager wins: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 643\n",
      "Villager wins: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = parrallel_raw_env(num_agents=10, werewolves=1)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on approval\n",
    "\n",
    "Because we need to generate approvals for every other agent, we will have to call the neural network n-1 times. We should look at batching this, as well as generating the proper observation\n",
    "\n",
    "maybe we have to call the model n times, but then add the loss together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villager wins 0: 100%|██████████| 2/2 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 2, Villager wins : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def play_and_return_stats(env, wolf_policy, agent_policy, num_games=1000):\n",
    "    wolf_wins = 0\n",
    "    villager_wins = 0\n",
    "\n",
    "    game_replays = []\n",
    "    loop = tqdm(range(num_games))\n",
    "    for _ in loop:\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : {\"self_votes\": 0, \"dead_votes\": 0, \"lasted_for\": 0} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = wolf_policy(observation, None, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    # action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = agent_policy.get_action_and_value(obs)\n",
    "                        action = agent_policy.convert_actions_to_approvals(action)\n",
    "                    else:\n",
    "                        action = None\n",
    "                        \n",
    "                env.step(action)\n",
    "            \n",
    "        game_replays.append(env.history)\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        loop.set_description(f\"Villager wins {villager_wins}\")\n",
    "        \n",
    "    return wolf_wins, villager_wins, game_replays\n",
    "\n",
    "\n",
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "# ppo_agent = torch.load(\"ppo_agent\")\n",
    "\n",
    "wwins, vwins, game_replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=2)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#mlflow.set_tracking_uri(\"http://mlflow:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [2:06:11<00:00,  3.96it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.19\n",
      "Wolf wins : 27777\n",
      "Villager wins: 2223\n",
      "Avg amount of self votes a game across villagers: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 32 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 10000 #\n",
    "update_epochs = 3 #\n",
    "\n",
    "# stats to keep track of for custom metrics\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "parallel_ppo_agent = Agent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(ppo_agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Approval Training Random Wolf behavior'):\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            self_votes = 0\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : [] for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    game_action = random_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = game_action\n",
    "                else:\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = parallel_ppo_agent.get_action_and_value(obs)\n",
    "                        game_action = parallel_ppo_agent.convert_actions_to_approvals(action)\n",
    "                    else:\n",
    "                        action = None\n",
    "                        game_action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : THIS only works if you have a -1 approval for your own ID\n",
    "                    if action != None and action[observation['observation']['self_id']] == -1:\n",
    "                        self_votes += 1\n",
    "                \n",
    "                    # TODO : Do not care about dead votes yet\n",
    "                    # if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                    #     dead_votes += 1\n",
    "\n",
    "                    magent_list[magent].append({\n",
    "                        \"obs\": obs, \n",
    "                        \"action\": action,\n",
    "                        \"prev_reward\": reward,\n",
    "                        \"logprobs\": logprobs,\n",
    "                        \"term\": termination,\n",
    "                        \"value\": value\n",
    "                        })\n",
    "\n",
    "                env.step(game_action)\n",
    "            \n",
    "            # take the sequential observations of each agent, and store them appropriately\n",
    "            magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': [], 'terms': []} for agent in magent_list}\n",
    "            for key, value in magent_list.items():\n",
    "                # print(f'-- {key} --')\n",
    "                for s1, s2 in zip(value, value[1:]):\n",
    "                    magent_obs[key]['obs'].append(s1['obs'])\n",
    "                    magent_obs[key]['rewards'].append(s2['prev_reward'])\n",
    "                    magent_obs[key]['actions'].append(s1['action'])\n",
    "                    magent_obs[key]['logprobs'].append(s1['logprobs'])\n",
    "                    magent_obs[key]['values'].append(s1['value'])\n",
    "                    magent_obs[key]['terms'].append(s2['term'])\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        self_voting.append(self_votes)\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "        avg_game_length += (day * 1.0)/total_episodes\n",
    "        # END OF POST GAME STATS #\n",
    "\n",
    "        mlflow.log_metric(\"avg_game_len\", f'{avg_game_length:.2f}')\n",
    "        mlflow.log_metric(\"avg_self_votes\", f'{sum(self_voting)/len(self_voting)}')\n",
    "        mlflow.log_metric(\"wolf wins\", wolf_wins)\n",
    "        mlflow.log_metric(\"villager wins\", villager_wins)\n",
    "        if episode % 50 == 0:\n",
    "            #wwins, vwins = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=100)\n",
    "            #mlflow.log_metric(\"wwins\", wwins)\n",
    "            #mlflow.log_metric(\"vwins\", vwins)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "\n",
    "        # We will do this for each agent in the episode\n",
    "        # essentially we are calculating advantages and returns\n",
    "        with torch.no_grad():\n",
    "            for player, records in magent_obs.items():\n",
    "                # print(f'{records}')\n",
    "                advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "\n",
    "                for t in reversed(range(len(records['obs']))):\n",
    "                    # print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                    # not using terms, as these are episodic\n",
    "\n",
    "                    ## this was the last one. We are not using any terminal states in a good way\n",
    "\n",
    "                    if t == len(records['obs']) - 1:\n",
    "                        #print(f'T: {t} - Rewards at end : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions at end : {torch.tensor(records[\"actions\"])[t]} ')\n",
    "                        delta = records[\"rewards\"][t] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta\n",
    "                    else:\n",
    "                        #print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions : {torch.tensor(records[\"actions\"])[t]} ')                    \n",
    "                        delta = records[\"rewards\"][t] + gamma * records[\"values\"][t+1] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta + gamma * gamma * advantages[t+1]\n",
    "\n",
    "                    #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "                magent_obs[player][\"advantages\"] = advantages\n",
    "                magent_obs[player][\"returns\"] = advantages + torch.tensor(records[\"values\"])\n",
    "                    #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "        \n",
    "\n",
    "        # new logic, maybe we do this after a couple of games, so we get more data overall?\n",
    "        \n",
    "\n",
    "        # optimize the policy and the value network now\n",
    "        # we can take all our observations now and flatten them into one bigger list of individual transitions\n",
    "        # TODO: could make this setting into a single loop, but maybe this is clearer. ALso could make all these tensors earlier\n",
    "        b_observations = torch.cat([torch.stack(item['obs']) for item in magent_obs.values()])\n",
    "        b_logprobs = torch.cat([torch.stack(item['logprobs']) for item in magent_obs.values()])\n",
    "        b_actions = torch.cat([torch.stack(item['actions']) for item in magent_obs.values()])\n",
    "        b_returns = torch.cat([item['returns'] for item in magent_obs.values()])\n",
    "        b_values = torch.cat([torch.stack(item['values']) for item in magent_obs.values()])\n",
    "        b_advantages =  torch.cat([item['advantages'] for item in magent_obs.values()])\n",
    "\n",
    "\n",
    "\n",
    "        # b_index stands for batch index\n",
    "        b_index = np.arange(len(b_observations))\n",
    "        clip_fracs = []\n",
    "        for epoch in range(update_epochs):\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_observations), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                # TODO: batched actions, How to handle batched observations and acctions properly in the agent\n",
    "                #       Maybe a different \n",
    "\n",
    "                # newlogprob needs to return a list of logprobs\n",
    "                _, newlogprob, entropy, value = parallel_ppo_agent.get_batched_action_and_value(\n",
    "                    b_observations[batch_index], b_actions[batch_index])\n",
    "                \n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "                \n",
    "                # normalizing advantages\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = advantages.float()\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # policy loss\n",
    "                pg_loss1 = -advantages * ratio\n",
    "                pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # could move them from GPU here\n",
    "        y_pred, y_true = b_values.numpy(), b_returns.numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        \n",
    "        # if episode % 20 == 0:\n",
    "            # print(f\"Training episode {episode}\")\n",
    "            # #print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "            # #print(f\"Episode Length: {end_step}\")\n",
    "            # print(\"\")\n",
    "            # print(f\"Value Loss: {v_loss.item()}\")\n",
    "            # print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "            # print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "            # print(f\"Approx KL: {approx_kl.item()}\")\n",
    "            # print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "            # print(f\"Explained Variance: {explained_var.item()}\")\n",
    "            # print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "            # also check some stats and try to log these\n",
    "\n",
    "    # At the end, print some stuff here for overall stats\n",
    "\n",
    "    print(f'Average game length = {avg_game_length:.2f}')\n",
    "    print(f'Wolf wins : {wolf_wins}')\n",
    "    print(f'Villager wins: {villager_wins}')\n",
    "    print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "\n",
    "torch.save(parallel_ppo_agent, \"long_approval_agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:07<00:00, 16.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 8689, Villager wins : 1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "# num_agents = 10\n",
    "# observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "# ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "ppo_agent = torch.load(\"approval_agent\")\n",
    "\n",
    "wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=10000)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the random wolf and random villager 100 times for 1000 games and average the villager wins.\n",
    "We will do the same for the approval agent, and the long_approval_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:36<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 65.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "\n",
    "    villager_wins = 0\n",
    "    wolf_wins = 0\n",
    "    for _ in range(num_games):\n",
    "        env.reset()\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        \n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            day = observation['observation']['day']\n",
    "            phase = observation['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "            if role == Roles.WEREWOLF:\n",
    "                action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                wolf_brain['action'] = action\n",
    "            else:\n",
    "                action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        # get some stats\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    villager_win_list.append(villager_wins)    \n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')\n",
    "# print(f'Average game length = {avg_game_length:.2f}')\n",
    "# print(f'Wolf wins : {wolf_wins}')\n",
    "# print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 65.04\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.81it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.32it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.29it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.13it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.19it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.24it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.84it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 14.93it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.82it/s]\n",
      "100%|██████████| 1000/1000 [01:07<00:00, 14.79it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 14.97it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.30it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.97it/s]\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.62it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.05it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.76it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.32it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.38it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.37it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.23it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.27it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.33it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.36it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.20it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.98it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.69it/s]\n",
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.52it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.49it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.24it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.62it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.00it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.22it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.36it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.44it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.30it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.73it/s]\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.57it/s]\n",
      "100%|██████████| 1000/1000 [01:06<00:00, 15.13it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.41it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.04it/s]\n",
      "100%|██████████| 1000/1000 [01:17<00:00, 12.92it/s]\n",
      "100%|██████████| 1000/1000 [01:18<00:00, 12.81it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.42it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.93it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.94it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.74it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 15.98it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.75it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.87it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.09it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.11it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.06it/s]\n",
      "100%|██████████| 1000/1000 [01:02<00:00, 16.04it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.84it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.56it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.57it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.30it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.45it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.39it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.46it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.43it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.58it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.47it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.35it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.37it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.50it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.26it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.42it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.47it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.50it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.62it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.51it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.60it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.31it/s]\n",
      "100%|██████████| 1000/1000 [01:00<00:00, 16.41it/s]\n",
      "100%|██████████| 100/100 [1:44:44<00:00, 62.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 132.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "ppo_agent = torch.load(\"approval_agent\")\n",
    "\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "    wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=num_games)\n",
    "    villager_win_list.append(vwins)\n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 132.29\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villager wins 9: 100%|██████████| 100/100 [00:07<00:00, 13.56it/s]\n",
      "Villager wins 9: 100%|██████████| 100/100 [00:07<00:00, 13.36it/s]\n",
      "Villager wins 5: 100%|██████████| 100/100 [00:06<00:00, 14.43it/s]\n",
      "Villager wins 6: 100%|██████████| 100/100 [00:07<00:00, 14.15it/s]\n",
      "Villager wins 5: 100%|██████████| 100/100 [00:07<00:00, 13.96it/s]\n",
      "Villager wins 14: 100%|██████████| 100/100 [00:07<00:00, 12.51it/s]\n",
      "Villager wins 8: 100%|██████████| 100/100 [00:07<00:00, 13.17it/s]\n",
      "Villager wins 5: 100%|██████████| 100/100 [00:07<00:00, 12.95it/s]\n",
      "Villager wins 7: 100%|██████████| 100/100 [00:07<00:00, 13.74it/s]\n",
      "Villager wins 7: 100%|██████████| 100/100 [00:07<00:00, 13.89it/s]\n",
      "100%|██████████| 10/10 [01:13<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "ppo_agent = torch.load(\"long_approval_agent\")\n",
    "\n",
    "\n",
    "num_times = 10\n",
    "num_games = 100\n",
    "avg_game_length = 0\n",
    "\n",
    "villager_win_list = []\n",
    "for _ in tqdm(range(num_times)):\n",
    "    wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=num_games)\n",
    "    villager_win_list.append(vwins)\n",
    "\n",
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average villager wins in 1000 games : 129.95\n"
     ]
    }
   ],
   "source": [
    "print(f' Average villager wins in 1000 games : {np.mean(villager_win_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 3,\n",
       " 'phase': 2,\n",
       " 'alive': ['player_2', 'player_4', 'player_6', 'player_8'],\n",
       " 'killed': ['player_7', 'player_0', 'player_3'],\n",
       " 'executed': ['player_1', 'player_9', 'player_5'],\n",
       " 'werewolves': ['player_4', 'player_6'],\n",
       " 'villagers': ['player_0',\n",
       "  'player_1',\n",
       "  'player_2',\n",
       "  'player_3',\n",
       "  'player_5',\n",
       "  'player_7',\n",
       "  'player_8',\n",
       "  'player_9'],\n",
       " 'votes': {'player_4': [0, 0, 0, -1, 1, 0, 1, 0, 0, 0],\n",
       "  'player_6': [0, 0, 0, -1, 1, 0, 1, 0, 0, 0]},\n",
       " 'winners': <Roles.WEREWOLF: 1>}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "villager_wins = [r for r in replay if r[-1][\"winners\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, values = zip(*Counter([item for sublist in [villager_win[-1][\"werewolves\"] for villager_win in villager_wins] for item in sublist]).items())\n",
    "indexes = np.arange(len(labels))\n",
    "width = num_agents\n",
    "\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes + width * 0.5, labels)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "ppo_agent = torch.load(\"long_approval_agent\")\n",
    "\n",
    "\n",
    "num_times = 100\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "\n",
    "wwins, vwins, replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "villager_wins = [r for r in replay if r[-1][\"winners\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(villager_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'day': 1,\n",
       "  'phase': <Phase.ACCUSATION: 0>,\n",
       "  'alive': ['player_0',\n",
       "   'player_1',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'killed': [],\n",
       "  'executed': [],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {},\n",
       "  'winners': None},\n",
       " {'day': 1,\n",
       "  'phase': <Phase.ACCUSATION: 0>,\n",
       "  'alive': ['player_0',\n",
       "   'player_1',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'killed': [],\n",
       "  'executed': [],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_0': [-1, -1, -1, 0, 0, -1, -1, 1, -1, -1],\n",
       "   'player_1': [0, 1, 0, 1, 0, -1, 0, 0, 0, 0],\n",
       "   'player_2': [1, 0, 0, 1, 0, 1, -1, 1, -1, -1],\n",
       "   'player_3': [0, 1, 0, 1, 0, -1, 0, 0, 0, 0],\n",
       "   'player_4': [-1, -1, 1, -1, 0, 1, 0, -1, 0, 0],\n",
       "   'player_5': [-1, 0, -1, -1, -1, -1, 1, -1, -1, -1],\n",
       "   'player_6': [1, 0, 0, 0, 1, -1, -1, 1, -1, 0],\n",
       "   'player_7': [-1, -1, -1, 1, 0, -1, 1, 0, -1, -1],\n",
       "   'player_8': [-1, -1, -1, 0, 1, 1, 0, 0, -1, -1],\n",
       "   'player_9': [0, 0, 0, 1, -1, 0, -1, 0, 0, -1]},\n",
       "  'winners': None},\n",
       " {'day': 1,\n",
       "  'phase': 1,\n",
       "  'alive': ['player_0',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'killed': [],\n",
       "  'executed': ['player_1'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_0': [1, 0, 1, -1, -1, 1, -1, -1, 1, 0],\n",
       "   'player_1': [0, 1, 0, 1, 0, -1, 0, 0, 0, 0],\n",
       "   'player_2': [1, -1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
       "   'player_3': [0, 1, 0, 1, 0, -1, 0, 0, 0, 0],\n",
       "   'player_4': [0, -1, 1, -1, 0, 0, 1, 0, 0, 1],\n",
       "   'player_5': [0, -1, -1, 1, 1, 1, 0, 1, 0, 1],\n",
       "   'player_6': [-1, 1, -1, 0, -1, -1, 1, 0, -1, 0],\n",
       "   'player_7': [0, -1, 0, 1, -1, 0, 1, 1, 1, 0],\n",
       "   'player_8': [0, -1, -1, -1, 1, -1, -1, 0, -1, 1],\n",
       "   'player_9': [1, 0, -1, 0, 0, 1, 0, -1, -1, 0]},\n",
       "  'winners': None},\n",
       " {'day': 1,\n",
       "  'phase': 2,\n",
       "  'alive': ['player_0',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'killed': ['player_5'],\n",
       "  'executed': ['player_1'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_3': [0, 1, 0, 1, 0, -1, 0, 0, 0, 0]},\n",
       "  'winners': None},\n",
       " {'day': 2,\n",
       "  'phase': 0,\n",
       "  'alive': ['player_0',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'killed': ['player_5'],\n",
       "  'executed': ['player_1'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_0': [-1, -1, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "   'player_2': [1, 1, -1, 1, 0, 0, -1, -1, 1, -1],\n",
       "   'player_3': [0, 1, 0, 1, 0, 0, -1, 0, 0, 0],\n",
       "   'player_4': [1, -1, -1, 0, 0, -1, 0, -1, 0, -1],\n",
       "   'player_6': [0, 1, 1, -1, 0, 1, -1, -1, 1, 1],\n",
       "   'player_7': [0, -1, 1, 1, 1, 1, 0, -1, 0, 1],\n",
       "   'player_8': [0, 1, 0, 0, -1, 0, 0, 1, 0, 0],\n",
       "   'player_9': [1, -1, 0, 1, 0, -1, 1, 1, 0, -1]},\n",
       "  'winners': None},\n",
       " {'day': 2,\n",
       "  'phase': 1,\n",
       "  'alive': ['player_0',\n",
       "   'player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8'],\n",
       "  'killed': ['player_5'],\n",
       "  'executed': ['player_1', 'player_9'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_0': [0, -1, -1, -1, 1, 1, 0, 0, 1, -1],\n",
       "   'player_2': [0, -1, -1, 1, 1, 0, 0, 1, 1, -1],\n",
       "   'player_3': [0, 1, 0, 1, 0, 0, 0, 0, 0, -1],\n",
       "   'player_4': [0, -1, 1, 0, 0, 1, 1, 0, 0, 1],\n",
       "   'player_6': [-1, -1, 1, 1, -1, 0, 1, -1, 0, 0],\n",
       "   'player_7': [1, -1, 1, 1, 0, 1, -1, 1, 0, -1],\n",
       "   'player_8': [0, 1, 0, 1, 0, -1, 1, -1, 0, 1],\n",
       "   'player_9': [1, -1, 1, 1, 0, 1, 1, 0, 0, -1]},\n",
       "  'winners': None},\n",
       " {'day': 2,\n",
       "  'phase': 2,\n",
       "  'alive': ['player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8'],\n",
       "  'killed': ['player_5', 'player_0'],\n",
       "  'executed': ['player_1', 'player_9'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_3': [-1, 1, 0, 1, 0, 0, 0, 0, 0, 0]},\n",
       "  'winners': None},\n",
       " {'day': 3,\n",
       "  'phase': 0,\n",
       "  'alive': ['player_2',\n",
       "   'player_3',\n",
       "   'player_4',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8'],\n",
       "  'killed': ['player_5', 'player_0'],\n",
       "  'executed': ['player_1', 'player_9'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_2': [-1, -1, 0, -1, -1, 1, 1, 0, -1, 1],\n",
       "   'player_3': [0, 1, 0, 1, 0, 0, 0, -1, 0, 0],\n",
       "   'player_4': [-1, 0, 0, -1, -1, 0, 0, 0, 0, 1],\n",
       "   'player_6': [0, 0, -1, -1, 1, 1, 1, 1, -1, 1],\n",
       "   'player_7': [1, 1, 1, 0, 1, -1, -1, 1, -1, -1],\n",
       "   'player_8': [0, 0, 0, 1, 0, 0, 0, 0, 0, 1]},\n",
       "  'winners': None},\n",
       " {'day': 3,\n",
       "  'phase': 1,\n",
       "  'alive': ['player_2', 'player_3', 'player_4', 'player_6', 'player_7'],\n",
       "  'killed': ['player_5', 'player_0'],\n",
       "  'executed': ['player_1', 'player_9', 'player_8'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_2': [-1, -1, -1, 0, 1, 0, 0, 0, 1, 0],\n",
       "   'player_3': [0, 1, 0, 1, 0, 0, 0, 0, -1, 0],\n",
       "   'player_4': [1, 1, -1, -1, 1, 1, 1, -1, -1, 0],\n",
       "   'player_6': [-1, 0, 0, -1, 1, -1, 0, 1, 0, 1],\n",
       "   'player_7': [1, -1, -1, 1, 1, 1, -1, -1, -1, 0],\n",
       "   'player_8': [0, -1, 1, -1, 1, 1, 1, 0, 0, 0]},\n",
       "  'winners': None},\n",
       " {'day': 3,\n",
       "  'phase': 2,\n",
       "  'alive': ['player_2', 'player_3', 'player_4', 'player_7'],\n",
       "  'killed': ['player_5', 'player_0', 'player_6'],\n",
       "  'executed': ['player_1', 'player_9', 'player_8'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_3': [0, 1, 0, 1, 0, 0, -1, 0, 0, 0]},\n",
       "  'winners': None},\n",
       " {'day': 4,\n",
       "  'phase': 0,\n",
       "  'alive': ['player_2', 'player_3', 'player_4', 'player_7'],\n",
       "  'killed': ['player_5', 'player_0', 'player_6'],\n",
       "  'executed': ['player_1', 'player_9', 'player_8'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_2': [1, 1, -1, 1, 0, 0, 1, 1, -1, 1],\n",
       "   'player_3': [0, 1, 0, 1, 0, 0, 0, -1, 0, 0],\n",
       "   'player_4': [0, 0, -1, 1, 0, 1, 0, -1, 0, -1],\n",
       "   'player_7': [1, 0, -1, 0, -1, -1, -1, 0, 1, 0]},\n",
       "  'winners': None},\n",
       " {'day': 4,\n",
       "  'phase': 1,\n",
       "  'alive': ['player_2', 'player_4', 'player_7'],\n",
       "  'killed': ['player_5', 'player_0', 'player_6'],\n",
       "  'executed': ['player_1', 'player_9', 'player_8', 'player_3'],\n",
       "  'werewolves': ['player_1', 'player_3'],\n",
       "  'villagers': ['player_0',\n",
       "   'player_2',\n",
       "   'player_4',\n",
       "   'player_5',\n",
       "   'player_6',\n",
       "   'player_7',\n",
       "   'player_8',\n",
       "   'player_9'],\n",
       "  'votes': {'player_2': [1, 1, -1, -1, 1, 0, 1, -1, 1, 1],\n",
       "   'player_3': [0, 1, -1, 1, 0, 0, 0, 0, 0, 0],\n",
       "   'player_4': [0, -1, 0, -1, 1, 0, -1, 1, 0, 1],\n",
       "   'player_7': [-1, 1, 1, -1, 1, 0, -1, -1, 0, 1]},\n",
       "  'winners': <Roles.VILLAGER: 0>}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villager_wins[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
