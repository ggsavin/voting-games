{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import parrallel_raw_env, Roles\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, approval_states, num_players, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, approval_states), std=0.01),\n",
    "        )\n",
    "\n",
    "        self.num_players = num_players\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)]))\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        # could call the network each time, with a different integer for each player?  get approvals that way\n",
    "        # x is the flattened observation. we should go ahead and run each of the player_ids appended to full obs to get multiple classifications\n",
    "        # how  to handle entropy here? maybe we multiply all the probs, and then calculate the overall entropy\n",
    "        # self.critic needs to be changed too, to return an array\n",
    "\n",
    "        # option to have critic/actors for every single player?\n",
    "\n",
    "        # option to also delevt n-1 * n-2 for -1s on the wolf\n",
    "        \n",
    "        # get logits for every single player in the game.\n",
    "        x = torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        # we multiply the entropy, and we add the log_probs together\n",
    "        # TODO: multiple values for critic. should I average?\n",
    "        return action, torch.sum(probs.log_prob(action)), torch.prod(probs.entropy()), self.critic(x)\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    player_status = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(player_status, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    player = random.choice(legal_actions)\n",
    "\n",
    "    action = [0] * len(action_mask)\n",
    "    action[me] = 1\n",
    "    action[player] = -1\n",
    "    return action\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == -1 and i == me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "    choice = [-1] * len(observation['action_mask'])\n",
    "\n",
    "    choice[me] = 1\n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1]\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    player_selected =  random.choice(villagers_alive)\n",
    "    choice = [0] * len(observation['observation']['player_status'])\n",
    "    \n",
    "    for wid in wolf_ids:\n",
    "        choice[wid] = 1\n",
    "\n",
    "    choice[player_selected] = -1\n",
    "\n",
    "    return action if action != None else choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 439.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.16\n",
      "Wolf wins : 943\n",
      "Villager wins: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 430.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.04\n",
      "Wolf wins : 997\n",
      "Villager wins: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 451.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 643\n",
      "Villager wins: 357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = parrallel_raw_env(num_agents=10, werewolves=1)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on approval\n",
    "\n",
    "Because we need to generate approvals for every other agent, we will have to call the neural network n-1 times. We should look at batching this, as well as generating the proper observation\n",
    "\n",
    "maybe we have to call the model n times, but then add the loss together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 2, Villager wins : 0\n"
     ]
    }
   ],
   "source": [
    "def play_and_return_stats(env, wolf_policy, agent_policy, num_games=1000):\n",
    "    wolf_wins = 0\n",
    "    villager_wins = 0\n",
    "\n",
    "    game_replays = []\n",
    "    for _ in range(num_games):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : {\"self_votes\": 0, \"dead_votes\": 0, \"lasted_for\": 0} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = wolf_policy(observation, None, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    # action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "                    obs = torch.Tensor(env.convert_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = agent_policy.get_action_and_value(obs)\n",
    "                        action = action.tolist()\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : maybe make these callbacks?\n",
    "                    if action == observation['observation']['self_id']:\n",
    "                        magent_list[magent]['self_votes'] += 1\n",
    "                \n",
    "                    if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                        magent_list[magent]['dead_votes'] += 1\n",
    "                        \n",
    "                env.step(action)\n",
    "            \n",
    "        game_replays.append(env.history)\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    return wolf_wins, villager_wins, game_replays\n",
    "\n",
    "\n",
    "env = parrallel_raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "# ppo_agent = torch.load(\"ppo_agent\")\n",
    "\n",
    "wwins, vwins, game_replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=2)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
