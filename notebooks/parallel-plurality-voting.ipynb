{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, plurality_Phase, plurality_Role\n",
    "import random\n",
    "import copy\n",
    "from typing import Any, Generator, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_coordinated_wolf(env, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    return {wolf: int(target.split(\"_\")[-1]) for wolf in wolves_remaining}\n",
    "\n",
    "def random_wolfs(env):\n",
    "    return {wolf: env.action_space(wolf).sample() for\n",
    "            wolf in set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])}\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    \n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    return int(random.choice(list(targets)).split(\"_\")[-1])\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent, action=None):\n",
    "   return env.action_space(agent).sample()\n",
    "\n",
    "def random_coordinated_single_wolf(env, agent, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    return action if action != None else int(random.choice(list(villagers_remaining)).split(\"_\")[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 396 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:04<00:00, 229.45it/s]\n",
      "Villagers won 302 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:04<00:00, 222.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(plurality_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            # at least one wolf\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "                \n",
    "            # wolf steps\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "play_static_wolf_game(env, random_coordinated_single_wolf, random_single_target_villager, num_times=1000)\n",
    "play_static_wolf_game(env, random_coordinated_single_wolf, random_agent_action, num_times=1000)\n",
    "\n",
    "# print(\"Random Coordinated Wolves\")\n",
    "# print(\"\\t vs. Single Target Random Villagers\")\n",
    "# play_static_wolf_game(env, random_coordinated_wolf, random_single_target_villager, num_times=1000)\n",
    "# print(\"\\t vs. Random Villagers\")\n",
    "# play_static_wolf_game(env, random_coordinated_wolf, random_agent_action, num_times=1000)\n",
    "# print(\"------------------------------------\\n\")\n",
    "# print(\"Random Wolves\")\n",
    "# print(\"\\t vs. Single Target Random Villagers\")\n",
    "# play_static_wolf_game(env, random_wolfs, random_single_target_villager, num_times=1000)\n",
    "# print(\"\\t vs. Random Villagers\")\n",
    "# play_static_wolf_game(env, random_wolfs, random_agent_action, num_times=1000)\n",
    "# print(\"------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None):\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, num_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityRecurrentAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None, hidden_state_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # actor\n",
    "        self.a_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.a_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.a_fc2 = self._layer_init(torch.nn.Linear(hidden_state_size,num_actions), std=0.01)\n",
    "\n",
    "        # critic\n",
    "        self.c_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.c_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.c_fc2 = self._layer_init(torch.nn.Linear(hidden_state_size,1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, recurrent_cell:torch.tensor):\n",
    "        h = torch.tanh(self.c_fc1(x))\n",
    "        h, recurrent_cell = self.c_recurrent_layer(torch.unsqueeze(h,1), recurrent_cell)\n",
    "        h = torch.tanh(self.c_fc2(h))\n",
    "\n",
    "        # UPDATE THIS X\n",
    "        return h, recurrent_cell\n",
    "    \n",
    "    def get_action_and_value(self, x, a_recurrent_cell:torch.tensor, c_recurrent_cell:torch.tensor, action=None):\n",
    "        h = torch.tanh(self.a_fc1(x))\n",
    "        # we are strictly on a sequence length of 1 here, using prior information baked in\n",
    "        h, recurrent_cell = self.a_recurrent_layer(torch.unsqueeze(h,1), a_recurrent_cell)\n",
    "        h = torch.tanh(self.a_fc2(h))\n",
    "        probs = torch.distributions.categorical.Categorical(logits=h)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        c_val, c_rec = self.get_value(x, c_recurrent_cell)\n",
    "        # UPDATE THIS X\n",
    "        return action, probs.log_prob(action), probs.entropy(), recurrent_cell, c_val, c_rec\n",
    "    \n",
    "class PluralityRecurrentAgentv2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None, hidden_state_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # actor\n",
    "        self.a_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(128, hidden_state_size, batch_first=True))\n",
    "        self.a_fc1 = self._layer_init(torch.nn.Linear(obs_size, 128))\n",
    "        self.a_fc2 = self._layer_init(torch.nn.Linear(hidden_state_size,num_actions), std=0.01)\n",
    "\n",
    "        # critic\n",
    "        self.c_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(128, hidden_state_size, batch_first=True))\n",
    "        self.c_fc1 = self._layer_init(torch.nn.Linear(obs_size,128))\n",
    "        self.c_fc2 = self._layer_init(torch.nn.Linear(hidden_state_size,1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, recurrent_cell:torch.tensor):\n",
    "        h = torch.tanh(self.c_fc1(x))\n",
    "        h, recurrent_cell = self.c_recurrent_layer(torch.unsqueeze(h,1), recurrent_cell)\n",
    "        h = h.squeeze(1)\n",
    "        h = torch.tanh(self.c_fc2(h))\n",
    "\n",
    "        # UPDATE THIS X\n",
    "        return h, recurrent_cell\n",
    "    \n",
    "    def get_action_and_value(self, x, a_recurrent_cell:torch.tensor, c_recurrent_cell:torch.tensor, action=None):\n",
    "        h = torch.tanh(self.a_fc1(x))\n",
    "        h, recurrent_cell = self.a_recurrent_layer(torch.unsqueeze(h,1), a_recurrent_cell)\n",
    "        h = h.squeeze(1)\n",
    "        h = torch.tanh(self.a_fc2(h))\n",
    "        probs = torch.distributions.categorical.Categorical(logits=h)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        c_val, c_rec = self.get_value(x, c_recurrent_cell)\n",
    "        # UPDATE THIS X\n",
    "        return action, probs.log_prob(action), probs.entropy(), recurrent_cell, c_val, c_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 buffer_size: int, \n",
    "                 gamma: float, \n",
    "                 gae_lambda: float,\n",
    "                 is_recurrent: bool,\n",
    "                 recurrent_size: int = None,\n",
    "                 ):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        self.steps = []\n",
    "\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = None \n",
    "        self.critic_hcxs = None \n",
    "\n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.is_recurrent = is_recurrent\n",
    "        if self.is_recurrent:\n",
    "            self.recurrent_size = recurrent_size\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = []\n",
    "        self.critic_hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.actor_hcxs.append(game[\"hcxs\"][:-1])\n",
    "         self.critic_hcxs.append(game[\"c_hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.cat([item for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.cat([item for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages).float()\n",
    "        actor_hxs, actor_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.actor_hcxs for item in sublist]])\n",
    "        critic_hxs, critic_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.critic_hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"actor_hxs\": torch.swapaxes(torch.cat(actor_hxs)[batch_index],0,1),\n",
    "                \"actor_cxs\": torch.swapaxes(torch.cat(actor_cxs)[batch_index],0,1),\n",
    "                \"critic_hxs\": torch.swapaxes(torch.cat(critic_hxs)[batch_index],0,1),\n",
    "                \"critic_cxs\": torch.swapaxes(torch.cat(critic_cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None) -> RolloutBuffer:\n",
    "\n",
    "    buffer = RolloutBuffer(buffer_size=10, \n",
    "                           gamma=0.99, \n",
    "                           gae_lambda=0.95,\n",
    "                           is_recurrent=True)\n",
    "    buffer.reset()\n",
    "    \n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'a_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                              'c_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                actor_recurrent_cell = magent_obs[villager][\"a_hcxs\"][-1]\n",
    "                critic_recurrent_cell = magent_obs[villager][\"c_hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell = villager_agent.get_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n",
    "                actions[villager] = ml_action.item()\n",
    "\n",
    "                # can store some stuff \n",
    "                magent_obs[villager][\"obs\"].append(obs)\n",
    "                magent_obs[villager][\"actions\"].append(ml_action)\n",
    "                magent_obs[villager][\"logprobs\"].append(logprobs)\n",
    "                magent_obs[villager][\"values\"].append(c_val)\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"a_hcxs\"].append(actor_recurrent_cell)\n",
    "                magent_obs[villager][\"c_hcxs\"].append(critic_recurrent_cell)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "        \n",
    "\n",
    "# env = plurality_env(num_agents=10, werewolves=2)\n",
    "# observations, rewards, terminations, truncations, infos = env.reset()\n",
    "\n",
    "# obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "# rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "# def test_policy(obs, net, agent=None, env=None, a_rec=None, c_rec=None):\n",
    "#    # \n",
    "#    return rec_agent.get_action_and_value(obs, a_rec, c_rec)\n",
    "\n",
    "# def test_recurrent_policy(obs, agent=None, env=None):\n",
    "#     # we need to return the hx and cx from the model, chyou know? and also have an initial one of 0 to feed the model the first time\n",
    "#     pass\n",
    "\n",
    "\n",
    "# buff = fill_recurrent_buffer(env, random_coordinated_wolf, test_policy, num_times=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_minibatch_loss(agent: PluralityRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # TODO:Consider checking for NAans anywhere. we cant have these. also do this in the model itself\n",
    "    # if torch.isnan(tensor).any(): print(f\"{label} contains NaN values\")\n",
    "\n",
    "    # get new log probs need to pass in the recurrent cells as well for actor and critic\n",
    "    _, logprobs, entropies, _, values, _ = agent.get_action_and_value(samples['observations'], \n",
    "                                (samples['actor_hxs'], samples['actor_cxs']),\n",
    "                                (samples['critic_hxs'], samples['critic_cxs']),\n",
    "                                samples['actions']\n",
    "                                )\n",
    "    \n",
    "    ratio = torch.exp(logprobs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return [policy_loss.cpu().data.numpy(),\n",
    "            vf_loss.cpu().data.numpy(),\n",
    "            loss.cpu().data.numpy(),\n",
    "            entropy_loss.cpu().data.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None):\n",
    "    \n",
    "    wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "    for _ in loop:\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'a_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                              'c_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                actor_recurrent_cell = magent_obs[villager][\"a_hcxs\"][-1]\n",
    "                critic_recurrent_cell = magent_obs[villager][\"c_hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell = villager_agent.get_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n",
    "                actions[villager] = ml_action.item()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"a_hcxs\"].append(actor_recurrent_cell)\n",
    "                magent_obs[villager][\"c_hcxs\"].append(critic_recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 33 out of a total of 100 games: 100%|██████████| 100/100 [00:09<00:00, 10.49it/s]\n",
      "Villagers won 28 out of a total of 100 games: 100%|██████████| 100/100 [00:24<00:00,  4.13it/s]\n",
      "Villagers won 30 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 13.68it/s]\n",
      "Villagers won 31 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 14.26it/s]\n",
      "Villagers won 34 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 14.08it/s]\n",
      "Villagers won 33 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 13.78it/s]\n",
      "Villagers won 23 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 14.29it/s]\n",
      "Villagers won 37 out of a total of 100 games: 100%|██████████| 100/100 [00:07<00:00, 14.01it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (1, 10)) of distribution Categorical(logits: torch.Size([1, 10])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m    wins \u001b[39m=\u001b[39m play_recurrent_game(env, random_coordinated_single_wolf, train_agent, num_times\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, hidden_state_size\u001b[39m=\u001b[39mHIDDEN_STATE_SIZE)\n\u001b[1;32m     28\u001b[0m \u001b[39m# fill buffer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m buff \u001b[39m=\u001b[39m fill_recurrent_buffer(env, random_coordinated_single_wolf, train_agent, num_times\u001b[39m=\u001b[39;49mGAMES_PER_EPOCH, hidden_state_size\u001b[39m=\u001b[39;49mHIDDEN_STATE_SIZE)\n\u001b[1;32m     31\u001b[0m \u001b[39m# run through batches and train network\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m buff\u001b[39m.\u001b[39mget_minibatch_generator(BATCH_SIZE):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mfill_recurrent_buffer\u001b[0;34m(env, wolf_policy, villager_agent, num_times, hidden_state_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m critic_recurrent_cell \u001b[39m=\u001b[39m magent_obs[villager][\u001b[39m\"\u001b[39m\u001b[39mc_hcxs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[39m# ensure that the obs is of size (batch,seq,inputs)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell \u001b[39m=\u001b[39m villager_agent\u001b[39m.\u001b[39;49mget_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n\u001b[1;32m     47\u001b[0m actions[villager] \u001b[39m=\u001b[39m ml_action\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m \u001b[39m# can store some stuff \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m, in \u001b[0;36mPluralityRecurrentAgentv2.get_action_and_value\u001b[0;34m(self, x, a_recurrent_cell, c_recurrent_cell, action)\u001b[0m\n\u001b[1;32m     91\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_fc2(h))\n\u001b[0;32m---> 93\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mcategorical\u001b[39m.\u001b[39;49mCategorical(logits\u001b[39m=\u001b[39;49mh)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     action \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m(Categorical, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:56\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     55\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 56\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (1, 10)) of distribution Categorical(logits: torch.Size([1, 10])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
     ]
    }
   ],
   "source": [
    "\n",
    "def calc_minibatch_loss(agent: PluralityRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # TODO:Consider checking for NAans anywhere. we cant have these. also do this in the model itself\n",
    "    # if torch.isnan(tensor).any(): print(f\"{label} contains NaN values\")\n",
    "\n",
    "    # get new log probs need to pass in the recurrent cells as well for actor and critic\n",
    "    _, logprobs, entropies, _, values, _ = agent.get_action_and_value(samples['observations'], \n",
    "                                (samples['actor_hxs'], samples['actor_cxs']),\n",
    "                                (samples['critic_hxs'], samples['critic_cxs']),\n",
    "                                samples['actions']\n",
    "                                )\n",
    "    \n",
    "    ratio = torch.exp(logprobs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return [policy_loss.cpu().data.numpy(),\n",
    "            vf_loss.cpu().data.numpy(),\n",
    "            loss.cpu().data.numpy(),\n",
    "            entropy_loss.cpu().data.numpy()]\n",
    "\n",
    "### Some \n",
    "CLIP_RANGE = 0.1\n",
    "BETA = 0.1\n",
    "V_LOSS_COEF = 0.1\n",
    "BATCH_SIZE = 256\n",
    "TRAIN_LOOPS = 1000\n",
    "EPOCHS = 3\n",
    "GAMES_PER_EPOCH = 100\n",
    "HIDDEN_STATE_SIZE=256\n",
    "\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "train_agent = PluralityRecurrentAgentv2(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size,hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "optimizer = torch.optim.Adam(train_agent.parameters(), lr=0.0005, eps=1e-5)\n",
    "# Testing passing a minibatch into this \n",
    "\n",
    "train_info = []\n",
    "\n",
    "for tid in range(TRAIN_LOOPS):\n",
    "   # train 100 times\n",
    "   for epid in range(EPOCHS):\n",
    "      if tid % 10 == 0 and epid == 0:\n",
    "         # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "         wins = play_recurrent_game(env, random_coordinated_single_wolf, train_agent, num_times=100, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "      # fill buffer\n",
    "      buff = fill_recurrent_buffer(env, random_coordinated_single_wolf, train_agent, num_times=GAMES_PER_EPOCH, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "      # run through batches and train network\n",
    "      for batch in buff.get_minibatch_generator(BATCH_SIZE):\n",
    "         train_info.append(calc_minibatch_loss(train_agent, batch, clip_range=CLIP_RANGE, beta=BETA, v_loss_coef=V_LOSS_COEF, optimizer=optimizer))\n",
    "\n",
    "train_stats = np.mean(train_info, axis=0)\n",
    "print(train_stats)\n",
    "\n",
    "torch.save(train_agent, \"rnn_agent_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 299 out of a total of 1000 games: 100%|██████████| 1000/1000 [01:30<00:00, 11.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after training is done\n",
    "\n",
    "play_recurrent_game(env, random_coordinated_single_wolf, train_agent, num_times=1000, hidden_state_size=HIDDEN_STATE_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent with shared network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityRecurrentAgentv3(torch.nn.Module):\n",
    "    def __init__(self, config:dict, num_actions, obs_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # recurrent layer\n",
    "        # TODO: Do I want 2 here?\n",
    "        self.recurrent_layer = self._rec_layer_init(torch.nn.LSTM(obs_size, config['rec_hidden_size'], num_layers=config['rec_layers'], batch_first=True))\n",
    "\n",
    "        # hidden layers\n",
    "        self.fc_joint = self._layer_init(torch.nn.Linear(config['rec_hidden_size'], config['hidden_mlp_size']))\n",
    "        self.policy_hidden = self._layer_init(torch.nn.Linear(config['hidden_mlp_size'], config['hidden_mlp_size']))\n",
    "        self.value_hidden = self._layer_init(torch.nn.Linear(config['hidden_mlp_size'], config['hidden_mlp_size']))\n",
    "\n",
    "        # policy output\n",
    "        self.policy_out = self._layer_init(torch.nn.Linear(config['hidden_mlp_size'], num_actions), std=0.01)\n",
    "\n",
    "        # value output\n",
    "        self.value_out = self._layer_init(torch.nn.Linear(config['hidden_mlp_size'], 1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        # torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            # if \"bias\" in name:\n",
    "                # torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "    \n",
    "    \n",
    "    def forward(self, x, recurrent_cell: torch.tensor):\n",
    "\n",
    "        # pass  through the Recurrence Layer\n",
    "        h, recurrent_cell = self.recurrent_layer(torch.unsqueeze(x,1), recurrent_cell)\n",
    "        h = torch.squeeze(h,1)\n",
    "\n",
    "        # Pass through a hidden layer\n",
    "        h = torch.relu(self.fc_joint(h))\n",
    "\n",
    "        # Split for Value and Policy\n",
    "        h_value = torch.relu(self.value_hidden(h))\n",
    "        h_policy = torch.relu(self.policy_hidden(h))\n",
    "\n",
    "        # value\n",
    "        value = self.value_out(h_value).reshape(-1)\n",
    "\n",
    "        # policy\n",
    "        policy = self.policy_out(h_policy)\n",
    "        policy = torch.distributions.Categorical(logits=policy)\n",
    "\n",
    "        return policy, value, recurrent_cell\n",
    "    \n",
    "\n",
    "class RolloutBufferv3():\n",
    "    \n",
    "    def __init__(self, buffer_size: int, gamma: float, gae_lambda: float):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = None \n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.reset(gamma=gamma, gae_lambda=gae_lambda)\n",
    "\n",
    "    def reset(self, gamma: float, gae_lambda: float):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.hcxs.append(game[\"hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.cat([item for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.cat([item for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages).float()\n",
    "\n",
    "        # TODO : Gotta update these to work with a single set of hxs, rxs\n",
    "        hxs, cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"hxs\": torch.swapaxes(torch.cat(hxs)[batch_index],0,1),\n",
    "                \"cxs\": torch.swapaxes(torch.cat(cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            } \n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(buffer, env, config:dict, wolf_policy, villager_agent) -> RolloutBuffer:\n",
    "\n",
    "    buffer.reset(gamma=config[\"training\"][\"gamma\"], gae_lambda=config[\"training\"][\"gae_lambda\"])\n",
    "    \n",
    "    for _ in range(config[\"training\"][\"buffer_games_per_update\"]):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32), \n",
    "                                        torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                # this needs to be updated\n",
    "                policy, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                action = policy.sample()\n",
    "                \n",
    "                actions[villager] = action.item()\n",
    "\n",
    "                # can store some stuff \n",
    "                magent_obs[villager][\"obs\"].append(obs)\n",
    "                magent_obs[villager][\"actions\"].append(action)\n",
    "\n",
    "                # how do we get these\n",
    "                magent_obs[villager][\"logprobs\"].append(policy.log_prob(action))\n",
    "                magent_obs[villager][\"values\"].append(value)\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policy, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                action = policy.sample()\n",
    "                \n",
    "                actions[villager] = action.item()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins\n",
    "\n",
    "def calc_minibatch_loss(agent: PluralityRecurrentAgentv3, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # TODO:Consider checking for NAans anywhere. we cant have these. also do this in the model itself\n",
    "    # if torch.isnan(tensor).any(): print(f\"{label} contains NaN values\")\n",
    "    policies, values, _ = agent(samples['observations'], (samples['hxs'], samples['cxs']))\n",
    "    \n",
    "    # log_probs, entropies = [], []\n",
    "    log_probs = policies.log_prob(samples['actions'])\n",
    "    entropies = policies.entropy() # need to sum if we have more than 1 action\n",
    "    \n",
    "    ratio = torch.exp(log_probs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    return [policy_loss.cpu().data.numpy(),     # policy loss\n",
    "            vf_loss.cpu().data.numpy(),         # value loss\n",
    "            loss.cpu().data.numpy(),            # total loss\n",
    "            entropy_loss.cpu().data.numpy()]    # entropy loss\n",
    "\n",
    "### Some \n",
    "# CLIP_RANGE = 0.2\n",
    "# BETA = 0.1\n",
    "# V_LOSS_COEF = 0.1\n",
    "# BATCH_SIZE = 128\n",
    "# TRAIN_LOOPS = 100\n",
    "# EPOCHS = 6\n",
    "# GAMES_PER_EPOCH = 200\n",
    "# HIDDEN_STATE_SIZE=256\n",
    "\n",
    "\n",
    "# env = plurality_env(num_agents=10, werewolves=2)\n",
    "# observations, rewards, terminations, truncations, infos = env.reset()\n",
    "# obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "# nn_agent = PluralityRecurrentAgentv3({\"rec_hidden_size\": HIDDEN_STATE_SIZE, \"rec_layers\": 1, \"hidden_mlp_size\": 128},num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "# optimizer = torch.optim.Adam(nn_agent.parameters(), lr=0.0001)\n",
    "# # Testing passing a minibatch into this \n",
    "\n",
    "# for tid in range(TRAIN_LOOPS):\n",
    "#     # train 100 times\n",
    "#     if tid % 10 == 0:\n",
    "#         # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "#         wins = play_recurrent_game(env, random_coordinated_single_wolf, nn_agent, num_times=100, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "#     # fill buffer\n",
    "#     buff = fill_recurrent_buffer(env, random_coordinated_single_wolf, nn_agent, num_times=GAMES_PER_EPOCH, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "#     # train info will hold our metrics\n",
    "#     train_info = []\n",
    "#     for epid in range(EPOCHS):\n",
    "#       # run through batches and train network\n",
    "#       for batch in buff.get_minibatch_generator(BATCH_SIZE):\n",
    "#          train_info.append(calc_minibatch_loss(nn_agent, batch, clip_range=CLIP_RANGE, beta=BETA, v_loss_coef=V_LOSS_COEF, optimizer=optimizer))\n",
    "\n",
    "#     train_stats = np.mean(train_info, axis=0)\n",
    "\n",
    "#     # we can store the \n",
    "\n",
    "    \n",
    "\n",
    "# print(train_stats)\n",
    "\n",
    "# torch.save(nn_agent, \"rnn_agent_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_training = {\n",
    "    \"model\": {\n",
    "        \"recurrent_layers\": 1,\n",
    "        \"recurrent_hidden_size\": 128, # 256\n",
    "        \"mlp_size\": 128, # 256\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"batch_size\": 32, # 128\n",
    "        \"epochs\": 3, # 6\n",
    "        \"updates\": 10, # 1000\n",
    "        \"buffer_games_per_update\": 10, # 200\n",
    "        \"clip_range\": 0.2,\n",
    "        \"value_loss_coefficient\": 0.1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"beta\": 0.01, # entropy loss multiplier\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_eps\": 1e-8,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "    }\n",
    "}\n",
    "\n",
    "config_game = {\n",
    "    \"rewards\": {\n",
    "        \"day\": -1,\n",
    "        \"player_death\": -1,\n",
    "        \"player_win\": 10,\n",
    "        \"player_loss\": -5,\n",
    "        \"self_vote\": -1,\n",
    "        \"dead_vote\": -1,\n",
    "        \"dead_wolf\": 5,\n",
    "        \"no_viable_vote\": -1,\n",
    "        \"no_sleep\": -1,\n",
    "    },\n",
    "    \"gameplay\": {\n",
    "        \"accusation_phases\": 1,\n",
    "        \"num_agents\": 10,\n",
    "        \"num_werewolves\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"config_game\": config_game,\n",
    "    \"config_training\": config_training,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Training: 100%|██████████| 10/10 [02:56<00:00, 17.65s/it]                  \n"
     ]
    }
   ],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, config:dict, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\"), mlflow_uri:str=None) -> None:\n",
    "        \"\"\"Initializes all needed training components.\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            run_id {str, optional} -- A tag used to save Tensorboard Summaries and the trained model. Defaults to \"run\".\n",
    "            device {torch.device, optional} -- Determines the training device. Defaults to cpu.\n",
    "        \"\"\"\n",
    "        # Set variables\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.run_id = run_id\n",
    "        self.mlflow_uri = mlflow_uri\n",
    "        self.env = None\n",
    "\n",
    "        # we are not using schedules yet\n",
    "        # self.lr_schedule = config[\"learning_rate_schedule\"]\n",
    "        # self.beta_schedule = config[\"beta_schedule\"]\n",
    "        # self.cr_schedule = config[\"clip_range_schedule\"]\n",
    "\n",
    "        # Initialize Environment\n",
    "        env = plurality_env(num_agents=10, werewolves=2)\n",
    "        self.env = env\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "        # Initialize Buffer\n",
    "        self.buffer = RolloutBufferv3(buffer_size=10, gamma=0.99, gae_lambda=0.95)\n",
    "\n",
    "        # Initialize Model & Optimizer\n",
    "        self.agent = PluralityRecurrentAgentv3({\"rec_hidden_size\": self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"], \n",
    "                                                \"rec_layers\": self.config[\"config_training\"][\"model\"][\"recurrent_layers\"], \n",
    "                                                \"hidden_mlp_size\": self.config[\"config_training\"][\"model\"][\"mlp_size\"]},\n",
    "                                                num_actions=self.env.action_space(\"player_0\").n,\n",
    "                                                obs_size=obs_size)\n",
    "        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=0.0001, eps=1e-5)\n",
    "\n",
    "        # setup mlflow run if we are using it\n",
    "\n",
    "    def train(self, idx: int):\n",
    "        if self.mlflow_uri:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "\n",
    "        name = f'{self.run_id}_{idx}'\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            \n",
    "            mlflow.log_params(self.config[\"config_training\"][\"training\"])\n",
    "            mlflow.log_params(self.config[\"config_training\"][\"model\"])\n",
    "\n",
    "            loop = tqdm(range(self.config[\"config_training\"][\"training\"][\"updates\"]))\n",
    "\n",
    "            for tid, _ in enumerate(loop):\n",
    "                # train 100 times\n",
    "                if tid % 2 == 0:\n",
    "                    # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "                    loop.set_description(\"Playing games and averaging score\")\n",
    "                    wins = []\n",
    "                    for _ in range(10):\n",
    "                        wins.append(play_recurrent_game(self.env, \n",
    "                                                        random_coordinated_single_wolf, \n",
    "                                                        self.agent, \n",
    "                                                        num_times=50,\n",
    "                                                        hidden_state_size=self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"]))\n",
    "                    \n",
    "                    mlflow.log_metric(\"avg_wins/50\", np.mean(wins))\n",
    "\n",
    "                loop.set_description(\"Filling buffer\")\n",
    "                # fill buffer\n",
    "                buff = fill_recurrent_buffer(self.buffer, \n",
    "                                             self.env,\n",
    "                                             self.config[\"config_training\"],\n",
    "                                             random_coordinated_single_wolf, \n",
    "                                             self.agent)\n",
    "\n",
    "                # train info will hold our metrics\n",
    "                train_info = []\n",
    "                loop.set_description(\"Epoch Training\")\n",
    "                for _ in range(self.config['config_training'][\"training\"]['epochs']):\n",
    "                    # run through batches and train network\n",
    "                    for batch in buff.get_minibatch_generator(self.config['config_training'][\"training\"]['batch_size']):\n",
    "                        train_info.append(calc_minibatch_loss(self.agent, \n",
    "                                                              batch, \n",
    "                                                              clip_range=self.config['config_training'][\"training\"]['clip_range'], \n",
    "                                                              beta=self.config['config_training'][\"training\"]['beta'], \n",
    "                                                              v_loss_coef=self.config['config_training'][\"training\"]['value_loss_coefficient'], \n",
    "                                                              optimizer=self.optimizer))\n",
    "\n",
    "                train_stats = np.mean(train_info, axis=0)\n",
    "                mlflow.log_metric(\"policy loss\", train_stats[0])\n",
    "                mlflow.log_metric(\"value loss\", train_stats[1])\n",
    "                mlflow.log_metric(\"total loss\", train_stats[2])\n",
    "                mlflow.log_metric(\"entropy loss\", train_stats[3])\n",
    "            # one more run\n",
    "\n",
    "        # torch.save(self.agent, f\"rnn_agent_{self.run_id}\")\n",
    "\n",
    "\n",
    "trainer = PPOTrainer(config=config,run_id=\"plu\", mlflow_uri=\"http://mlflow:5000\")\n",
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "www"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
