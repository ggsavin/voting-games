{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, plurality_Phase, plurality_Role\n",
    "import random\n",
    "import copy\n",
    "from typing import Any, Generator, Optional, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_coordinated_wolf(env, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    return {wolf: int(target.split(\"_\")[-1]) for wolf in wolves_remaining}\n",
    "\n",
    "def random_wolfs(env):\n",
    "    return {wolf: env.action_space(wolf).sample() for\n",
    "            wolf in set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])}\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    \n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    return int(random.choice(list(targets)).split(\"_\")[-1])\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent):\n",
    "   return env.action_space(agent).sample()\n",
    "\n",
    "def random_coordinated_single_wolf(env, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    return action if action != None else int(random.choice(list(villagers_remaining)).split(\"_\")[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 3 out of a total of 10 games: 100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(plurality_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            # at least one wolf\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "                \n",
    "            # wolf steps\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "play_static_wolf_game(env, random_coordinated_single_wolf, random_single_target_villager, num_times=10)\n",
    "\n",
    "# print(\"Random Coordinated Wolves\")\n",
    "# print(\"\\t vs. Single Target Random Villagers\")\n",
    "# play_static_wolf_game(env, random_coordinated_wolf, random_single_target_villager, num_times=1000)\n",
    "# print(\"\\t vs. Random Villagers\")\n",
    "# play_static_wolf_game(env, random_coordinated_wolf, random_agent_action, num_times=1000)\n",
    "# print(\"------------------------------------\\n\")\n",
    "# print(\"Random Wolves\")\n",
    "# print(\"\\t vs. Single Target Random Villagers\")\n",
    "# play_static_wolf_game(env, random_wolfs, random_single_target_villager, num_times=1000)\n",
    "# print(\"\\t vs. Random Villagers\")\n",
    "# play_static_wolf_game(env, random_wolfs, random_agent_action, num_times=1000)\n",
    "# print(\"------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None):\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, num_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityRecurrentAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None, hidden_state_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # actor\n",
    "        self.a_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.a_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.a_fc2 = self._layer_init(torch.nn.Linear(64,num_actions), std=0.01)\n",
    "\n",
    "        # critic\n",
    "        self.c_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.c_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.c_fc2 = self._layer_init(torch.nn.Linear(64,1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, recurrent_cell:torch.tensor):\n",
    "        h = torch.tanh(self.c_fc1(x))\n",
    "        h, recurrent_cell = self.c_recurrent_layer(torch.unsqueeze(h,1), recurrent_cell)\n",
    "        h = torch.tanh(self.c_fc2(h))\n",
    "\n",
    "        # UPDATE THIS X\n",
    "        return h, recurrent_cell\n",
    "    \n",
    "    def get_action_and_value(self, x, a_recurrent_cell:torch.tensor, c_recurrent_cell:torch.tensor, action=None):\n",
    "        h = torch.tanh(self.a_fc1(x))\n",
    "        # we are strictly on a sequence length of 1 here, using prior information baked in\n",
    "        h, recurrent_cell = self.a_recurrent_layer(torch.unsqueeze(h,1), a_recurrent_cell)\n",
    "        h = torch.tanh(self.a_fc2(h))\n",
    "        probs = torch.distributions.categorical.Categorical(logits=h)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        c_val, c_rec = self.get_value(x, c_recurrent_cell)\n",
    "        # UPDATE THIS X\n",
    "        return action, probs.log_prob(action), probs.entropy(), recurrent_cell, c_val, c_rec\n",
    "    \n",
    "class PluralityRecurrentAgentv2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None, hidden_state_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # actor\n",
    "        self.a_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(obs_size, hidden_state_size, batch_first=True))\n",
    "        self.a_fc1 = self._layer_init(torch.nn.Linear(hidden_state_size, 64))\n",
    "        self.a_fc2 = self._layer_init(torch.nn.Linear(64,num_actions), std=0.01)\n",
    "\n",
    "        # critic\n",
    "        self.c_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(obs_size, hidden_state_size, batch_first=True))\n",
    "        self.c_fc1 = self._layer_init(torch.nn.Linear(hidden_state_size,64))\n",
    "        self.c_fc2 = self._layer_init(torch.nn.Linear(64,1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, recurrent_cell:torch.tensor):\n",
    "        h, recurrent_cell = self.c_recurrent_layer(torch.unsqueeze(x,1), recurrent_cell)\n",
    "        h = torch.tanh(self.c_fc1(h))\n",
    "        h = torch.tanh(self.c_fc2(h))\n",
    "\n",
    "        # UPDATE THIS X\n",
    "        return h, recurrent_cell\n",
    "    \n",
    "    def get_action_and_value(self, x, a_recurrent_cell:torch.tensor, c_recurrent_cell:torch.tensor, action=None):\n",
    "        h, recurrent_cell = self.a_recurrent_layer(torch.unsqueeze(x,1), a_recurrent_cell)\n",
    "        h = torch.tanh(self.a_fc1(h))\n",
    "        h = torch.tanh(self.a_fc2(h))\n",
    "        probs = torch.distributions.categorical.Categorical(logits=h)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        c_val, c_rec = self.get_value(x, c_recurrent_cell)\n",
    "        # UPDATE THIS X\n",
    "        return action, probs.log_prob(action), probs.entropy(), recurrent_cell, c_val, c_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PluralityRecurrentAgent.get_action_and_value() missing 1 required positional argument: 'c_recurrent_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(env\u001b[39m.\u001b[39mconvert_obs(observations[\u001b[39m'\u001b[39m\u001b[39mplayer_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m]), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m      9\u001b[0m tobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(tobs, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m rec_agent\u001b[39m.\u001b[39;49mget_action_and_value(tobs, \n\u001b[1;32m     11\u001b[0m                                (torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m), \u001b[39m64\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32),\n\u001b[1;32m     12\u001b[0m                                 torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m), \u001b[39m64\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m     13\u001b[0m                                )\n",
      "\u001b[0;31mTypeError\u001b[0m: PluralityRecurrentAgent.get_action_and_value() missing 1 required positional argument: 'c_recurrent_cell'"
     ]
    }
   ],
   "source": [
    "env = plurality_env()\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "# had to call, and unsqueeze the obs. we did this because we need to pass in a batch size.\n",
    "# we also want to keep the length to 1 for now, and pass each through the model\n",
    "tobs = torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "tobs = torch.unsqueeze(tobs, 0)\n",
    "rec_agent.get_action_and_value(tobs, \n",
    "                               (torch.zeros((1), 64, dtype=torch.float32),\n",
    "                                torch.zeros((1), 64, dtype=torch.float32))\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 buffer_size: int, \n",
    "                 gamma: float, \n",
    "                 gae_lambda: float,\n",
    "                 is_recurrent: bool,\n",
    "                 recurrent_size: int = None,\n",
    "                 ):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        self.steps = []\n",
    "\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = None \n",
    "        self.critic_hcxs = None \n",
    "\n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.is_recurrent = is_recurrent\n",
    "        if self.is_recurrent:\n",
    "            self.recurrent_size = recurrent_size\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = []\n",
    "        self.critic_hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.actor_hcxs.append(game[\"a_hcxs\"][:-1])\n",
    "         self.critic_hcxs.append(game[\"c_hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "            Arguments:\n",
    "                last_value {torch.tensor} -- Value of the last agent's state\n",
    "                gamma {float} -- Discount factor\n",
    "                lamda {float} -- GAE regularization parameter\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.cat([item for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.cat([item for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages).float()\n",
    "        actor_hxs, actor_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.actor_hcxs for item in sublist]])\n",
    "        critic_hxs, critic_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.critic_hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"actor_hxs\": torch.swapaxes(torch.cat(actor_hxs)[batch_index],0,1),\n",
    "                \"actor_cxs\": torch.swapaxes(torch.cat(actor_cxs)[batch_index],0,1),\n",
    "                \"critic_hxs\": torch.swapaxes(torch.cat(critic_hxs)[batch_index],0,1),\n",
    "                \"critic_cxs\": torch.swapaxes(torch.cat(critic_cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None) -> RolloutBuffer:\n",
    "\n",
    "    buffer = RolloutBuffer(buffer_size=10, \n",
    "                           gamma=0.90, \n",
    "                           gae_lambda=0.90,\n",
    "                           is_recurrent=True)\n",
    "    buffer.reset()\n",
    "    \n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'a_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                              'c_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                actor_recurrent_cell = magent_obs[villager][\"a_hcxs\"][-1]\n",
    "                critic_recurrent_cell = magent_obs[villager][\"c_hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell = villager_agent.get_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n",
    "                actions[villager] = ml_action.item()\n",
    "\n",
    "                # can store some stuff \n",
    "                magent_obs[villager][\"obs\"].append(obs)\n",
    "                magent_obs[villager][\"actions\"].append(ml_action)\n",
    "                magent_obs[villager][\"logprobs\"].append(logprobs)\n",
    "                magent_obs[villager][\"values\"].append(c_val)\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"a_hcxs\"].append(actor_recurrent_cell)\n",
    "                magent_obs[villager][\"c_hcxs\"].append(critic_recurrent_cell)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "        \n",
    "\n",
    "# env = plurality_env(num_agents=10, werewolves=2)\n",
    "# observations, rewards, terminations, truncations, infos = env.reset()\n",
    "\n",
    "# obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "# rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "# def test_policy(obs, net, agent=None, env=None, a_rec=None, c_rec=None):\n",
    "#    # \n",
    "#    return rec_agent.get_action_and_value(obs, a_rec, c_rec)\n",
    "\n",
    "# def test_recurrent_policy(obs, agent=None, env=None):\n",
    "#     # we need to return the hx and cx from the model, chyou know? and also have an initial one of 0 to feed the model the first time\n",
    "#     pass\n",
    "\n",
    "\n",
    "# buff = fill_recurrent_buffer(env, random_coordinated_wolf, test_policy, num_times=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_minibatch_loss(agent: PluralityRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # get new log probs need to pass in the recurrent cells as well for actor and critic\n",
    "    _, logprobs, entropies, _, values, _ = agent.get_action_and_value(samples['observations'], \n",
    "                                (samples['actor_hxs'], samples['actor_cxs']),\n",
    "                                (samples['critic_hxs'], samples['critic_cxs']),\n",
    "                                samples['actions']\n",
    "                                )\n",
    "    \n",
    "    ratio = torch.exp(logprobs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config[\"max_grad_norm\"])\n",
    "    optimizer.step()\n",
    "\n",
    "    return [policy_loss.cpu().data.numpy(),\n",
    "            vf_loss.cpu().data.numpy(),\n",
    "            loss.cpu().data.numpy(),\n",
    "            entropy_loss.cpu().data.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None):\n",
    "    \n",
    "    wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "    for _ in loop:\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'a_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                              'c_hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                actor_recurrent_cell = magent_obs[villager][\"a_hcxs\"][-1]\n",
    "                critic_recurrent_cell = magent_obs[villager][\"c_hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell = villager_agent.get_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n",
    "                actions[villager] = ml_action.item()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"a_hcxs\"].append(actor_recurrent_cell)\n",
    "                magent_obs[villager][\"c_hcxs\"].append(critic_recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 145 out of a total of 500 games: 100%|██████████| 500/500 [00:28<00:00, 17.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m    wins \u001b[39m=\u001b[39m play_recurrent_game(env, random_coordinated_single_wolf, train_agent, num_times\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, hidden_state_size\u001b[39m=\u001b[39mHIDDEN_STATE_SIZE)\n\u001b[1;32m     28\u001b[0m \u001b[39m# fill buffer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m buff \u001b[39m=\u001b[39m fill_recurrent_buffer(env, random_coordinated_single_wolf, train_agent, num_times\u001b[39m=\u001b[39;49mGAMES_PER_EPOCH, hidden_state_size\u001b[39m=\u001b[39;49mHIDDEN_STATE_SIZE)\n\u001b[1;32m     31\u001b[0m \u001b[39m# run through batches and train network\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m buff\u001b[39m.\u001b[39mget_minibatch_generator(BATCH_SIZE):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mfill_recurrent_buffer\u001b[0;34m(env, wolf_policy, villager_agent, num_times, hidden_state_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m critic_recurrent_cell \u001b[39m=\u001b[39m magent_obs[villager][\u001b[39m\"\u001b[39m\u001b[39mc_hcxs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[39m# ensure that the obs is of size (batch,seq,inputs)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell \u001b[39m=\u001b[39m villager_agent\u001b[39m.\u001b[39;49mget_action_and_value(obs, actor_recurrent_cell, critic_recurrent_cell)\n\u001b[1;32m     47\u001b[0m actions[villager] \u001b[39m=\u001b[39m ml_action\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     49\u001b[0m \u001b[39m# can store some stuff \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m, in \u001b[0;36mPluralityRecurrentAgentv2.get_action_and_value\u001b[0;34m(self, x, a_recurrent_cell, c_recurrent_cell, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_fc1(h))\n\u001b[1;32m     90\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_fc2(h))\n\u001b[0;32m---> 91\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mcategorical\u001b[39m.\u001b[39;49mCategorical(logits\u001b[39m=\u001b[39;49mh)\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     action \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:62\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[39m# Normalize\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits \u001b[39m=\u001b[39m logits \u001b[39m-\u001b[39m logits\u001b[39m.\u001b[39;49mlogsumexp(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Some \n",
    "CLIP_RANGE = 0.1\n",
    "BETA = 0.1\n",
    "V_LOSS_COEF = 0.1\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_LOOPS = 1000\n",
    "EPOCHS = 3\n",
    "GAMES_PER_EPOCH = 100\n",
    "HIDDEN_STATE_SIZE=64\n",
    "\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "train_agent = PluralityRecurrentAgentv2(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size,hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "optimizer = torch.optim.Adam(train_agent.parameters(), lr=0.001, eps=1e-5)\n",
    "# Testing passing a minibatch into this \n",
    "\n",
    "train_info = []\n",
    "\n",
    "for tid in range(TRAIN_LOOPS):\n",
    "   # train 100 times\n",
    "   for epid in range(EPOCHS):\n",
    "      if tid % 50 == 0 and epid == 0:\n",
    "         # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "         wins = play_recurrent_game(env, random_coordinated_single_wolf, train_agent, num_times=500, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "      # fill buffer\n",
    "      buff = fill_recurrent_buffer(env, random_coordinated_single_wolf, train_agent, num_times=GAMES_PER_EPOCH, hidden_state_size=HIDDEN_STATE_SIZE)\n",
    "\n",
    "      # run through batches and train network\n",
    "      for batch in buff.get_minibatch_generator(BATCH_SIZE):\n",
    "         train_info.append(calc_minibatch_loss(train_agent, batch, clip_range=CLIP_RANGE, beta=BETA, v_loss_coef=V_LOSS_COEF, optimizer=optimizer))\n",
    "\n",
    "train_stats = np.mean(train_info, axis=0)\n",
    "print(train_stats)\n",
    "\n",
    "\n",
    "# # loop through\n",
    "# for batch in buff.get_minibatch_generator(32):\n",
    "# minibatch_gen = buff.get_minibatch_generator(32)\n",
    "# first_batch = next(minibatch_gen)\n",
    "\n",
    "\n",
    "# actions, logprobs, entropies, _, values, _ = rec_agent.get_action_and_value(first_batch['observations'], \n",
    "#                                 (first_batch['actor_hxs'], first_batch['actor_cxs']),\n",
    "#                                 (first_batch['critic_hxs'], first_batch['critic_cxs']),\n",
    "#                                 first_batch['actions']\n",
    "#                                 )\n",
    "\n",
    "# clip_range: float, beta: float = entropy coefficient , v_loss_coef: 0.1\n",
    "# calc_minibatch_loss(rec_agent, first_batch, clip_range=0.1, beta=0.1, v_loss_coef=0.1, optimizer=optimizer)\n",
    "\n",
    "\n",
    "torch.save(train_agent, \"rnn_agent_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 38 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:37<00:00, 26.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after training is done\n",
    "play_recurrent_game(env, random_coordinated_wolf, train_agent, num_times=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Would cartpole work? Lets try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, config:dict, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\")) -> None:\n",
    "        \"\"\"Initializes all needed training components.\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            run_id {str, optional} -- A tag used to save Tensorboard Summaries and the trained model. Defaults to \"run\".\n",
    "            device {torch.device, optional} -- Determines the training device. Defaults to cpu.\n",
    "        \"\"\"\n",
    "        # Set variables\n",
    "        self.config = config\n",
    "        self.recurrence = config[\"recurrence\"]\n",
    "        self.device = device\n",
    "        self.run_id = run_id\n",
    "        self.lr_schedule = config[\"learning_rate_schedule\"]\n",
    "        self.beta_schedule = config[\"beta_schedule\"]\n",
    "        self.cr_schedule = config[\"clip_range_schedule\"]\n",
    "        \n",
    "\n",
    "        # setup mlflow run if we are using it\n",
    "    def \n",
    "    def _get_mini_batch_loss():\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
