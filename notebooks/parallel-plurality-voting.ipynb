{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, plurality_Phase, plurality_Role\n",
    "import random\n",
    "import copy\n",
    "from typing import Any, Generator, Optional, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_coordinated_wolf(env):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    return {wolf: int(target.split(\"_\")[-1]) for wolf in wolves_remaining}\n",
    "\n",
    "def random_wolfs(env):\n",
    "    return {wolf: env.action_space(wolf).sample() for\n",
    "            wolf in set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])}\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    \n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    return int(random.choice(list(targets)).split(\"_\")[-1])\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent):\n",
    "   return env.action_space(agent).sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Coordinated Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 105 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:05<00:00, 176.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 51 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:04<00:00, 200.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n",
      "Random Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 689 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:04<00:00, 200.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 589 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:05<00:00, 175.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(plurality_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            if env.world_state[\"phase\"] != plurality_Phase.NIGHT:\n",
    "                # villagers actions\n",
    "                for villager in villagers:\n",
    "                    actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            # wolf steps\n",
    "            actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == plurality_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None):\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, num_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PluralityRecurrentAgent(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, obs_size=None, hidden_state_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # actor\n",
    "        self.a_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.a_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.a_fc2 = self._layer_init(torch.nn.Linear(64,num_actions), std=0.01)\n",
    "\n",
    "        # critic\n",
    "        self.c_recurrent_layer = self._rec_layer_init(torch.nn.LSTM(64, hidden_state_size, batch_first=True))\n",
    "        self.c_fc1 = self._layer_init(torch.nn.Linear(obs_size,64))\n",
    "        self.c_fc2 = self._layer_init(torch.nn.Linear(64,1), std=1.0)\n",
    "    \n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def _rec_layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                torch.nn.init.constant_(param, bias_const)\n",
    "            if \"weight\" in name:\n",
    "                torch.nn.init.orthogonal_(param, std)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, recurrent_cell:torch.tensor):\n",
    "        h = torch.tanh(self.c_fc1(x))\n",
    "        h, recurrent_cell = self.c_recurrent_layer(torch.unsqueeze(h,1), recurrent_cell)\n",
    "        h = torch.tanh(self.c_fc2(h))\n",
    "\n",
    "        # UPDATE THIS X\n",
    "        return h, recurrent_cell\n",
    "    \n",
    "    def get_action_and_value(self, x, a_recurrent_cell:torch.tensor, c_recurrent_cell:torch.tensor, action=None):\n",
    "        h = torch.tanh(self.a_fc1(x))\n",
    "        # we are strictly on a sequence length of 1 here, using prior information baked in\n",
    "        h, recurrent_cell = self.a_recurrent_layer(torch.unsqueeze(h,1), a_recurrent_cell)\n",
    "        h = torch.tanh(self.a_fc2(h))\n",
    "        probs = torch.distributions.categorical.Categorical(logits=h)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        c_val, c_rec = self.get_value(x, c_recurrent_cell)\n",
    "        # UPDATE THIS X\n",
    "        return action, probs.log_prob(action), probs.entropy(), recurrent_cell, c_val, c_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PluralityRecurrentAgent.get_action_and_value() missing 1 required positional argument: 'c_recurrent_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(env\u001b[39m.\u001b[39mconvert_obs(observations[\u001b[39m'\u001b[39m\u001b[39mplayer_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m]), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m      9\u001b[0m tobs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(tobs, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m rec_agent\u001b[39m.\u001b[39;49mget_action_and_value(tobs, \n\u001b[1;32m     11\u001b[0m                                (torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m), \u001b[39m64\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32),\n\u001b[1;32m     12\u001b[0m                                 torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m), \u001b[39m64\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m     13\u001b[0m                                )\n",
      "\u001b[0;31mTypeError\u001b[0m: PluralityRecurrentAgent.get_action_and_value() missing 1 required positional argument: 'c_recurrent_cell'"
     ]
    }
   ],
   "source": [
    "env = plurality_env()\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "# had to call, and unsqueeze the obs. we did this because we need to pass in a batch size.\n",
    "# we also want to keep the length to 1 for now, and pass each through the model\n",
    "tobs = torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "tobs = torch.unsqueeze(tobs, 0)\n",
    "rec_agent.get_action_and_value(tobs, \n",
    "                               (torch.zeros((1), 64, dtype=torch.float32),\n",
    "                                torch.zeros((1), 64, dtype=torch.float32))\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 buffer_size: int, \n",
    "                 gamma: float, \n",
    "                 gae_lambda: float,\n",
    "                 is_recurrent: bool,\n",
    "                 recurrent_size: int = None,\n",
    "                 ):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        self.steps = []\n",
    "\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = None \n",
    "        self.critic_hcxs = None \n",
    "\n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.is_recurrent = is_recurrent\n",
    "        if self.is_recurrent:\n",
    "            self.recurrent_size = recurrent_size\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.actor_hcxs = []\n",
    "        self.critic_hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.actor_hcxs.append(game[\"a_hcxs\"][:-1])\n",
    "         self.critic_hcxs.append(game[\"c_hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "            Arguments:\n",
    "                last_value {torch.tensor} -- Value of the last agent's state\n",
    "                gamma {float} -- Discount factor\n",
    "                lamda {float} -- GAE regularization parameter\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.cat([item for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.cat([item for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages)\n",
    "        actor_hxs, actor_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.actor_hcxs for item in sublist]])\n",
    "        critic_hxs, critic_cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.critic_hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"actor_hxs\": torch.swapaxes(torch.cat(actor_hxs)[batch_index],0,1),\n",
    "                \"actor_cxs\": torch.swapaxes(torch.cat(actor_cxs)[batch_index],0,1),\n",
    "                \"critic_hxs\": torch.swapaxes(torch.cat(critic_hxs)[batch_index],0,1),\n",
    "                \"critic_cxs\": torch.swapaxes(torch.cat(critic_cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(env, wolf_policy, villager_policy, num_times=10) -> RolloutBuffer:\n",
    "\n",
    "    buffer = RolloutBuffer(buffer_size=10, \n",
    "                           gamma=0.90, \n",
    "                           gae_lambda=0.90,\n",
    "                           is_recurrent=True)\n",
    "    buffer.reset()\n",
    "    \n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'a_hcxs': [(torch.zeros((1,1,64), dtype=torch.float32), torch.zeros((1,1,64), dtype=torch.float32))],\n",
    "                              'c_hcxs': [(torch.zeros((1,1,64), dtype=torch.float32), torch.zeros((1,1,64), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            if env.world_state[\"phase\"] != plurality_Phase.NIGHT:\n",
    "                # villagers actions\n",
    "                for villager in villagers:\n",
    "                    #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                    torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                    obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                    # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                    actor_recurrent_cell = magent_obs[villager][\"a_hcxs\"][-1]\n",
    "                    critic_recurrent_cell = magent_obs[villager][\"c_hcxs\"][-1]\n",
    "                    \n",
    "                    # ensure that the obs is of size (batch,seq,inputs)\n",
    "                    ml_action,  logprobs, _, actor_recurrent_cell, c_val, critic_recurrent_cell = villager_policy(obs, actor_recurrent_cell, critic_recurrent_cell)\n",
    "                    actions[villager] = ml_action.item()\n",
    "\n",
    "                    # can store some stuff \n",
    "                    magent_obs[villager][\"obs\"].append(obs)\n",
    "                    magent_obs[villager][\"actions\"].append(ml_action)\n",
    "                    magent_obs[villager][\"logprobs\"].append(logprobs)\n",
    "                    magent_obs[villager][\"values\"].append(c_val)\n",
    "\n",
    "                    #store the next recurrent cells\n",
    "                    magent_obs[villager][\"a_hcxs\"].append(actor_recurrent_cell)\n",
    "                    magent_obs[villager][\"c_hcxs\"].append(critic_recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                    if env.history[-1][\"phase\"] == plurality_Phase.NIGHT:\n",
    "                        magent_obs[villager][\"rewards\"][-1] += rewards[villager]\n",
    "                        magent_obs[villager][\"terms\"][-1] = terminations[villager]\n",
    "                    else:\n",
    "                        magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                        magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "        \n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "def test_policy(obs, net, agent=None, env=None, a_rec=None, c_rec=None):\n",
    "   # \n",
    "   return rec_agent.get_action_and_value(obs, a_rec, c_rec)\n",
    "\n",
    "def test_recurrent_policy(obs, agent=None, env=None):\n",
    "    # we need to return the hx and cx from the model, chyou know? and also have an initial one of 0 to feed the model the first time\n",
    "    pass\n",
    "\n",
    "\n",
    "buff = fill_recurrent_buffer(env, random_coordinated_wolf, test_policy, num_times=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "observations, rewards, terminations, truncations, infos = env.reset()\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "rec_agent = PluralityRecurrentAgent(num_actions=env.action_space(\"player_0\").n, obs_size=obs_size)\n",
    "\n",
    "# Testing passing a minibatch into this \n",
    "\n",
    "minibatch_gen = buff.get_minibatch_generator(32)\n",
    "first_batch = next(minibatch_gen)\n",
    "\n",
    "\n",
    "actions, logprobs, entropies, _, values, _ = rec_agent.get_action_and_value(first_batch['observations'], \n",
    "                                (first_batch['actor_hxs'], first_batch['actor_cxs']),\n",
    "                                (first_batch['critic_hxs'], first_batch['critic_cxs']),\n",
    "                                first_batch['actions']\n",
    "                                )\n",
    "\n",
    "print(\"helo World\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, config:dict, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\")) -> None:\n",
    "        \"\"\"Initializes all needed training components.\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            run_id {str, optional} -- A tag used to save Tensorboard Summaries and the trained model. Defaults to \"run\".\n",
    "            device {torch.device, optional} -- Determines the training device. Defaults to cpu.\n",
    "        \"\"\"\n",
    "        # Set variables\n",
    "        self.config = config\n",
    "        self.recurrence = config[\"recurrence\"]\n",
    "        self.device = device\n",
    "        self.run_id = run_id\n",
    "        self.lr_schedule = config[\"learning_rate_schedule\"]\n",
    "        self.beta_schedule = config[\"beta_schedule\"]\n",
    "        self.cr_schedule = config[\"clip_range_schedule\"]\n",
    "        \n",
    "\n",
    "        # setup mlflow run if we are using it\n",
    "    def \n",
    "    def _get_mini_batch_loss():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a buffer, lets try and train it\n",
    "def calc_minibatch_loss(agent: PluralityRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, lr: float):\n",
    "\n",
    "    # get new log probs need to pass in the recurrent cells as well for actor and critic\n",
    "    _, logprobs, entropies, values = agent.get_action_and_value(observations, samples['actions'])\n",
    "    ratio = torch.exp(logprobs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # Compute gradients\n",
    "    for pg in self.optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.config[\"max_grad_norm\"])\n",
    "    self.optimizer.step()\n",
    "\n",
    "    return [policy_loss.cpu().data.numpy(),\n",
    "            vf_loss.cpu().data.numpy(),\n",
    "            loss.cpu().data.numpy(),\n",
    "            entropy_loss.cpu().data.numpy()]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
