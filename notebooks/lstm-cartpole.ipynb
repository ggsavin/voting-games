{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = [True,  True, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9647, 0.0177, 0.0177])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sprobs = torch.softmax(torch.tensor([5.0,1.,1.]), -1)\n",
    "sprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0360, -4.0360, -4.0360])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsprobs = torch.log_softmax(torch.tensor([5.0,1.,1.]), -1)\n",
    "lsprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9647, 0.0177, 0.0177])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = torch.distributions.Categorical(torch.tensor([5.0,1.,1.]))\n",
    "c = torch.distributions.Categorical(sprobs)\n",
    "c.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3827, -0.0586,  0.0845,  0.4314,  2.2272]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((1,5))\n",
    "a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9.0 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 3, 5}\n",
      "{1, 3, 5, 6}\n",
      "False\n",
      "{1, 3, 5, 6}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "a = set([1, 3, 5])\n",
    "b = set([1, 3 , 5, 6])\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(a == b)\n",
    "print(b | a)\n",
    "print(a - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quicker Gameplay\n",
    "\n",
    "Going to see if we can improve a gamestep by making all the actions happen at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "    \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "                if voting_type == \"plurality\":\n",
    "                    actions[villager] = game_action.item()\n",
    "                elif voting_type == \"approval\":\n",
    "                    actions[villager] = game_action.tolist()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "            \n",
    "            # clear the wolf action if needed\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
