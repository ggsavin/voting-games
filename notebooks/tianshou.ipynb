{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/gym/envs/registration.py:250: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for plugin in metadata.entry_points().get(entry_point, []):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import friendly_env, Roles, Phase\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import play_static_game, play_recurrent_game\n",
    "from notebooks.learning_agents.static_agents import (\n",
    "    random_plurality_villager, \n",
    "    random_coordinated_plurality_villager, \n",
    "    random_agent,\n",
    "    random_plurality_wolf,\n",
    "    revenge_plurality_wolf,\n",
    "    coordinated_revenge_plurality_wolf)\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "from pettingzoo.utils.conversions import parallel_to_aec, aec_to_parallel\n",
    "\n",
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv, PettingZooEnv\n",
    "from tianshou.policy import RandomPolicy, MultiAgentPolicyManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou RL\n",
    "\n",
    "Tianshou is an RL platform that has seen widespread usage, and is one of the adverstised frameworks on the Farama Foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:232: UserWarning: The base environment `werewolf_plurality_v1` does not have a `render_mode` defined.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = parallel_to_aec(friendly_env(num_agents=10, werewolves=2))\n",
    "env = PettingZooEnv(env)\n",
    "\n",
    "# each agent needs a policy\n",
    "policies = MultiAgentPolicyManager([RandomPolicy() for _ in range(10)], env)\n",
    "\n",
    "# env needs to be converted to vector format\n",
    "env = DummyVectorEnv([lambda: env])\n",
    "\n",
    "#Construct the Collector, which interfaces the policies with the vectorised environment\n",
    "collector = Collector(policies, env)\n",
    "\n",
    "result = collector.collect(n_episode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "\n",
    "\n",
    "class CustomEnvironment(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"custom_environment_v0\",\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.escape_y = None\n",
    "        self.escape_x = None\n",
    "        self.guard_y = None\n",
    "        self.guard_x = None\n",
    "        self.prisoner_y = None\n",
    "        self.prisoner_x = None\n",
    "        self.timestep = None\n",
    "        self.possible_agents = [\"prisoner\", \"guard\"]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agents = copy(self.possible_agents)\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.prisoner_x = 0\n",
    "        self.prisoner_y = 0\n",
    "\n",
    "        self.guard_x = 6\n",
    "        self.guard_y = 6\n",
    "\n",
    "        self.escape_x = random.randint(2, 5)\n",
    "        self.escape_y = random.randint(2, 5)\n",
    "\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "        return observations, {}\n",
    "\n",
    "    def step(self, actions):\n",
    "        # Execute actions\n",
    "        prisoner_action = actions[\"prisoner\"]\n",
    "        guard_action = actions[\"guard\"]\n",
    "\n",
    "        if prisoner_action == 0 and self.prisoner_x > 0:\n",
    "            self.prisoner_x -= 1\n",
    "        elif prisoner_action == 1 and self.prisoner_x < 6:\n",
    "            self.prisoner_x += 1\n",
    "        elif prisoner_action == 2 and self.prisoner_y > 0:\n",
    "            self.prisoner_y -= 1\n",
    "        elif prisoner_action == 3 and self.prisoner_y < 6:\n",
    "            self.prisoner_y += 1\n",
    "\n",
    "        if guard_action == 0 and self.guard_x > 0:\n",
    "            self.guard_x -= 1\n",
    "        elif guard_action == 1 and self.guard_x < 6:\n",
    "            self.guard_x += 1\n",
    "        elif guard_action == 2 and self.guard_y > 0:\n",
    "            self.guard_y -= 1\n",
    "        elif guard_action == 3 and self.guard_y < 6:\n",
    "            self.guard_y += 1\n",
    "\n",
    "        # Check termination conditions\n",
    "        terminations = {a: False for a in self.agents}\n",
    "        rewards = {a: 0 for a in self.agents}\n",
    "        if self.prisoner_x == self.guard_x and self.prisoner_y == self.guard_y:\n",
    "            rewards = {\"prisoner\": -1, \"guard\": 1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        elif self.prisoner_x == self.escape_x and self.prisoner_y == self.escape_y:\n",
    "            rewards = {\"prisoner\": 1, \"guard\": -1}\n",
    "            terminations = {a: True for a in self.agents}\n",
    "\n",
    "        # Check truncation conditions (overwrites termination conditions)\n",
    "        truncations = {a: False for a in self.agents}\n",
    "        if self.timestep > 100:\n",
    "            rewards = {\"prisoner\": 0, \"guard\": 0}\n",
    "            truncations = {\"prisoner\": True, \"guard\": True}\n",
    "            self.agents = []\n",
    "        self.timestep += 1\n",
    "\n",
    "        # Get observations\n",
    "        observations = {\n",
    "            a: (\n",
    "                self.prisoner_x + 7 * self.prisoner_y,\n",
    "                self.guard_x + 7 * self.guard_y,\n",
    "                self.escape_x + 7 * self.escape_y,\n",
    "            )\n",
    "            for a in self.agents\n",
    "        }\n",
    "\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((7, 7), \" \")\n",
    "        grid[self.prisoner_y, self.prisoner_x] = \"P\"\n",
    "        grid[self.guard_y, self.guard_x] = \"G\"\n",
    "        grid[self.escape_y, self.escape_x] = \"E\"\n",
    "        print(f\"{grid} \\n\")\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return MultiDiscrete([7 * 7] * 3)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prisoner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m parallel_to_aec(CustomEnvironment())\n\u001b[0;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m PettingZooEnv(env)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tianshou/env/pettingzoo_env.py:66\u001b[0m, in \u001b[0;36mPettingZooEnv.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mobservation_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\n\u001b[1;32m     55\u001b[0m            \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents), \\\n\u001b[1;32m     56\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mObservation spaces for all agents must be identical. Perhaps \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     57\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSuperSuit\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms pad_observations wrapper can help (useage: \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     58\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_observations(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\n\u001b[1;32m     61\u001b[0m            \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents), \\\n\u001b[1;32m     62\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mAction spaces for all agents must be identical. Perhaps \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     63\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSuperSuit\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms pad_action_space wrapper can help (useage: \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     64\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_action_space(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tianshou/env/pettingzoo_env.py:71\u001b[0m, in \u001b[0;36mPettingZooEnv.reset\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mdict\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 71\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mlast(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m observation:\n\u001b[1;32m     74\u001b[0m         observation_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m     75\u001b[0m             \u001b[39m'\u001b[39m\u001b[39magent_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection,\n\u001b[1;32m     76\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m: observation[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     77\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m             [\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m obm \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m obm \u001b[39min\u001b[39;00m observation[\u001b[39m'\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     79\u001b[0m         }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pettingzoo/utils/env.py:190\u001b[0m, in \u001b[0;36mAECEnv.last\u001b[0;34m(self, observe)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39massert\u001b[39;00m agent\n\u001b[1;32m    184\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserve(agent) \u001b[39mif\u001b[39;00m observe \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    186\u001b[0m     observation,\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cumulative_rewards[agent],\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminations[agent],\n\u001b[1;32m    189\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtruncations[agent],\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfos[agent],\n\u001b[1;32m    191\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prisoner'"
     ]
    }
   ],
   "source": [
    "env = parallel_to_aec(CustomEnvironment())\n",
    "env = PettingZooEnv(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
