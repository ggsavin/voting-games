{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, pare_Role, pare_Phase\n",
    "import random\n",
    "from learning_agents.approval_agents import ApprovalRecurrentAgent\n",
    "from tqdm import tqdm\n",
    "import mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approval Voting Scenario\n",
    "\n",
    "In approval voting, an agent has to present a positive opinion of agents they trust, a negative opinion (synonymous to a kill vote) of those they dont, and 0 for a neutral opinion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for static wolf strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def aggressive_wolf(env, agent, action=None):\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "    action = [-1] * len(env.possible_agents)\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    # TODO:\n",
    "    return None\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent, action=None):\n",
    "   return env.action_space(agent).sample().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Coordinated Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Villager should not have voted during the night",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRandom Coordinated Wolves\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m vs. Single Target Random Villagers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m vs. Random Villagers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m play_static_wolf_game(env, random_wolf, random_agent_action, num_times\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m, in \u001b[0;36mplay_static_wolf_game\u001b[0;34m(env, wolf_policy, villager_agent, num_times)\u001b[0m\n\u001b[1;32m     29\u001b[0m         actions[wolf] \u001b[39m=\u001b[39m action\n\u001b[1;32m     30\u001b[0m         wolf_brain[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m action\n\u001b[0;32m---> 32\u001b[0m     observations, rewards, terminations, truncations, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m     34\u001b[0m winner \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mworld_state[\u001b[39m'\u001b[39m\u001b[39mwinners\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m winner \u001b[39m==\u001b[39m pare_Role\u001b[39m.\u001b[39mVILLAGER:\n",
      "File \u001b[0;32m/workspaces/voting-games/notebooks/../voting_games/env/parallel_werewolf_env_approval.py:250\u001b[0m, in \u001b[0;36mraw_env.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mfor\u001b[39;00m agent, info \u001b[39min\u001b[39;00m infos\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_roles[agent] \u001b[39m==\u001b[39m Roles\u001b[39m.\u001b[39mVILLAGER \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m Phase\u001b[39m.\u001b[39mNIGHT:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVillager should not have voted during the night\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mphase\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m Phase\u001b[39m.\u001b[39mACCUSATION:\n\u001b[1;32m    253\u001b[0m         \u001b[39mif\u001b[39;00m info[\u001b[39m\"\u001b[39m\u001b[39mself_vote\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mException\u001b[0m: Villager should not have voted during the night"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == pare_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            # wolf steps\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                actions[wolf] = action\n",
    "                wolf_brain['action'] = action\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for training our agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    \n",
    "    def __init__(self, buffer_size: int, gamma: float, gae_lambda: float):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = None \n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.reset(gamma=gamma, gae_lambda=gae_lambda)\n",
    "\n",
    "    def reset(self, gamma: float, gae_lambda: float):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.hcxs.append(game[\"hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.cat([item for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.cat([item for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages).float()\n",
    "\n",
    "        # TODO : Gotta update these to work with a single set of hxs, rxs\n",
    "        hxs, cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"hxs\": torch.swapaxes(torch.cat(hxs)[batch_index],0,1),\n",
    "                \"cxs\": torch.swapaxes(torch.cat(cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            } \n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(buffer, env, config:dict, wolf_policy, villager_agent) -> RolloutBuffer:\n",
    "\n",
    "    buffer.reset(gamma=config[\"training\"][\"gamma\"], gae_lambda=config[\"training\"][\"gae_lambda\"])\n",
    "    \n",
    "    for _ in range(config[\"training\"][\"buffer_games_per_update\"]):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32), \n",
    "                                        torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                # this needs to be updated\n",
    "                policy, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                action = policy.sample()\n",
    "                \n",
    "                actions[villager] = action.item()\n",
    "\n",
    "                # can store some stuff \n",
    "                magent_obs[villager][\"obs\"].append(obs)\n",
    "                magent_obs[villager][\"actions\"].append(action)\n",
    "\n",
    "                # how do we get these\n",
    "                magent_obs[villager][\"logprobs\"].append(policy.log_prob(action))\n",
    "                magent_obs[villager][\"values\"].append(value)\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == plurality_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policy, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                action = policy.sample()\n",
    "                \n",
    "                actions[villager] = action.item()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == pare_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins\n",
    "\n",
    "def calc_minibatch_loss(agent: ApprovalRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # TODO:Consider checking for NAans anywhere. we cant have these. also do this in the model itself\n",
    "    # if torch.isnan(tensor).any(): print(f\"{label} contains NaN values\")\n",
    "    policies, values, _ = agent(samples['observations'], (samples['hxs'], samples['cxs']))\n",
    "    \n",
    "    # log_probs, entropies = [], []\n",
    "    log_probs = policies.log_prob(samples['actions'])\n",
    "    entropies = policies.entropy() # need to sum if we have more than 1 action\n",
    "    \n",
    "    ratio = torch.exp(log_probs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # normalized_advantage = normalized_advantage.unsqueeze(1).repeat(1, len(self.action_space_shape)) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    return [policy_loss.cpu().data.numpy(),     # policy loss\n",
    "            vf_loss.cpu().data.numpy(),         # value loss\n",
    "            loss.cpu().data.numpy(),            # total loss\n",
    "            entropy_loss.cpu().data.numpy()]    # entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_training = {\n",
    "    \"model\": {\n",
    "        \"recurrent_layers\": 1,\n",
    "        \"recurrent_hidden_size\": 128, # 256\n",
    "        \"mlp_size\": 128, # 256\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"batch_size\": 32, # 128\n",
    "        \"epochs\": 3, # 6\n",
    "        \"updates\": 10, # 1000\n",
    "        \"buffer_games_per_update\": 10, # 200\n",
    "        \"clip_range\": 0.2,\n",
    "        \"value_loss_coefficient\": 0.1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"beta\": 0.01, # entropy loss multiplier\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_eps\": 1e-8,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "    }\n",
    "}\n",
    "\n",
    "config_game = {\n",
    "    \"rewards\": {\n",
    "        \"day\": -1,\n",
    "        \"player_death\": -1,\n",
    "        \"player_win\": 10,\n",
    "        \"player_loss\": -5,\n",
    "        \"self_vote\": -1,\n",
    "        \"dead_vote\": -1,\n",
    "        \"dead_wolf\": 5,\n",
    "        \"no_viable_vote\": -1,\n",
    "        \"no_sleep\": -1,\n",
    "    },\n",
    "    \"gameplay\": {\n",
    "        \"accusation_phases\": 1,\n",
    "        \"num_agents\": 10,\n",
    "        \"num_werewolves\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"config_game\": config_game,\n",
    "    \"config_training\": config_training,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, config:dict, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\"), mlflow_uri:str=None) -> None:\n",
    "        \"\"\"Initializes all needed training components.\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            run_id {str, optional} -- A tag used to save Tensorboard Summaries and the trained model. Defaults to \"run\".\n",
    "            device {torch.device, optional} -- Determines the training device. Defaults to cpu.\n",
    "        \"\"\"\n",
    "        # Set variables\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.run_id = run_id\n",
    "        self.mlflow_uri = mlflow_uri\n",
    "        self.env = None\n",
    "\n",
    "        # we are not using schedules yet\n",
    "        # self.lr_schedule = config[\"learning_rate_schedule\"]\n",
    "        # self.beta_schedule = config[\"beta_schedule\"]\n",
    "        # self.cr_schedule = config[\"clip_range_schedule\"]\n",
    "\n",
    "        # Initialize Environment\n",
    "        env = pare(num_agents=10, werewolves=2)\n",
    "        self.env = env\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "        # Initialize Buffer\n",
    "        self.buffer = RolloutBuffer(buffer_size=10, gamma=0.99, gae_lambda=0.95)\n",
    "\n",
    "        # Initialize Model & Optimizer\n",
    "        self.agent = ApprovalRecurrentAgent({\"rec_hidden_size\": self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"], \n",
    "                                                \"rec_layers\": self.config[\"config_training\"][\"model\"][\"recurrent_layers\"], \n",
    "                                                \"hidden_mlp_size\": self.config[\"config_training\"][\"model\"][\"mlp_size\"]},\n",
    "                                                num_actions=self.env.action_space(\"player_0\").n,\n",
    "                                                obs_size=obs_size)\n",
    "        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=0.0001, eps=1e-5)\n",
    "\n",
    "        # setup mlflow run if we are using it\n",
    "\n",
    "    def train(self, idx: int):\n",
    "        if self.mlflow_uri:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "\n",
    "        name = f'{self.run_id}_{idx}'\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            \n",
    "            mlflow.log_params(self.config[\"config_training\"][\"training\"])\n",
    "            mlflow.log_params(self.config[\"config_training\"][\"model\"])\n",
    "\n",
    "            loop = tqdm(range(self.config[\"config_training\"][\"training\"][\"updates\"]))\n",
    "\n",
    "            for tid, _ in enumerate(loop):\n",
    "                # train 100 times\n",
    "                if tid % 2 == 0:\n",
    "                    # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "                    loop.set_description(\"Playing games and averaging score\")\n",
    "                    wins = []\n",
    "                    for _ in range(10):\n",
    "                        wins.append(play_recurrent_game(self.env, \n",
    "                                                        random_coordinated_single_wolf, \n",
    "                                                        self.agent, \n",
    "                                                        num_times=50,\n",
    "                                                        hidden_state_size=self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"]))\n",
    "                    \n",
    "                    mlflow.log_metric(\"avg_wins/50\", np.mean(wins))\n",
    "\n",
    "                loop.set_description(\"Filling buffer\")\n",
    "                # fill buffer\n",
    "                buff = fill_recurrent_buffer(self.buffer, \n",
    "                                             self.env,\n",
    "                                             self.config[\"config_training\"],\n",
    "                                             random_coordinated_single_wolf, \n",
    "                                             self.agent)\n",
    "\n",
    "                # train info will hold our metrics\n",
    "                train_info = []\n",
    "                loop.set_description(\"Epoch Training\")\n",
    "                for _ in range(self.config['config_training'][\"training\"]['epochs']):\n",
    "                    # run through batches and train network\n",
    "                    for batch in buff.get_minibatch_generator(self.config['config_training'][\"training\"]['batch_size']):\n",
    "                        train_info.append(calc_minibatch_loss(self.agent, \n",
    "                                                              batch, \n",
    "                                                              clip_range=self.config['config_training'][\"training\"]['clip_range'], \n",
    "                                                              beta=self.config['config_training'][\"training\"]['beta'], \n",
    "                                                              v_loss_coef=self.config['config_training'][\"training\"]['value_loss_coefficient'], \n",
    "                                                              optimizer=self.optimizer))\n",
    "\n",
    "                train_stats = np.mean(train_info, axis=0)\n",
    "                mlflow.log_metric(\"policy loss\", train_stats[0])\n",
    "                mlflow.log_metric(\"value loss\", train_stats[1])\n",
    "                mlflow.log_metric(\"total loss\", train_stats[2])\n",
    "                mlflow.log_metric(\"entropy loss\", train_stats[3])\n",
    "            # one more run\n",
    "\n",
    "        # torch.save(self.agent, f\"rnn_agent_{self.run_id}\")\n",
    "\n",
    "\n",
    "trainer = PPOTrainer(config=config,run_id=\"Approval agent training\", mlflow_uri=\"http://mlflow:5000\")\n",
    "trainer.train(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_coordinated_wolf(env):\n",
    "    actions = {}\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return actions\n",
    "\n",
    "def aggressive_wolf(env):\n",
    "    actions = {}\n",
    "\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [-1] * len(env.possible_agents)\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return actions\n",
    "\n",
    "def random_wolfs(env):\n",
    "    return {wolf: env.action_space(wolf).sample().tolist() for\n",
    "            wolf in set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])}\n",
    "\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    \n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent):\n",
    "   return env.action_space(agent).sample().tolist()\n",
    "\n",
    "# def random_wolf(env, action=None):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Coordinated Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 117 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:07<00:00, 136.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 69 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:11<00:00, 89.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n",
      "Aggresive Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 23 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:06<00:00, 160.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 7 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:10<00:00, 95.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n",
      "Random Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 666 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:08<00:00, 112.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 585 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:12<00:00, 78.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            if env.world_state[\"phase\"] != pare_Phase.NIGHT:\n",
    "                # villagers actions\n",
    "                for villager in villagers:\n",
    "                    actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            # wolf steps\n",
    "            actions = actions | wolf_policy(env)\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent structure will be much the same as the agent structure in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning_agents.approval_agents import SequentialAgent\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [34:47<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 32 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 10000 #\n",
    "update_epochs = 3 #\n",
    "\n",
    "# stats to keep track of for custom metrics\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_agents = 10\n",
    "env = pare(num_agents=num_agents, werewolves=2)\n",
    "env.reset()\n",
    "observation_size = env.convert_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "training_agent = SequentialAgent(num_players=num_agents, approval_states= 3, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(training_agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Training our agents'):\n",
    "    \n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': [], 'terms': []} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while env.agents:\n",
    "                actions = {}\n",
    "                villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "                wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "                if env.world_state[\"phase\"] != pare_Phase.NIGHT:\n",
    "                    # villagers actions\n",
    "                    for villager in villagers:\n",
    "                        # only cares about observations\n",
    "                        obs = torch.Tensor(env.convert_obs(observations[villager]['observation']))\n",
    "                        ml_action, logprobs, _, value = training_agent.get_action_and_value(obs)\n",
    "                        \n",
    "                        actions[villager] = training_agent.convert_actions_to_approvals(ml_action)\n",
    "\n",
    "                        # missing rewards, term\n",
    "                        magent_obs[villager][\"obs\"].append(obs)\n",
    "                        magent_obs[villager][\"actions\"].append(ml_action)\n",
    "                        magent_obs[villager][\"logprobs\"].append(logprobs)\n",
    "                        magent_obs[villager][\"values\"].append(value)\n",
    "\n",
    "                # TODO : pass in a wolf policy possibly\n",
    "                actions = actions | random_coordinated_wolf(env)\n",
    "            \n",
    "                observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "                # lets add the post step information now\n",
    "                # TODO: Should we not care about villagers losing here? What if they die at night, we want them to get a death reward\n",
    "                # TODO: What if the game ends on a night kill? We want the villagers to still get their rewards\n",
    "                #if env.history[-1][\"phase\"] != pare_Phase.NIGHT:\n",
    "                for villager in villagers:\n",
    "                    if env.history[-1][\"phase\"] == pare_Phase.NIGHT:\n",
    "                        magent_obs[villager][\"rewards\"][-1] += rewards[villager]\n",
    "                        magent_obs[villager][\"terms\"][-1] += terminations[villager]\n",
    "                    else:\n",
    "                        magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                        magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "            # POST GAME STATS #\n",
    "            winner = env.world_state['winners']\n",
    "\n",
    "            if winner == pare_Role.VILLAGER:\n",
    "                villager_wins += 1\n",
    "        \n",
    "            # END OF POST GAME STATS #\n",
    "            mlflow.log_metric(\"villager wins\", villager_wins)\n",
    "            if episode % 50 == 0:\n",
    "                #wwins, vwins = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=100)\n",
    "                #mlflow.log_metric(\"wwins\", wwins)\n",
    "                #mlflow.log_metric(\"vwins\", vwins)\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We will do this for each agent in the episode\n",
    "        # essentially we are calculating advantages and returns\n",
    "            with torch.no_grad():\n",
    "                for player, records in magent_obs.items():\n",
    "                    # print(f'{records}')\n",
    "                    advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "\n",
    "                    for t in reversed(range(len(records['obs']))):\n",
    "                        # print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                        # not using terms, as these are episodic\n",
    "\n",
    "                        ## this was the last one. We are not using any terminal states in a good way\n",
    "\n",
    "                        if t == len(records['obs']) - 1:\n",
    "                            #print(f'T: {t} - Rewards at end : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                            #print(f'T: {t} - Actions at end : {torch.tensor(records[\"actions\"])[t]} ')\n",
    "                            delta = records[\"rewards\"][t] - records[\"values\"][t]\n",
    "                            advantages[t]  = delta\n",
    "                        else:\n",
    "                            #print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                            #print(f'T: {t} - Actions : {torch.tensor(records[\"actions\"])[t]} ')                    \n",
    "                            delta = records[\"rewards\"][t] + gamma * records[\"values\"][t+1] - records[\"values\"][t]\n",
    "                            advantages[t]  = delta + gamma * gamma * advantages[t+1]\n",
    "\n",
    "                        #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "                    magent_obs[player][\"advantages\"] = advantages\n",
    "                    magent_obs[player][\"returns\"] = advantages + torch.tensor(records[\"values\"])\n",
    "                        #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "        \n",
    "            # new logic, maybe we do this after a couple of games, so we get more data overall?\n",
    "\n",
    "            # optimize the policy and the value network now\n",
    "            # we can take all our observations now and flatten them into one bigger list of individual transitions\n",
    "            # TODO: could make this setting into a single loop, but maybe this is clearer. ALso could make all these tensors earlier\n",
    "            b_observations = torch.cat([torch.stack(item['obs']) for item in magent_obs.values()])\n",
    "            b_logprobs = torch.cat([torch.stack(item['logprobs']) for item in magent_obs.values()])\n",
    "            b_actions = torch.cat([torch.stack(item['actions']) for item in magent_obs.values()])\n",
    "            b_returns = torch.cat([item['returns'] for item in magent_obs.values()])\n",
    "            b_values = torch.cat([torch.stack(item['values']) for item in magent_obs.values()])\n",
    "            b_advantages =  torch.cat([item['advantages'] for item in magent_obs.values()])\n",
    "\n",
    "\n",
    "\n",
    "            # b_index stands for batch index\n",
    "            b_index = np.arange(len(b_observations))\n",
    "            clip_fracs = []\n",
    "            for epoch in range(update_epochs):\n",
    "                np.random.shuffle(b_index)\n",
    "                for start in range(0, len(b_observations), batch_size):\n",
    "                    end = start + batch_size\n",
    "                    batch_index = b_index[start:end]\n",
    "\n",
    "                    # TODO: batched actions, How to handle batched observations and acctions properly in the agent\n",
    "                    #       Maybe a different \n",
    "\n",
    "                    # newlogprob needs to return a list of logprobs\n",
    "                    _, newlogprob, entropy, value = training_agent.get_batched_action_and_value(\n",
    "                        b_observations[batch_index], b_actions[batch_index])\n",
    "                    \n",
    "                    logratio = newlogprob - b_logprobs[batch_index]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clip_fracs += [\n",
    "                            ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                        ]\n",
    "                    \n",
    "                    # normalizing advantages\n",
    "                    advantages = b_advantages[batch_index]\n",
    "                    advantages = advantages.float()\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                    # policy loss\n",
    "                    pg_loss1 = -advantages * ratio\n",
    "                    pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                    # value loss\n",
    "                    value = value.flatten()\n",
    "                    v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                    v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                        value - b_values[batch_index],\n",
    "                        -clip_coef,\n",
    "                        clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    # loss = torch.Variable(loss, requires_grad = True)\n",
    "                    loss.requires_grad = True\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # could move them from GPU here\n",
    "            y_pred, y_true = b_values.numpy(), b_returns.numpy()\n",
    "            var_y = np.var(y_true)\n",
    "            explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # At the end, print some stuff here for overall stats\n",
    "\n",
    "        # print(f'Average game length = {avg_game_length:.2f}')\n",
    "        # print(f'Wolf wins : {wolf_wins}')\n",
    "        # print(f'Villager wins: {villager_wins}')\n",
    "        # print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "\n",
    "torch.save(training_agent, \"pare_no_dead_wolf_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, approval_states, num_players, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size+1, 256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256,256)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(256, approval_states), std=0.01),\n",
    "        )\n",
    "\n",
    "        self.num_players = num_players\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        # TODO: We need torch.mean because PPO will use value, and we have a bunch here. \n",
    "        #       Do we need to change PPO here?\n",
    "        return torch.mean(self.critic(torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])))\n",
    "    \n",
    "    # only doing this for the PPO batched call so I don't need extra logic in the regular get action and value\n",
    "    def get_batched_action_and_value(self, x, actions=None):\n",
    "\n",
    "        if actions is None:\n",
    "            raise ValueError(\"We need batched actions here\")\n",
    "\n",
    "        log_probs = []\n",
    "        entropies = []\n",
    "        critics = []\n",
    "        for current_obs, action in zip(x, actions):\n",
    "            updated_obs = torch.stack([torch.cat((torch.tensor([i]), current_obs)) for i in range(self.num_players)])\n",
    "\n",
    "            logits = self.actor(updated_obs)\n",
    "            probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "            \n",
    "            # update our return tensors\n",
    "            log_probs.append(torch.sum(probs.log_prob(action)))\n",
    "            entropies.append(torch.prod(probs.entropy()))\n",
    "            critics.append(torch.mean(self.critic(updated_obs)))\n",
    "            \n",
    "        return actions, torch.stack(log_probs), torch.stack(entropies), torch.stack(critics)\n",
    "\n",
    "    def convert_actions_to_approvals(self, actions):\n",
    "        return [-1 if a == 2 else a.item() for a in actions]\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        # could call the network each time, with a different integer for each player?  get approvals that way\n",
    "        # x is the flattened observation. we should go ahead and run each of the player_ids appended to full obs to get multiple classifications\n",
    "        # how  to handle entropy here? maybe we multiply all the probs, and then calculate the overall entropy\n",
    "        # self.critic needs to be changed too, to return an array\n",
    "\n",
    "        # option to have critic/actors for every single player?\n",
    "\n",
    "        # option to also delevt n-1 * n-2 for -1s on the wolf\n",
    "        \n",
    "        # get logits for every single player in the game.\n",
    "        x = torch.stack([torch.cat((torch.tensor([i]), x)) for i in range(self.num_players)])\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        # we multiply the entropy, and we add the log_probs together\n",
    "        # TODO: multiple values for critic. should I average?\n",
    "        return action, torch.sum(probs.log_prob(action)), torch.prod(probs.entropy()), torch.mean(self.critic(x))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_report_stats(env, information, ignore_wolf=True, mlflow_uri=None):\n",
    "    total_self_votes = len([vals for player, vals in information.items() if vals[\"self_vote\"] and (ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF)])\n",
    "    total_dead_votes = sum([vals[\"dead_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    total_viable_votes = sum([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "\n",
    "    avg_self_votes = total_self_votes/len(information)\n",
    "    avg_dead_votes = total_dead_votes/len(information)\n",
    "    avg_viable_votes = total_viable_votes/len(information)\n",
    "\n",
    "    return {\n",
    "        \"total_self_votes\": total_self_votes,\n",
    "        \"total_dead_votes\": total_dead_votes,\n",
    "        \"total_viable_votes\": total_viable_votes,\n",
    "        \"avg_self_votes\": avg_self_votes,\n",
    "        \"avg_dead_votes\": avg_dead_votes,\n",
    "        \"avg_viable_votes\": avg_viable_votes,\n",
    "        \"players_with_viable_votes\": len([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 2 out of a total of 10 games: 100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "def play_static_wolf_game_w_agents(env, wolf_policy, trained_villager, num_times=2) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    with mlflow.start_run(run_name='Gameplay stats'):\n",
    "        for _ in loop:\n",
    "            observations, rewards, terminations, truncations, infos = env.reset()\n",
    "            while env.agents:\n",
    "                actions = {}\n",
    "\n",
    "                villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "                wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "                # villager steps\n",
    "                if env.world_state[\"phase\"] != pare_Phase.NIGHT:\n",
    "                    # villagers actions\n",
    "                    for villager in villagers:\n",
    "                        obs = torch.Tensor(env.convert_obs(observations[villager]['observation']))\n",
    "                        action, logprobs, _, value = trained_villager.get_action_and_value(obs)\n",
    "                        actions[villager] = trained_villager.convert_actions_to_approvals(action)\n",
    "                # wolf steps\n",
    "                actions = actions | wolf_policy(env)\n",
    "            \n",
    "                observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "                mlflow.log_metrics(collect_and_report_stats(env, infos))\n",
    "            winner = env.world_state['winners']\n",
    "            if winner == pare_Role.VILLAGER:\n",
    "                villager_wins += 1\n",
    "\n",
    "            loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "# trained_agent = torch.load(\"long_approval_agent\")\n",
    "test_agent = torch.load(\"pare_no_dead_wolf_reward\")\n",
    "play_static_wolf_game_w_agents(env, random_coordinated_wolf, test_agent, num_times=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
