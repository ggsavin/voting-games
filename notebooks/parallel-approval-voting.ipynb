{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import copy\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import pare, pare_Role, pare_Phase\n",
    "import random\n",
    "from learning_agents.approval_agents import ApprovalRecurrentAgent\n",
    "from tqdm import tqdm\n",
    "import mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approval Voting Scenario\n",
    "\n",
    "In approval voting, an agent has to present a positive opinion of agents they trust, a negative opinion (synonymous to a kill vote) of those they dont, and 0 for a neutral opinion."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for static wolf strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "def aggressive_wolf(env, agent, action=None):\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "    action = [-1] * len(env.possible_agents)\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    # TODO:\n",
    "    return None\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent, action=None):\n",
    "   return env.action_space(agent).sample().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Coordinated Wolves\n",
      "\t vs. Single Target Random Villagers\n",
      "\t\t Villager wins : 389\n",
      "\t vs. Random Villagers\n",
      "\t\t Villager wins : 295\n",
      "------------------------------------\n",
      "\n",
      "Aggresive Wolves\n",
      "\t vs. Single Target Random Villagers\n",
      "\t\t Villager wins : 25\n",
      "\t vs. Random Villagers\n",
      "\t\t Villager wins : 7\n",
      "------------------------------------\n",
      "\n",
      "Random Wolves\n",
      "\t vs. Single Target Random Villagers\n",
      "\t\t Villager wins : 661\n",
      "\t vs. Random Villagers\n",
      "\t\t Villager wins : 621\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in range(num_times):\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            for villager in villagers:\n",
    "                actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == pare_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            # wolf steps\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                actions[wolf] = action\n",
    "                wolf_brain['action'] = action\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "    return villager_wins\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_single_target_villager, num_times=1000)}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_wolf, random_agent_action, num_times=1000)}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)}')\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_single_target_villager, num_times=1000)}')\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "print(f'\\t\\t Villager wins : {play_static_wolf_game(env, random_agent_action, random_agent_action, num_times=1000)}')\n",
    "print(\"------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for training our agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    \n",
    "    def __init__(self, buffer_size: int, gamma: float, gae_lambda: float):\n",
    "        '''\n",
    "            @bufffer_size: This is the number of trajectories\n",
    "        '''\n",
    "        \n",
    "        self.rewards = None\n",
    "        self.actions = None\n",
    "        self.dones = None\n",
    "        self.observations = None\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = None \n",
    "\n",
    "        self.log_probs = None\n",
    "        self.values = None\n",
    "        self.advantages = None\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.reset(gamma=gamma, gae_lambda=gae_lambda)\n",
    "\n",
    "    def reset(self, gamma: float, gae_lambda: float):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.dones = []\n",
    "        self.observations = []\n",
    "\n",
    "        # do we want these for both actor and critic?\n",
    "        self.hcxs = []\n",
    "\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "        self.gamma = gamma \n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def add_replay(self, game) -> bool:\n",
    "         \n",
    "         self.rewards.append(game['rewards'])\n",
    "         self.actions.append(game['actions'])\n",
    "         self.dones.append(game[\"terms\"])\n",
    "         self.observations.append(game[\"obs\"])\n",
    "         self.log_probs.append(game[\"logprobs\"])\n",
    "         self.values.append(game[\"values\"])\n",
    "         self.hcxs.append(game[\"hcxs\"][:-1])\n",
    "        \n",
    "         advantages, returns = self._calculate_advantages(game)\n",
    "             \n",
    "         self.advantages.append(advantages)\n",
    "         self.returns.append(returns)\n",
    "\n",
    "         return True\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _calculate_advantages(self, game):\n",
    "        \"\"\"Generalized advantage estimation (GAE)\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(torch.tensor(game['rewards']))\n",
    "\n",
    "        for t in reversed(range(len(game['rewards']))):\n",
    "             delta = game['rewards'][t] + self.gamma * game['values'][max((t+1)%len(game['rewards']),t)] - game['values'][t]\n",
    "             advantages[t] = delta + self.gamma * self.gae_lambda * advantages[max((t+1)%len(game['rewards']),t)]\n",
    "\n",
    "        # adv and returns\n",
    "        return advantages, advantages + torch.tensor(game['values'])\n",
    "    \n",
    "    def get_minibatch_generator(self, batch_size):\n",
    "\n",
    "        # fold and stack observations\n",
    "        actions = torch.stack([torch.tensor(item) for sublist in self.actions for item in sublist])\n",
    "        logprobs = torch.stack([item.reshape(-1) for sublist in self.log_probs for item in sublist])\n",
    "        returns = torch.cat(self.returns)\n",
    "        values = torch.cat([item for sublist in self.values for item in sublist])\n",
    "        advantages = torch.cat(self.advantages).float()\n",
    "\n",
    "        # TODO : Gotta update these to work with a single set of hxs, rxs\n",
    "        hxs, cxs = zip(*[(hxs, cxs) for hxs, cxs in [item for sublist in self.hcxs for item in sublist]])\n",
    "        observations = torch.cat([item for sublist in self.observations for item in sublist])\n",
    "\n",
    "        index = np.arange(len(observations))\n",
    "\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        # We do not handle remaining stuff here\n",
    "        for start in range(0,len(observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = index[start:end].astype(int)\n",
    "\n",
    "            yield {\n",
    "                \"actions\": actions[batch_index],\n",
    "                \"logprobs\": logprobs[batch_index],\n",
    "                \"returns\": returns[batch_index],\n",
    "                \"values\": values[batch_index],\n",
    "                \"advantages\": advantages[batch_index],\n",
    "                # we are using sequence lengths of 1, because everything should be encoded in \n",
    "                \"hxs\": torch.swapaxes(torch.cat(hxs)[batch_index],0,1),\n",
    "                \"cxs\": torch.swapaxes(torch.cat(cxs)[batch_index],0,1),\n",
    "                \"observations\": observations[batch_index]\n",
    "            } \n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_recurrent_buffer(buffer, env, config:dict, wolf_policy, villager_agent) -> RolloutBuffer:\n",
    "\n",
    "    buffer.reset(gamma=config[\"training\"][\"gamma\"], gae_lambda=config[\"training\"][\"gae_lambda\"])\n",
    "    \n",
    "    for _ in range(config[\"training\"][\"buffer_games_per_update\"]):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              'rewards': [], \n",
    "                              'actions': [], \n",
    "                              'logprobs': [], \n",
    "                              'values': [], \n",
    "                              'terms': [],\n",
    "\n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32), \n",
    "                                        torch.zeros((1,1,config[\"model\"][\"recurrent_hidden_size\"]), dtype=torch.float32))]\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "                # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                # this needs to be updated\n",
    "                policies, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                \n",
    "                policy_action, game_action = villager_agent.get_action_from_policies(policies)\n",
    "                actions[villager] = game_action.tolist()\n",
    "\n",
    "                # can store some stuff \n",
    "                magent_obs[villager][\"obs\"].append(obs)\n",
    "                magent_obs[villager][\"actions\"].append(policy_action)\n",
    "\n",
    "                # how do we get these\n",
    "                magent_obs[villager][\"logprobs\"].append(torch.stack([policy.log_prob(opinion) for policy, opinion in zip(policies, policy_action)], dim=1).reshape(-1))\n",
    "                magent_obs[villager][\"values\"].append(value)\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == pare_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "            for villager in villagers:\n",
    "                magent_obs[villager][\"rewards\"].append(rewards[villager])\n",
    "                magent_obs[villager][\"terms\"].append(terminations[villager])\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        for agent in magent_obs:\n",
    "            buffer.add_replay(magent_obs[agent])\n",
    "    \n",
    "    return buffer\n",
    "\n",
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "        \n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policies, value, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "\n",
    "                _, game_action = villager_agent.get_action_from_policies(policies)\n",
    "                actions[villager] = game_action.tolist()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == pare_Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            # actions = actions | wolf_policy(env)\n",
    "        \n",
    "            next_observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins\n",
    "\n",
    "def calc_minibatch_loss(agent: ApprovalRecurrentAgent, samples: dict, clip_range: float, beta: float, v_loss_coef: float, optimizer):\n",
    "\n",
    "    # TODO:Consider checking for NAans anywhere. we cant have these. also do this in the model itself\n",
    "    # if torch.isnan(tensor).any(): print(f\"{label} contains NaN values\")\n",
    "    policies, values, _ = agent(samples['observations'], (samples['hxs'], samples['cxs']))\n",
    "    \n",
    "    log_probs, entropies = [], []\n",
    "    for i, policy_head in enumerate(policies):\n",
    "        # append the log_probs for 1 -> n other agent opinions\n",
    "        log_probs.append(policy_head.log_prob(samples['actions'][:,i]))\n",
    "        entropies.append(policy_head.entropy())\n",
    "    log_probs = torch.stack(log_probs, dim=1)\n",
    "    entropies = torch.stack(entropies, dim=1).sum(1).reshape(-1)\n",
    "    \n",
    "    ratio = torch.exp(log_probs - samples['logprobs'])\n",
    "\n",
    "    # normalize advantages\n",
    "    norm_advantage = (samples[\"advantages\"] - samples[\"advantages\"].mean()) / (samples[\"advantages\"].std() + 1e-8)\n",
    "    # 10 here is for number of actions an agent had to output \n",
    "    norm_advantage = norm_advantage.unsqueeze(1).repeat(1, agent.num_players) # Repeat is necessary for multi-discrete action spaces\n",
    "\n",
    "    # policy loss w/ surrogates\n",
    "    surr1 = norm_advantage * ratio\n",
    "    surr2 = norm_advantage * torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range)\n",
    "    policy_loss = torch.min(surr1, surr2)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    # Value  function loss\n",
    "    clipped_values = samples[\"values\"] + (values - samples[\"values\"]).clamp(min=-clip_range, max=clip_range)\n",
    "    vf_loss = torch.max((values - samples['returns']) ** 2, (clipped_values - samples[\"returns\"]) ** 2)\n",
    "    vf_loss = vf_loss.mean()\n",
    "\n",
    "    # Entropy Bonus\n",
    "    entropy_loss = entropies.mean()\n",
    "\n",
    "    # Complete loss\n",
    "    loss = -(policy_loss - v_loss_coef * vf_loss + beta * entropy_loss)\n",
    "\n",
    "\n",
    "    # TODO : do i reset the LR here? do I want to?\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return [policy_loss.cpu().data.numpy(),     # policy loss\n",
    "            vf_loss.cpu().data.numpy(),         # value loss\n",
    "            loss.cpu().data.numpy(),            # total loss\n",
    "            entropy_loss.cpu().data.numpy()]    # entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_training = {\n",
    "    \"model\": {\n",
    "        \"recurrent_layers\": 1,\n",
    "        \"recurrent_hidden_size\": 128, # 256\n",
    "        \"mlp_size\": 128, # 256\n",
    "        \"approval_states\": 3, # dislike,neutral,like\n",
    "    },\n",
    "    \"training\" : {\n",
    "        \"batch_size\": 32, # 128\n",
    "        \"epochs\": 3, # 6\n",
    "        \"updates\": 10, # 1000\n",
    "        \"buffer_games_per_update\": 10, # 200\n",
    "        \"clip_range\": 0.2,\n",
    "        \"value_loss_coefficient\": 0.1,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"beta\": 0.01, # entropy loss multiplier\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"adam_eps\": 1e-8,\n",
    "        \"gamma\": 0.99,\n",
    "        \"gae_lambda\": 0.95,\n",
    "    }\n",
    "}\n",
    "\n",
    "config_game = {\n",
    "    \"rewards\": {\n",
    "        \"day\": -1,\n",
    "        \"player_death\": -1,\n",
    "        \"player_win\": 10,\n",
    "        \"player_loss\": -5,\n",
    "        \"self_vote\": -1,\n",
    "        \"dead_vote\": -1,\n",
    "        \"dead_wolf\": 5,\n",
    "        \"no_viable_vote\": -1,\n",
    "        \"no_sleep\": -1,\n",
    "    },\n",
    "    \"gameplay\": {\n",
    "        \"accusation_phases\": 1,\n",
    "        \"num_agents\": 10,\n",
    "        \"num_werewolves\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"config_game\": config_game,\n",
    "    \"config_training\": config_training,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Training: 100%|██████████| 10/10 [05:00<00:00, 30.08s/it]                  \n"
     ]
    }
   ],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, config:dict, run_id:str=\"run\", device:torch.device=torch.device(\"cpu\"), mlflow_uri:str=None) -> None:\n",
    "        \"\"\"Initializes all needed training components.\n",
    "        Arguments:\n",
    "            config {dict} -- Configuration and hyperparameters of the environment, trainer and model.\n",
    "            run_id {str, optional} -- A tag used to save Tensorboard Summaries and the trained model. Defaults to \"run\".\n",
    "            device {torch.device, optional} -- Determines the training device. Defaults to cpu.\n",
    "        \"\"\"\n",
    "        # Set variables\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.run_id = run_id\n",
    "        self.mlflow_uri = mlflow_uri\n",
    "        self.env = None\n",
    "\n",
    "        # we are not using schedules yet\n",
    "        # self.lr_schedule = config[\"learning_rate_schedule\"]\n",
    "        # self.beta_schedule = config[\"beta_schedule\"]\n",
    "        # self.cr_schedule = config[\"clip_range_schedule\"]\n",
    "\n",
    "        # Initialize Environment\n",
    "        env = pare(num_agents=self.config[\"config_game\"][\"gameplay\"][\"num_agents\"], werewolves=self.config[\"config_game\"][\"gameplay\"][\"num_werewolves\"])\n",
    "        self.env = env\n",
    "        \n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "\n",
    "        # Initialize Buffer\n",
    "        self.buffer = RolloutBuffer(buffer_size=10, gamma=0.99, gae_lambda=0.95)\n",
    "\n",
    "        # Initialize Model & Optimizer\n",
    "        self.agent = ApprovalRecurrentAgent({\"rec_hidden_size\": self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"], \n",
    "                                                \"rec_layers\": self.config[\"config_training\"][\"model\"][\"recurrent_layers\"], \n",
    "                                                \"hidden_mlp_size\": self.config[\"config_training\"][\"model\"][\"mlp_size\"],\n",
    "                                                \"approval_states\": self.config[\"config_training\"][\"model\"][\"approval_states\"]},\n",
    "                                                num_players=self.config[\"config_game\"][\"gameplay\"][\"num_agents\"],\n",
    "                                                obs_size=obs_size)\n",
    "        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=0.0001, eps=1e-5)\n",
    "\n",
    "        # setup mlflow run if we are using it\n",
    "\n",
    "    def train(self, idx: int):\n",
    "        if self.mlflow_uri:\n",
    "            mlflow.set_tracking_uri(self.mlflow_uri)\n",
    "\n",
    "        name = f'{self.run_id}_{idx}'\n",
    "        with mlflow.start_run(run_name=name):\n",
    "            \n",
    "            mlflow.log_params(self.config[\"config_training\"][\"training\"])\n",
    "            mlflow.log_params(self.config[\"config_training\"][\"model\"])\n",
    "\n",
    "            loop = tqdm(range(self.config[\"config_training\"][\"training\"][\"updates\"]))\n",
    "\n",
    "            for tid, _ in enumerate(loop):\n",
    "                # train 100 times\n",
    "                if tid % 2 == 0:\n",
    "                    # print(f'Playing games with our trained agent after {epid} epochs')\n",
    "                    loop.set_description(\"Playing games and averaging score\")\n",
    "                    wins = []\n",
    "                    for _ in range(10):\n",
    "                        wins.append(play_recurrent_game(self.env, \n",
    "                                                        random_wolf, \n",
    "                                                        self.agent, \n",
    "                                                        num_times=50,\n",
    "                                                        hidden_state_size=self.config[\"config_training\"][\"model\"][\"recurrent_hidden_size\"]))\n",
    "                    \n",
    "                    mlflow.log_metric(\"avg_wins/50\", np.mean(wins))\n",
    "\n",
    "                loop.set_description(\"Filling buffer\")\n",
    "                # fill buffer\n",
    "                buff = fill_recurrent_buffer(self.buffer, \n",
    "                                             self.env,\n",
    "                                             self.config[\"config_training\"],\n",
    "                                             random_wolf, \n",
    "                                             self.agent)\n",
    "\n",
    "                # train info will hold our metrics\n",
    "                train_info = []\n",
    "                loop.set_description(\"Epoch Training\")\n",
    "                for _ in range(self.config['config_training'][\"training\"]['epochs']):\n",
    "                    # run through batches and train network\n",
    "                    for batch in buff.get_minibatch_generator(self.config['config_training'][\"training\"]['batch_size']):\n",
    "                        train_info.append(calc_minibatch_loss(self.agent, \n",
    "                                                              batch, \n",
    "                                                              clip_range=self.config['config_training'][\"training\"]['clip_range'], \n",
    "                                                              beta=self.config['config_training'][\"training\"]['beta'], \n",
    "                                                              v_loss_coef=self.config['config_training'][\"training\"]['value_loss_coefficient'], \n",
    "                                                              optimizer=self.optimizer))\n",
    "\n",
    "                train_stats = np.mean(train_info, axis=0)\n",
    "                mlflow.log_metric(\"policy loss\", train_stats[0])\n",
    "                mlflow.log_metric(\"value loss\", train_stats[1])\n",
    "                mlflow.log_metric(\"total loss\", train_stats[2])\n",
    "                mlflow.log_metric(\"entropy loss\", train_stats[3])\n",
    "            # one more run\n",
    "\n",
    "        # torch.save(self.agent, f\"rnn_agent_{self.run_id}\")\n",
    "\n",
    "\n",
    "trainer = PPOTrainer(config=config,run_id=\"Approval agent training\", mlflow_uri=\"http://mlflow:5000\")\n",
    "# trainer = PPOTrainer(config=config,run_id=\"Approval agent training\")\n",
    "trainer.train(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "def random_coordinated_wolf(env):\n",
    "    actions = {}\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return actions\n",
    "\n",
    "def aggressive_wolf(env):\n",
    "    actions = {}\n",
    "\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [-1] * len(env.possible_agents)\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return actions\n",
    "\n",
    "def random_wolfs(env):\n",
    "    return {wolf: env.action_space(wolf).sample().tolist() for\n",
    "            wolf in set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])}\n",
    "\n",
    "\n",
    "def revenge_coordinated_wolf(env, actions = None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # who tried to vote out a wolf last time?\n",
    "    \n",
    "    target = random.choice(list(villagers_remaining))\n",
    "    # pick \n",
    "    for wolf in wolves_remaining:\n",
    "        actions[wolf] = [0] * len(env.possible_agents)\n",
    "        actions[wolf][int(target.split(\"_\")[-1])] = -1\n",
    "        for curr_wolf in wolves_remaining:\n",
    "            actions[wolf][int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "    # for wolf in env.werewolves_remaining:\n",
    "\n",
    "def random_single_target_villager(env, agent):\n",
    "    targets = set(env.world_state[\"alive\"]) - set([agent])\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(agent.split(\"_\")[-1])] = 1\n",
    "    action[int(random.choice(list(targets)).split(\"_\")[-1])] = -1\n",
    "\n",
    "    return action\n",
    "    # for villager in env.villagers_remaining:\n",
    "\n",
    "# random_coordinated_wolf(env)\n",
    "def random_agent_action(env, agent):\n",
    "   return env.action_space(agent).sample().tolist()\n",
    "\n",
    "# def random_wolf(env, action=None):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Coordinated Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 117 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:07<00:00, 136.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 69 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:11<00:00, 89.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n",
      "Aggresive Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 23 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:06<00:00, 160.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 7 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:10<00:00, 95.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n",
      "Random Wolves\n",
      "\t vs. Single Target Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 666 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:08<00:00, 112.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t vs. Random Villagers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 585 out of a total of 1000 games: 100%|██████████| 1000/1000 [00:12<00:00, 78.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play_static_wolf_game(env, wolf_policy, villager_agent, num_times=100) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    for _ in loop:\n",
    "        observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        while env.agents:\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villager steps\n",
    "            if env.world_state[\"phase\"] != pare_Phase.NIGHT:\n",
    "                # villagers actions\n",
    "                for villager in villagers:\n",
    "                    actions[villager] = villager_agent(env, villager)\n",
    "\n",
    "            # wolf steps\n",
    "            actions = actions | wolf_policy(env)\n",
    "        \n",
    "            observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == pare_Role.VILLAGER:\n",
    "            villager_wins += 1\n",
    "\n",
    "        loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "print(\"Random Coordinated Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_coordinated_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Aggresive Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, aggressive_wolf, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n",
    "print(\"Random Wolves\")\n",
    "print(\"\\t vs. Single Target Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_single_target_villager, num_times=1000)\n",
    "print(\"\\t vs. Random Villagers\")\n",
    "play_static_wolf_game(env, random_wolfs, random_agent_action, num_times=1000)\n",
    "print(\"------------------------------------\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Agents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent structure will be much the same as the agent structure in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_report_stats(env, information, ignore_wolf=True, mlflow_uri=None):\n",
    "    total_self_votes = len([vals for player, vals in information.items() if vals[\"self_vote\"] and (ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF)])\n",
    "    total_dead_votes = sum([vals[\"dead_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    total_viable_votes = sum([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "\n",
    "    avg_self_votes = total_self_votes/len(information)\n",
    "    avg_dead_votes = total_dead_votes/len(information)\n",
    "    avg_viable_votes = total_viable_votes/len(information)\n",
    "\n",
    "    return {\n",
    "        \"total_self_votes\": total_self_votes,\n",
    "        \"total_dead_votes\": total_dead_votes,\n",
    "        \"total_viable_votes\": total_viable_votes,\n",
    "        \"avg_self_votes\": avg_self_votes,\n",
    "        \"avg_dead_votes\": avg_dead_votes,\n",
    "        \"avg_viable_votes\": avg_viable_votes,\n",
    "        \"players_with_viable_votes\": len([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villagers won 2 out of a total of 10 games: 100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "def play_static_wolf_game_w_agents(env, wolf_policy, trained_villager, num_times=2) -> tuple(pare_Role):\n",
    "\n",
    "    villager_wins = 0\n",
    "    loop = tqdm(range(num_times))\n",
    "\n",
    "    with mlflow.start_run(run_name='Gameplay stats'):\n",
    "        for _ in loop:\n",
    "            observations, rewards, terminations, truncations, infos = env.reset()\n",
    "            while env.agents:\n",
    "                actions = {}\n",
    "\n",
    "                villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "                wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "                # villager steps\n",
    "                if env.world_state[\"phase\"] != pare_Phase.NIGHT:\n",
    "                    # villagers actions\n",
    "                    for villager in villagers:\n",
    "                        obs = torch.Tensor(env.convert_obs(observations[villager]['observation']))\n",
    "                        action, logprobs, _, value = trained_villager.get_action_and_value(obs)\n",
    "                        actions[villager] = trained_villager.convert_actions_to_approvals(action)\n",
    "                # wolf steps\n",
    "                actions = actions | wolf_policy(env)\n",
    "            \n",
    "                observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "                mlflow.log_metrics(collect_and_report_stats(env, infos))\n",
    "            winner = env.world_state['winners']\n",
    "            if winner == pare_Role.VILLAGER:\n",
    "                villager_wins += 1\n",
    "\n",
    "            loop.set_description(f\"Villagers won {villager_wins} out of a total of {num_times} games\")\n",
    "\n",
    "env = pare(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "# trained_agent = torch.load(\"long_approval_agent\")\n",
    "test_agent = torch.load(\"pare_no_dead_wolf_reward\")\n",
    "play_static_wolf_game_w_agents(env, random_coordinated_wolf, test_agent, num_times=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
