{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, pare, Phase, Roles\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.learning_agents.models import ActorCriticAgent\n",
    "from notebooks.learning_agents.utils import convert_obs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying villager voting behavior patterns and stats\n",
    "\n",
    "This notebook details the code used to look for certain identifiers in the voting patterns of villagers\n",
    "\n",
    "## Behavioral Indicators and Stats\n",
    "\n",
    "Other than training stats, possible in-game statistics may indicate different behavior and voting patterns amongst the agents\n",
    "\n",
    "We want to \n",
    " - Look at unique votes from villagers throughout accusation and voting. Do these go down?\n",
    "    - Is there a difference between wins/losses?\n",
    "\n",
    " - When are wolves voted out? Earlier or later?\n",
    "    - If so, when is the second wolf found? How many voting rounds after the first wolf killed\n",
    "\n",
    "\n",
    "\n",
    "Here are some things we might care about finding out:\n",
    "- [ ] How many execution votes led to ties? \n",
    "    - Did these favor the wolves or the villagers?\n",
    "- [ ] Villagers who have the same votes as others between accusations (is there accord between villagers)\n",
    "- [ ] What id's do wolves have in games with villager wins\n",
    "- [ ] How many distinct votes do we have? (per accusation round, per voting round)\n",
    "\n",
    "\n",
    "Some stuff might be specific to approval, plurality or ranked voting\n",
    "Approval \n",
    "- [ ] How many villagers vote the exact same way (with approvals and neutrals )\n",
    "- [ ] How many negative votes do villagers give on average. Does this change with training ? *Stat to track while training\n",
    "\n",
    "\n",
    "In terms of gathering up the stats, does it make more sense to look at the history, or does it make sense to just gather them while we train? it might be better to do it post game?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game_w_replays(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    game_replays = []\n",
    "\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, _, _, _, _ = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = convert_obs(observations[villager]['observation'], voting_type=voting_type, one_hot=False)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "                if voting_type == \"plurality\":\n",
    "                    actions[villager] = game_action.item()\n",
    "                elif voting_type == \"approval\":\n",
    "                    actions[villager] = game_action.tolist()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "\n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "\n",
    "            # clear the wolf action if needed\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        game_replays.append(copy.deepcopy(env.history))\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins, game_replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_coordinated_single_wolf(env, agent, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    return action if action != None else int(random.choice(list(villagers_remaining)).split(\"_\")[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "We are going to use a trained agent, an untrained agent, and a random agent.\n",
    "This way we can maybe identify patterns that would seperate each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "observations['player_0']['observation']\n",
    "\n",
    "untrained_agent = ActorCriticAgent({\"rec_hidden_size\": 128, \n",
    "                                        \"rec_layers\": 1,\n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 1,\n",
    "                                        \"approval_states\": 10},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "\n",
    "trained_agent = ActorCriticAgent({\"rec_hidden_size\": 128,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 1,\n",
    "                                        \"approval_states\": 10},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_agent.load_state_dict(torch.load(\"stored_agents/lstm_first_no_one_hot_128_128/plurality_agent_10_score_46\"))\n",
    "\n",
    "# random_agent = None\n",
    "\n",
    "# trained_wins, trained_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, trained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "# untrained_wins, untrained_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, untrained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "# random_wins, random_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, random_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained villagers won 469 games\n",
      "Untrained villagers won 42 games\n"
     ]
    }
   ],
   "source": [
    "trained_villager_wins = [r for r in trained_replays if r[-1][\"winners\"] == Roles.VILLAGER]\n",
    "print(f'Trained villagers won {len(trained_villager_wins)} games')\n",
    "untrained_villager_wins = [r for r in untrained_replays if r[-1][\"winners\"] == Roles.VILLAGER]\n",
    "print(f'Untrained villagers won {len(untrained_villager_wins)} games')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the environment stores history is slightly different than observations. Whereas the latter stores the prior votes, env.history steps have the votes and the outcomes that occured at that particular day/phase/round.\n",
    "This makes analysis slightly easier, but we still need to track a few things\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<Phase.VOTING: 1>: 316}\n"
     ]
    }
   ],
   "source": [
    "# TODO: SANITY CHECK THAT NO VILLAGER WINS HAPPENED OTHER THAN AFTER THE VOTING ROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average amount of days until a win is achieved by villagers\n",
      "\t Trained villagers : 2.886994\n",
      "\t Untrained villagers : 3.190476\n"
     ]
    }
   ],
   "source": [
    "print(\"Average amount of days until a win is achieved by villagers\")\n",
    "print(f'\\t Trained villagers : {np.mean([villager_win[-1][\"day\"] for villager_win in trained_villager_wins]):2f}')\n",
    "print(f'\\t Untrained villagers : {np.mean([villager_win[-1][\"day\"] for villager_win in untrained_villager_wins]):2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to increase the number of players to 15, keeping 2 werewolves, to see if the mean day stretches further"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at unique votes accusation -> voting\n",
    "How many unique votes are there from accusation to voting?\n",
    "If they switch, does a bigger count switch down to a little one? or does a smaller count lose a number to the bigger one?\n",
    "\n",
    "Did a villager switch to a werewolf vote?\n",
    "wolf to wolf?\n",
    "\n",
    "What proportion of votes are for werewolves?\n",
    "\n",
    "How often is the vote against a dead player?\n",
    "\n",
    "- [ ] In situations where we have a wolf who died, do players still vote against them?\n",
    "\n",
    "- [ ] Do we have 3 way ties between both werewolves and the werewolf target?\n",
    "        Is it possible that we actually lose because there is less consensus between agents on who they want to target?\n",
    "\n",
    "do villagers win more given a first wolf vote?\n",
    "is there a vote they key on quicker?\n",
    "\n",
    "On voting rounds:\n",
    "    - [ ] Does a wolf get lucky, and agents split their votes between the wolves, and the wolf target actually gets killed?\n",
    "\n",
    "General game questions:\n",
    "    - Does the first target a wolf pick impact their chance of winning? Alternatively, do villagers key better on certain first wolf votes?\n",
    "    - Do certain wolf id combinations lead to more villager wins? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def when_did_wolves_get_killed(game):\n",
    "    wolves = game[0]['werewolves']\n",
    "\n",
    "    days_wolves_executed = []\n",
    "    just_votes = []\n",
    "    for step in game:\n",
    "        if step[\"phase\"] == Phase.VOTING:\n",
    "            # first eecution\n",
    "            if len(step[\"executed\"]) == 1:\n",
    "                if step['executed'][0] in wolves:\n",
    "                    days_wolves_executed.append(step['day'])\n",
    "            else:\n",
    "                who_was_killed = list(set(step['executed']) - set(just_votes[-1]['executed']))[0]\n",
    "                if who_was_killed  in wolves:\n",
    "                    days_wolves_executed.append(step['day'])\n",
    "\n",
    "            just_votes.append(step)\n",
    "    \n",
    "    if len(days_wolves_executed) < len(wolves):\n",
    "        print(\"Not every wolf was killed!\")\n",
    "    \n",
    "    return days_wolves_executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_villager_wins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNot every wolf was killed!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m days_wolves_executed\n\u001b[0;32m---> 25\u001b[0m wolf_execution_days \u001b[39m=\u001b[39m [when_did_wolves_get_killed(trained_villager_win) \u001b[39mfor\u001b[39;00m trained_villager_win \u001b[39min\u001b[39;00m trained_villager_wins]\n\u001b[1;32m     26\u001b[0m wolf_execution_duration_between \u001b[39m=\u001b[39m [b\u001b[39m-\u001b[39ma \u001b[39mfor\u001b[39;00m a,b \u001b[39min\u001b[39;00m wolf_execution_days]\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDays between wolf kills for trained agents : \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(wolf_execution_duration_between)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_villager_wins' is not defined"
     ]
    }
   ],
   "source": [
    "wolf_execution_days = [when_did_wolves_get_killed(trained_villager_win) for trained_villager_win in trained_villager_wins]\n",
    "wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]\n",
    "print(f'Days between wolf kills for trained agents : {np.mean(wolf_execution_duration_between):.3f}')\n",
    "\n",
    "wolf_execution_days = [when_did_wolves_get_killed(untrained_villager_win) for untrained_villager_win in untrained_villager_wins]\n",
    "wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]\n",
    "print(f'Days between wolf kills for untrained agents : {np.mean(wolf_execution_duration_between):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.45 winning games had a round randomly determined due to a tie\n",
      " 0.04 winning games had a round randomly where a wolf was targetted, but lucked out\n",
      " 0.04 winning games had a round randomly where more than one wolf was targetted, but lucked out\n",
      " 0.40 losing games had a round randomly determined due to a tie\n",
      " 0.04 losing games had a round randomly where a wolf was targetted, but lucked out\n",
      " 0.04 losing games had a round randomly where more than one wolf was targetted, but lucked out\n"
     ]
    }
   ],
   "source": [
    "def tie_game_info(game):\n",
    "\twolves = game[0]['werewolves']\n",
    "\n",
    "\tjust_votes = []\n",
    "\ttie_days = []\n",
    "\n",
    "\t# wolf won the tie flip\n",
    "\tlucky_wolf_day = []\n",
    "\n",
    "\t# wolf won the tie flip, with multiple wolfs being tied targets\n",
    "\tsuper_lucky_wolf_day = []\n",
    "\n",
    "\tfor step in game:\n",
    "\t\tvillager_votes = [vote for player, vote in step['votes'].items() if player not in wolves]\n",
    "\t\twolf_votes = [vote for player, vote in step['votes'].items() if player in wolves]\n",
    "\t\tall_votes = list(step['votes'].values())\n",
    "\n",
    "\t\tvillager_vote_counter = Counter(villager_votes)\n",
    "\t\tall_vote_counter = Counter(all_votes)\n",
    "\n",
    "\t\tif step[\"phase\"] == Phase.VOTING:\n",
    "\t\t\tjust_votes.append(step)\n",
    "\n",
    "    \t\t# was the vote a tie? did it lead to \n",
    "\t\t\tmax_votes_on_target = max(all_vote_counter.values())\n",
    "\t\t\ttargets = [k for k in all_vote_counter if all_vote_counter[k] == max_votes_on_target]\n",
    "\n",
    "\t\t\t# we have a tie\n",
    "\t\t\tif len(targets) > 1:\n",
    "\t\t\t\ttie_days.append(step[\"day\"])\n",
    "\t\t\t\t# are one of the targets a wolf target?\n",
    "\t\t\t\tis_a_target_a_wolf_target = sum([target in wolf_votes for target in targets])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# is the tie between a dead player and a live player?\n",
    "\t\t\t\t# this won't  trigger a tie trigger though\n",
    "\t\t\t\tif len(step['executed']) == 1:\n",
    "\t\t\t\t\tdead_players = list(set(step['executed']) | set(step['killed']))\n",
    "\t\t\t\t\tkilled_this_turn = step['executed'][0]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdead_players =  list((set(step['executed']) & set(just_votes[-2]['executed'])) | set(step['killed']))\n",
    "\t\t\t\t\tkilled_this_turn = list(set(step['executed']) - set(just_votes[-2]['executed']))[0]\n",
    "\n",
    "\t\t\t\t# is the tie only between dead players?\n",
    "\t\t\t\tis_a_target_a_living_wolf = sum([f'player_{target}' in wolves for target in targets if target not in dead_players])\n",
    "\n",
    "\t\t\t\t# so now, we want to know if we have at tie between a wolf target and a living wolf\n",
    "\t\t\t\tif is_a_target_a_living_wolf and is_a_target_a_wolf_target:\n",
    "\t\t\t\t\tif killed_this_turn not in wolves:\n",
    "\t\t\t\t\t\t# wolves got lucky\n",
    "\t\t\t\t\t\tlucky_wolf_day.append(step[\"day\"])\n",
    "\t\t\t\t\tif is_a_target_a_living_wolf > 1:\n",
    "\t\t\t\t\t\tsuper_lucky_wolf_day.append(step[\"day\"])\n",
    "\n",
    "\treturn tie_days, lucky_wolf_day, super_lucky_wolf_day\n",
    "\n",
    "tie_game_stats = [tie_game_info(trained_villager_win) for trained_villager_win in trained_villager_wins]\n",
    "tie_games = [tie_game for tie_game in tie_game_stats if len(tie_game[0]) >= 1]\n",
    "wolf_ties = [tie_game for tie_game in tie_game_stats if len(tie_game[1]) >= 1]\n",
    "super_lucky_wolf_ties = [tie_game for tie_game in tie_game_stats if len(tie_game[2]) >= 1]\n",
    "print(f' {len(tie_games)/len(trained_villager_wins):.2f} winning games had a round randomly determined due to a tie')\n",
    "print(f' {len(wolf_ties)/len(trained_villager_wins):.2f} winning games had a round randomly where a wolf was targetted, but lucked out')\n",
    "print(f' {len(super_lucky_wolf_ties)/len(trained_villager_wins):.2f} winning games had a round randomly where more than one wolf was targetted, but lucked out')\n",
    "\n",
    "trained_villager_losses = [r for r in trained_replays if r[-1][\"winners\"] == Roles.WEREWOLF]\n",
    "tie_games_stats = [tie_game_info(trained_villager_loss) for trained_villager_loss in trained_villager_losses]\n",
    "tie_games = [tie_game for tie_game in tie_game_stats if len(tie_game[0]) >= 1]\n",
    "wolf_ties = [tie_game for tie_game in tie_game_stats if len(tie_game[1]) >= 1]\n",
    "super_lucky_wolf_ties = [tie_game for tie_game in tie_game_stats if len(tie_game[2]) >= 1]\n",
    "print(f' {len(tie_games)/len(trained_villager_losses):.2f} losing games had a round randomly determined due to a tie')\n",
    "print(f' {len(wolf_ties)/len(trained_villager_wins):.2f} losing games had a round randomly where a wolf was targetted, but lucked out')\n",
    "print(f' {len(super_lucky_wolf_ties)/len(trained_villager_wins):.2f} losing games had a round randomly where more than one wolf was targetted, but lucked out')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators_per_game(game_replay):\n",
    "    wolves = game_replay[0]['werewolves']\n",
    "\n",
    "    just_votes = []\n",
    "    for step in trained_villager_wins[0]:\n",
    "        if step[\"phase\"] == Phase.NIGHT:\n",
    "            continue\n",
    "        if step[\"phase\"] == Phase.VOTING:\n",
    "            just_votes.append(step)\n",
    "\n",
    "        villager_votes = [vote for player, vote in step['votes'].items() if player not in wolves]\n",
    "        all_votes = list(step['votes'].values())\n",
    "\n",
    "        villager_vote_counter = Counter(villager_votes)\n",
    "        all_vote_counter = Counter(all_votes)\n",
    "\n",
    "        unique_villager_votes = len(villager_vote_counter)\n",
    "        percent_of_villagers_targetting_wolves = sum([villager_vote_counter[int(wolf.split(\"_\")[-1])] for wolf in wolves]) / float(len(villager_votes))\n",
    "\n",
    "        # want to be careful, because on voting rounds, there is also the executed player in this set. so we need to remove them first\n",
    "        if step[\"phase\"] == Phase.VOTING:\n",
    "            just_votes.append(step)\n",
    "            \n",
    "            if len(just_votes) == 1:\n",
    "                percent_of_villagers_targetting_dead_players = 0 / float(len(villager_votes))\n",
    "            else:\n",
    "                percent_of_villagers_targetting_dead_players = sum([villager_vote_counter[dead_player] for dead_player in list((set(step['executed']) & set(just_votes[-2]['executed'])) | set(step['killed']))]) / float(len(villager_votes))\n",
    "        else:\n",
    "            percent_of_villagers_targetting_dead_players = sum([villager_vote_counter[dead_player] for dead_player in list(set(step['executed']) | set(step['killed']))]) / float(len(villager_votes))\n",
    "\n",
    "    # per round\n",
    "    \n",
    "\n",
    "    \n",
    "    # print(f'Wolves : {wolves}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolves : ['player_7', 'player_9']\n",
      "\n",
      "Day : 1 | Phase : 0 | Round : 0\n",
      "Villager votes : [10, 10, 10, 10, 10, 10, 10, 10]\n",
      "\t | - 1 players targetted, with 0.000 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 1 | Phase : 0 | Round : 0\n",
      "Villager votes : [2, 8, 0, 9, 8, 9, 9, 6]\n",
      "\t | - 5 players targetted, with 0.375 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 1 | Phase : 0 | Round : 1\n",
      "Villager votes : [8, 5, 5, 5, 8, 5, 7, 8]\n",
      "\t | - 3 players targetted, with 0.125 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 1 | Phase : 1 | Round : 0\n",
      "Villager votes : [5, 7, 5, 5, 8, 8, 7, 6]\n",
      "\t | - 4 players targetted, with 0.250 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 2 | Phase : 0 | Round : 0\n",
      "Villager votes : [2, 9, 9, 9, 7, 8]\n",
      "\t | - 4 players targetted, with 0.667 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 2 | Phase : 0 | Round : 1\n",
      "Villager votes : [7, 7, 7, 6, 9, 6]\n",
      "\t | - 3 players targetted, with 0.667 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 2 | Phase : 1 | Round : 0\n",
      "Villager votes : [7, 7, 7, 7, 7, 4]\n",
      "\t | - 2 players targetted, with 0.833 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 3 | Phase : 0 | Round : 0\n",
      "Villager votes : [7, 6, 9, 9, 2]\n",
      "\t | - 4 players targetted, with 0.600 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 3 | Phase : 0 | Round : 1\n",
      "Villager votes : [9, 9, 9, 9, 6]\n",
      "\t | - 2 players targetted, with 0.800 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n",
      "Day : 3 | Phase : 1 | Round : 0\n",
      "Villager votes : [9, 9, 9, 9, 6]\n",
      "\t | - 2 players targetted, with 0.800 of the votes targetting wolves\n",
      "\t | - 0.000 share of the votes targetting dead players\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wolves = trained_villager_wins[0][0]['werewolves']\n",
    "print(f'Wolves : {wolves}\\n')\n",
    "\n",
    "just_votes = []\n",
    "for step in trained_villager_wins[0]:\n",
    "    if step[\"phase\"] == Phase.NIGHT:\n",
    "        continue\n",
    "    if step[\"phase\"] == Phase.VOTING:\n",
    "        just_votes.append(step)\n",
    "\n",
    "    villager_votes = [vote for player, vote in step['votes'].items() if player not in wolves]\n",
    "    all_votes = list(step['votes'].values())\n",
    "\n",
    "    villager_vote_counter = Counter(villager_votes)\n",
    "    all_vote_counter = Counter(all_votes)\n",
    "\n",
    "    unique_villager_votes = len(villager_vote_counter)\n",
    "    percent_of_villagers_targetting_wolves = sum([villager_vote_counter[int(wolf.split(\"_\")[-1])] for wolf in wolves]) / float(len(villager_votes))\n",
    "\n",
    "    # want to be careful, because on voting rounds, there is also the executed player in this set. so we need to remove them first\n",
    "    if step[\"phase\"] == Phase.VOTING:\n",
    "        just_votes.append(step)\n",
    "        \n",
    "        if len(just_votes) == 1:\n",
    "            percent_of_villagers_targetting_dead_players = 0 / float(len(villager_votes))\n",
    "        else:\n",
    "            percent_of_villagers_targetting_dead_players = sum([villager_vote_counter[dead_player] for dead_player in list((set(step['executed']) & set(just_votes[-2]['executed'])) | set(step['killed']))]) / float(len(villager_votes))\n",
    "    else:\n",
    "        percent_of_villagers_targetting_dead_players = sum([villager_vote_counter[dead_player] for dead_player in list(set(step['executed']) | set(step['killed']))]) / float(len(villager_votes))\n",
    "\n",
    "    # percent_of_villagers_targetting_a_dead_wolf = None\n",
    "    \n",
    "    print(f'Day : {step[\"day\"]} | Phase : {step[\"phase\"]} | Round : {step[\"round\"]}')\n",
    "    print(f'Villager votes : {villager_votes}')\n",
    "    print(f'\\t | - {unique_villager_votes} players targetted, with {percent_of_villagers_targetting_wolves:.3f} of the votes targetting wolves')\n",
    "    print(f'\\t | - {percent_of_villagers_targetting_dead_players:.3f} share of the votes targetting dead players\\n')\n",
    "\n",
    "\n",
    "    #print(f'Villager counters : {Counter(villager_votes)}')\n",
    "\n",
    "    #print(f'All votes : {all_votes}')\n",
    "    #print(f'All votes counters : {Counter(all_votes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try doing this to approval voting games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_wolf(env, agent, action=None):\n",
    "    if action != None:\n",
    "        return action\n",
    "\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    wolves_remaining = set(env.world_state[\"werewolves\"]) & set(env.world_state['alive'])\n",
    "\n",
    "    # pick a living target\n",
    "    target = random.choice(list(villagers_remaining))\n",
    "\n",
    "    action = [0] * len(env.possible_agents)\n",
    "    action[int(target.split(\"_\")[-1])] = -1\n",
    "    for curr_wolf in wolves_remaining:\n",
    "        action[int(curr_wolf.split(\"_\")[-1])] = 1\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pare(num_agents=10, werewolves=2, num_accusations=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "observations['player_0']['observation']\n",
    "\n",
    "approval_untrained_agent = ActorCriticAgent({\"rec_hidden_size\": 256, \n",
    "                                        \"rec_layers\": 1,\n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 10,\n",
    "                                        \"approval_states\": 3},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "\n",
    "approval_trained_agent = ActorCriticAgent({\"rec_hidden_size\": 256,\n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"joint_mlp_size\": 128,\n",
    "                                        \"split_mlp_size\": 128,\n",
    "                                        \"num_votes\": 10,\n",
    "                                        \"approval_states\": 3},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "approval_trained_agent.load_state_dict(torch.load(\"stored_agents/lstm_first_no_one_hot_256_128/approval_agent_10_score_49\"))\n",
    "\n",
    "# random_agent = None\n",
    "\n",
    "approval_trained_wins, approval_trained_replays = play_recurrent_game_w_replays(env, random_wolf, approval_trained_agent, num_times=1000, hidden_state_size=256, voting_type=\"approval\")\n",
    "approval_untrained_wins, approval_untrained_replays = play_recurrent_game_w_replays(env, random_wolf, approval_untrained_agent, num_times=1000, hidden_state_size=256, voting_type=\"approval\")\n",
    "# random_wins, random_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, random_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained villagers won 477 games\n",
      "Untrained villagers won 75 games\n"
     ]
    }
   ],
   "source": [
    "approval_trained_villager_wins = [r for r in approval_trained_replays if r[-1][\"winners\"] == Roles.VILLAGER]\n",
    "print(f'Trained villagers won {len(approval_trained_villager_wins)} games')\n",
    "approval_untrained_villager_wins = [r for r in approval_untrained_replays if r[-1][\"winners\"] == Roles.VILLAGER]\n",
    "print(f'Untrained villagers won {len(approval_untrained_villager_wins)} games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days between wolf kills for trained agents : 1.465\n",
      "Days between wolf kills for untrained agents : 1.733\n"
     ]
    }
   ],
   "source": [
    "wolf_execution_days = [when_did_wolves_get_killed(trained_villager_win) for trained_villager_win in approval_trained_villager_wins]\n",
    "wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]\n",
    "print(f'Days between wolf kills for trained agents : {np.mean(wolf_execution_duration_between):.3f}')\n",
    "\n",
    "wolf_execution_days = [when_did_wolves_get_killed(untrained_villager_win) for untrained_villager_win in approval_untrained_villager_wins]\n",
    "wolf_execution_duration_between = [b-a for a,b in wolf_execution_days]\n",
    "print(f'Days between wolf kills for untrained agents : {np.mean(wolf_execution_duration_between):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day : 1 | Phase : 0 | Round : 0\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 0.00 others, liked 0.00 others, felt neutral for 10.00 others\n",
      "0 wolves targetted in top 0 votes\n",
      "0 wolves liked in top 0 likes\n",
      "\n",
      "\n",
      "Day : 1 | Phase : 0 | Round : 0\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 2.88 others, liked 3.75 others, felt neutral for 3.38 others\n",
      "2 wolves targetted in top 5 votes\n",
      "0 wolves liked in top 5 likes\n",
      "\n",
      "\n",
      "Day : 1 | Phase : 0 | Round : 1\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 3.12 others, liked 4.62 others, felt neutral for 2.25 others\n",
      "2 wolves targetted in top 4 votes\n",
      "1 wolves liked in top 5 likes\n",
      "\n",
      "\n",
      "Day : 1 | Phase : 1 | Round : 0\n",
      "\t Voting Phase\n",
      "Villagers on average targetted 3.62 others, liked 3.75 others, felt neutral for 2.62 others\n",
      "1 wolves targetted in top 4 votes\n",
      "1 wolves liked in top 4 likes\n",
      "\n",
      "\n",
      "Day : 2 | Phase : 0 | Round : 0\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 4.14 others, liked 3.00 others, felt neutral for 2.86 others\n",
      "2 wolves targetted in top 5 votes\n",
      "1 wolves liked in top 4 likes\n",
      "\n",
      "\n",
      "Day : 2 | Phase : 0 | Round : 1\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 4.00 others, liked 3.57 others, felt neutral for 2.43 others\n",
      "1 wolves targetted in top 5 votes\n",
      "1 wolves liked in top 5 likes\n",
      "\n",
      "\n",
      "Day : 2 | Phase : 1 | Round : 0\n",
      "\t Voting Phase\n",
      "Villagers on average targetted 2.43 others, liked 4.14 others, felt neutral for 3.43 others\n",
      "1 wolves targetted in top 5 votes\n",
      "2 wolves liked in top 5 likes\n",
      "\n",
      "\n",
      "Day : 3 | Phase : 0 | Round : 0\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 3.20 others, liked 3.40 others, felt neutral for 3.40 others\n",
      "1 wolves targetted in top 4 votes\n",
      "1 wolves liked in top 4 likes\n",
      "\n",
      "\n",
      "Day : 3 | Phase : 0 | Round : 1\n",
      "\t Accusation Phase\n",
      "Villagers on average targetted 3.40 others, liked 3.40 others, felt neutral for 3.20 others\n",
      "2 wolves targetted in top 5 votes\n",
      "0 wolves liked in top 5 likes\n",
      "\n",
      "\n",
      "Day : 3 | Phase : 1 | Round : 0\n",
      "\t Voting Phase\n",
      "Villagers on average targetted 3.60 others, liked 2.80 others, felt neutral for 3.60 others\n",
      "2 wolves targetted in top 3 votes\n",
      "0 wolves liked in top 3 likes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some speficic approval stuff?\n",
    "\n",
    "\n",
    "def votes_info(game):\n",
    "    wolves = game[0]['werewolves']\n",
    "\n",
    "    for step in game:\n",
    "        if step['phase'] == Phase.NIGHT:\n",
    "            continue\n",
    "\n",
    "        print(f'Day : {step[\"day\"]} | Phase : {step[\"phase\"]} | Round : {step[\"round\"]}')\n",
    "        if step[\"phase\"] == Phase.VOTING:\n",
    "            print(\"\\t Voting Phase\")\n",
    "        else:\n",
    "            print(\"\\t Accusation Phase\")\n",
    "\n",
    "        villager_votes = [vote for player, vote in step['votes'].items() if player not in wolves]\n",
    "        all_votes = list(step['votes'].values())\n",
    "\n",
    "        villager_targets = [np.where(np.array(villager_vote) == -1)[0] for villager_vote in villager_votes]\n",
    "        villager_likes = [np.where(np.array(villager_vote) == 1)[0] for villager_vote in villager_votes]\n",
    "        villager_neutrals = [np.where(np.array(villager_vote) == 0)[0] for villager_vote in villager_votes]\n",
    "\n",
    "        v_target_counter = Counter(np.concatenate(villager_targets))\n",
    "        v_like_counter = Counter(np.concatenate(villager_likes))\n",
    "        v_neutral_counter = Counter(np.concatenate(villager_neutrals))\n",
    "\n",
    "        print(f'Villagers on average targetted {np.mean([len(targets) for targets in villager_targets]):.2f} others, liked {np.mean([len(targets) for targets in villager_likes]):.2f} others, felt neutral for {np.mean([len(targets) for targets in villager_neutrals]):.2f} others')\n",
    "        # is one of the targets dead? \n",
    "        # how many top targets are wolves?\n",
    "        wolves_in_most_common_targets =\\\n",
    "            [int(wolf.split(\"_\")[-1]) for wolf in wolves if int(wolf.split(\"_\")[-1]) in [idx for idx, _ in v_target_counter.most_common(max(1,int(len(v_target_counter)*0.5)))]]\n",
    "\n",
    "        # TODO: if only one of the 2 wolves are top targets, I wonder if they just get lucky for the second wolf kill\n",
    "        # TODO: do we want to focus on num voting rounds in this equation?\n",
    "        print(f'{len(wolves_in_most_common_targets)} wolves targetted in top {int(len(v_target_counter)*0.5)} votes')\n",
    "\n",
    "        # TODO: do we want to focus on num voting rounds in this equation?\n",
    "\n",
    "        wolves_in_most_common_likes =\\\n",
    "            [int(wolf.split(\"_\")[-1]) for wolf in wolves if int(wolf.split(\"_\")[-1]) in [idx for idx, _ in v_like_counter.most_common(max(1,int(len(v_like_counter)*0.5)))]]\n",
    "        print(f'{len(wolves_in_most_common_likes)} wolves liked in top {int(len(v_like_counter)*0.5)} likes')\n",
    "        # do the most liked individuals also get the least amount of votes?\n",
    "\n",
    "        # how many likes are for other trusted villagers?\n",
    "        # how many likes are towards wolves?\n",
    "        # We are hoping that a like is used as a communicative tool\n",
    "\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "votes_info(approval_trained_villager_wins[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we give a penalty to players that target dead wolves? More than just targetting dead players in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(*Counter([item for sublist in [villager_win[-1][\"werewolves\"] for villager_win in approval_trained_villager_wins] for item in sublist]).items())\n",
    "indexes = np.arange(len(labels))\n",
    "width = 10\n",
    "\n",
    "# plt.bar(indexes, values, width)\n",
    "# plt.xticks(indexes + width * 0.5, labels)\n",
    "# plt.tight_layout()\n",
    "# plt.xticks(rotation=60)\n",
    "# plt.show()npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up trained agent\n",
    "# load up fresh agent for comparisons\n",
    "\n",
    "# play recurrent game_w_replays with both agents\n",
    "\n",
    "# try to answer the questions below\n",
    "\n",
    "\n",
    "# play_recurrent_game_w_replays()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_report_stats(env, information, ignore_wolf=True, mlflow_uri=None):\n",
    "    total_self_votes = len([vals for player, vals in information.items() if vals[\"self_vote\"] and (ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF)])\n",
    "    total_dead_votes = sum([vals[\"dead_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF])\n",
    "    total_viable_votes = sum([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF])\n",
    "\n",
    "    avg_self_votes = total_self_votes/len(information)\n",
    "    avg_dead_votes = total_dead_votes/len(information)\n",
    "    avg_viable_votes = total_viable_votes/len(information)\n",
    "\n",
    "    return {\n",
    "        \"total_self_votes\": total_self_votes,\n",
    "        \"total_dead_votes\": total_dead_votes,\n",
    "        \"total_viable_votes\": total_viable_votes,\n",
    "        \"avg_self_votes\": avg_self_votes,\n",
    "        \"avg_dead_votes\": avg_dead_votes,\n",
    "        \"avg_viable_votes\": avg_viable_votes,\n",
    "        \"players_with_viable_votes\": len([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay visualization\n",
    "\n",
    "Need good ways to visualize the gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_replay(replay):\n",
    "    #[wolf for wolf in stage[\"werewolves\"]]\n",
    "    #print(\"Werewolves \\tVillagers\")\n",
    "    for stage in replay:\n",
    "        wolf_votes = [(f'p_{wolf.split(\"_\")[-1]}', stage[\"votes\"][wolf]) for wolf in stage[\"werewolves\"] if wolf in stage[\"votes\"]]\n",
    "        villager_votes = [(f'p{villager.split(\"_\")[-1]}', stage[\"votes\"][villager]) for villager in stage[\"villagers\"] if villager in stage[\"votes\"]]\n",
    "        print(f'Wolves \\t : {wolf_votes} \\t\\t Villagers : {villager_votes}')\n",
    "        #print([f'{wolf.split(\"_\")[-1]} : {stage[\"votes\"][\"player\"_{wolf.split(\"_\")[-1]}]}]' for wolf in stage[\"werewolves\"]])\n",
    "        # for wolf in stage['werewolves']:\n",
    "        #     wid = wolf.split(\"_\")[-1]\n",
    "        #     pid = f\"player_{wid}\"\n",
    "        #     stage[\"votes\"][pid]\n",
    "        #     print(f'p_{wid} : {stage[\"votes\"][pid]}')\n",
    "        # print(stage['votes'])\n",
    "\n",
    "print_replay(replay[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the gameplay process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "    \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "                if voting_type == \"plurality\":\n",
    "                    actions[villager] = game_action.item()\n",
    "                elif voting_type == \"approval\":\n",
    "                    actions[villager] = game_action.tolist()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "            \n",
    "            # clear the wolf action if needed\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_faster_game(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "    \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            v_obs = torch.cat([torch.unsqueeze(torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float), 0) for villager in villagers])\n",
    "            \n",
    "            # TODO: maybe this can be sped up? \n",
    "            hxs, cxs = zip(*[(hxs, cxs) for hxs, cxs in [magent_obs[villager][\"hcxs\"][-1] for villager in villagers]])\n",
    "            hxs = torch.swapaxes(torch.cat(hxs),0,1)\n",
    "            cxs = torch.swapaxes(torch.cat(cxs),0,1)\n",
    "\n",
    "            # TODO : make this policies to allow for approval speed up too\n",
    "            policy, _ , cells = villager_agent(v_obs, (hxs, cxs))\n",
    "\n",
    "            v_actions = policy[0].sample().tolist()\n",
    "            hxs_new, cxs_new = cells\n",
    "            hxs_new = torch.swapaxes(hxs_new,1,0)\n",
    "            cxs_new = torch.swapaxes(cxs_new,1,0)\n",
    "\n",
    "            for i, villager in enumerate(villagers):\n",
    "                actions[villager] = v_actions[i]\n",
    "                magent_obs[villager]['hcxs'].append((torch.unsqueeze(hxs_new[i], 0), torch.unsqueeze(cxs_new[i], 0)))\n",
    "\n",
    "            # # batch, sequence, input\n",
    "            # for villager in villagers:\n",
    "            #     #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "            #     torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "            #     obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "            #     # TODO: Testing this, we may need a better way to pass in villagers\n",
    "            #     recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "            #     # ensure that the obs is of size (batch,seq,inputs)\n",
    "            #     policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "            #     _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "            #     if voting_type == \"plurality\":\n",
    "            #         actions[villager] = game_action.item()\n",
    "            #     elif voting_type == \"approval\":\n",
    "            #         actions[villager] = game_action.tolist()\n",
    "\n",
    "            #     #store the next recurrent cells\n",
    "            #     magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "            \n",
    "            # clear the wolf action if needed\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_faster_game_on_gpu(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    \n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, rewards, terminations, truncations, infos = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32, device=torch.device(\"cuda:0\")), torch.zeros((1,1,hidden_state_size), dtype=torch.float32, device=torch.device(\"cuda:0\")))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "    \n",
    "        wolf_action = None\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            v_obs = torch.cat([torch.unsqueeze(torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float, device=torch.device(\"cuda:0\")), 0) for villager in villagers])\n",
    "            \n",
    "            # TODO: maybe this can be sped up? \n",
    "            hxs, cxs = zip(*[(hxs, cxs) for hxs, cxs in [magent_obs[villager][\"hcxs\"][-1] for villager in villagers]])\n",
    "            hxs = torch.swapaxes(torch.cat(hxs),0,1)\n",
    "            cxs = torch.swapaxes(torch.cat(cxs),0,1)\n",
    "\n",
    "            # TODO : make this policies to allow for approval speed up too\n",
    "            policy, _ , cells = villager_agent(v_obs, (hxs, cxs))\n",
    "\n",
    "            v_actions = policy[0].sample().tolist()\n",
    "            hxs_new, cxs_new = cells\n",
    "            hxs_new = torch.swapaxes(hxs_new,1,0)\n",
    "            cxs_new = torch.swapaxes(cxs_new,1,0)\n",
    "\n",
    "            for i, villager in enumerate(villagers):\n",
    "                actions[villager] = v_actions[i]\n",
    "                magent_obs[villager]['hcxs'].append((torch.unsqueeze(hxs_new[i], 0), torch.unsqueeze(cxs_new[i], 0)))\n",
    "\n",
    "            # # batch, sequence, input\n",
    "            # for villager in villagers:\n",
    "            #     #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "            #     torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "            #     obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "            #     # TODO: Testing this, we may need a better way to pass in villagers\n",
    "            #     recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "            #     # ensure that the obs is of size (batch,seq,inputs)\n",
    "            #     policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "            #     _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "            #     if voting_type == \"plurality\":\n",
    "            #         actions[villager] = game_action.item()\n",
    "            #     elif voting_type == \"approval\":\n",
    "            #         actions[villager] = game_action.tolist()\n",
    "\n",
    "            #     #store the next recurrent cells\n",
    "            #     magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            phase = env.world_state['phase']\n",
    "            for wolf in wolves:\n",
    "                wolf_action = wolf_policy(env, wolf, action=wolf_action)\n",
    "                actions[wolf] = wolf_action\n",
    "        \n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "            \n",
    "            # clear the wolf action if needed\n",
    "            if env.world_state['phase'] == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "            \n",
    "            if env.world_state['phase'] == Phase.ACCUSATION and phase == Phase.NIGHT:\n",
    "                wolf_action = None\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hopefully faster function time : 32.235\n",
      "Regular function time : 74.958\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wins = play_recurrent_faster_game(env, random_coordinated_single_wolf, trained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "end = time.time()\n",
    "print(f'Wins : {wins}, hopefully faster function time : {end-start:.3f}')\n",
    "\n",
    "# start = time.time()\n",
    "# play_recurrent_faster_game_on_gpu(env, random_coordinated_single_wolf, trained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "# end = time.time()\n",
    "# print(f'hopefully faster function time on GPU : {end-start:.3f}')\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "wins = play_recurrent_game(env, random_coordinated_single_wolf, trained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "end = time.time()\n",
    "print(f'Wins: {wins}, Regular function time : {end-start:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
