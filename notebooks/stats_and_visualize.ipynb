{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import plurality_env, Phase, Roles\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from learning_agents.actor_critic_model import ActorCriticAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_recurrent_game_w_replays(env, wolf_policy, villager_agent, num_times=10, hidden_state_size=None, voting_type=None):\n",
    "    wins = 0\n",
    "    # loop = tqdm(range(num_times))\n",
    "    game_replays = []\n",
    "\n",
    "    for _ in range(num_times):\n",
    "        ## Play the game \n",
    "        next_observations, _, _, _, _ = env.reset()\n",
    "        # init recurrent stuff for actor and critic to 0 as well\n",
    "        magent_obs = {agent: {'obs': [], \n",
    "                              # obs size, and 1,1,64 as we pass batch first\n",
    "                              'hcxs': [(torch.zeros((1,1,hidden_state_size), dtype=torch.float32), torch.zeros((1,1,hidden_state_size), dtype=torch.float32))],\n",
    "                    } for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "\n",
    "        while env.agents:\n",
    "            observations = copy.deepcopy(next_observations)\n",
    "            actions = {}\n",
    "\n",
    "            villagers = set(env.agents) & set(env.world_state[\"villagers\"])\n",
    "            wolves = set(env.agents) & set(env.world_state[\"werewolves\"])\n",
    "\n",
    "            # villagers actions\n",
    "            for villager in villagers:\n",
    "                #torch.tensor(env.convert_obs(observations['player_0']['observation']), dtype=torch.float)\n",
    "                torch_obs = torch.tensor(env.convert_obs(observations[villager]['observation']), dtype=torch.float)\n",
    "                obs = torch.unsqueeze(torch_obs, 0)\n",
    "\n",
    "                # TODO: Testing this, we may need a better way to pass in villagers\n",
    "                recurrent_cell = magent_obs[villager][\"hcxs\"][-1]\n",
    "                \n",
    "                # ensure that the obs is of size (batch,seq,inputs)\n",
    "                policies, _, recurrent_cell = villager_agent(obs, recurrent_cell)\n",
    "                _, game_action = villager_agent.get_action_from_policies(policies, voting_type=voting_type)\n",
    "\n",
    "                if voting_type == \"plurality\":\n",
    "                    actions[villager] = game_action.item()\n",
    "                elif voting_type == \"approval\":\n",
    "                    actions[villager] = game_action.tolist()\n",
    "\n",
    "                #store the next recurrent cells\n",
    "                magent_obs[villager][\"hcxs\"].append(recurrent_cell)\n",
    "\n",
    "            # wolf steps\n",
    "            day = observations[list(observations)[0]]['observation']['day']\n",
    "            phase = observations[list(observations)[0]]['observation']['phase']\n",
    "            \n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] == Phase.NIGHT:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "            \n",
    "            for wolf in wolves:\n",
    "                action = wolf_policy(env, wolf, action=wolf_brain['action'])\n",
    "                wolf_brain['action'] = action\n",
    "                actions[wolf] = action\n",
    "\n",
    "            next_observations, _, _, _, _ = env.step(actions)\n",
    "\n",
    "        ## Fill bigger buffer, keeping in mind sequence\n",
    "        winner = env.world_state['winners']\n",
    "        if winner == Roles.VILLAGER:\n",
    "            wins += 1\n",
    "\n",
    "        game_replays.append(copy.deepcopy(env.history))\n",
    "\n",
    "        # loop.set_description(f\"Villagers won {wins} out of a total of {num_times} games\")\n",
    "    \n",
    "    return wins, game_replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_coordinated_single_wolf(env, agent, action=None):\n",
    "    villagers_remaining = set(env.world_state[\"villagers\"]) & set(env.world_state['alive'])\n",
    "    return action if action != None else int(random.choice(list(villagers_remaining)).split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "env = plurality_env(num_agents=10, werewolves=2)\n",
    "observations, _, _, _, _ = env.reset()\n",
    "\n",
    "obs_size= env.convert_obs(observations['player_0']['observation']).shape[-1]\n",
    "print(obs_size)\n",
    "observations['player_0']['observation']\n",
    "\n",
    "untrained_agent = ActorCriticAgent({\"rec_hidden_size\": 128, \n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"hidden_mlp_size\": 128,\n",
    "                                        \"num_votes\": 1,\n",
    "                                        \"approval_states\": 10},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "\n",
    "trained_agent = ActorCriticAgent({\"rec_hidden_size\": 128, \n",
    "                                        \"rec_layers\": 1, \n",
    "                                        \"hidden_mlp_size\": 128,\n",
    "                                        \"num_votes\": 1,\n",
    "                                        \"approval_states\": 10},\n",
    "                                        num_players=10,\n",
    "                                        obs_size=obs_size)\n",
    "trained_agent.load_state_dict(torch.load(\"stored_agents/plurality_agent_10_score_50\"))\n",
    "\n",
    "random_agent = None\n",
    "\n",
    "trained_wins, trained_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, trained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n",
    "untrained_wins, untrained_replays = play_recurrent_game_w_replays(env, random_coordinated_single_wolf, untrained_agent, num_times=1000, hidden_state_size=128, voting_type=\"plurality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trained_wins)\n",
    "print(untrained_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up trained agent\n",
    "# load up fresh agent for comparisons\n",
    "\n",
    "# play recurrent game_w_replays with both agents\n",
    "\n",
    "# try to answer the questions below\n",
    "\n",
    "\n",
    "# play_recurrent_game_w_replays()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats\n",
    "\n",
    "Other than training stats, possible in-game statistics may indicate different behavior and voting patterns amongst the agents\n",
    "\n",
    "Here are some things we might care about finding out:\n",
    "- [ ] How many execution votes led to ties? \n",
    "    - Did these favor the wolves or the villagers?\n",
    "- [ ] Villagers who have the same votes as others between accusations (is there accord between villagers)\n",
    "- [ ] What id's do wolves have in games with villager wins\n",
    "\n",
    "\n",
    "Some stuff might be specific to approval, plurality or ranked voting\n",
    "Approval \n",
    "- [ ] How many villagers vote the exact same way (with approvals and neutrals )\n",
    "- [ ] How many negative votes do villagers give on average. Does this change with training ? *Stat to track while training\n",
    "\n",
    "\n",
    "In terms of gathering up the stats, does it make more sense to look at the history, or does it make sense to just gather them while we train? it might be better to do it post game?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_report_stats(env, information, ignore_wolf=True, mlflow_uri=None):\n",
    "    total_self_votes = len([vals for player, vals in information.items() if vals[\"self_vote\"] and (ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF)])\n",
    "    total_dead_votes = sum([vals[\"dead_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF])\n",
    "    total_viable_votes = sum([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != Roles.WEREWOLF])\n",
    "\n",
    "    avg_self_votes = total_self_votes/len(information)\n",
    "    avg_dead_votes = total_dead_votes/len(information)\n",
    "    avg_viable_votes = total_viable_votes/len(information)\n",
    "\n",
    "    return {\n",
    "        \"total_self_votes\": total_self_votes,\n",
    "        \"total_dead_votes\": total_dead_votes,\n",
    "        \"total_viable_votes\": total_viable_votes,\n",
    "        \"avg_self_votes\": avg_self_votes,\n",
    "        \"avg_dead_votes\": avg_dead_votes,\n",
    "        \"avg_viable_votes\": avg_viable_votes,\n",
    "        \"players_with_viable_votes\": len([vals[\"viable_vote\"] for player, vals in information.items() if ignore_wolf and env.agent_roles[player] != pare_Role.WEREWOLF])\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay visualization\n",
    "\n",
    "Need good ways to visualize the gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_replay(replay):\n",
    "    #[wolf for wolf in stage[\"werewolves\"]]\n",
    "    #print(\"Werewolves \\tVillagers\")\n",
    "    for stage in replay:\n",
    "        wolf_votes = [(f'p_{wolf.split(\"_\")[-1]}', stage[\"votes\"][wolf]) for wolf in stage[\"werewolves\"] if wolf in stage[\"votes\"]]\n",
    "        villager_votes = [(f'p{villager.split(\"_\")[-1]}', stage[\"votes\"][villager]) for villager in stage[\"villagers\"] if villager in stage[\"votes\"]]\n",
    "        print(f'Wolves \\t : {wolf_votes} \\t\\t Villagers : {villager_votes}')\n",
    "        #print([f'{wolf.split(\"_\")[-1]} : {stage[\"votes\"][\"player\"_{wolf.split(\"_\")[-1]}]}]' for wolf in stage[\"werewolves\"]])\n",
    "        # for wolf in stage['werewolves']:\n",
    "        #     wid = wolf.split(\"_\")[-1]\n",
    "        #     pid = f\"player_{wid}\"\n",
    "        #     stage[\"votes\"][pid]\n",
    "        #     print(f'p_{wid} : {stage[\"votes\"][pid]}')\n",
    "        # print(stage['votes'])\n",
    "\n",
    "print_replay(replay[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
