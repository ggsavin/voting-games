{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import raw_env, Roles\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Roles(enum.IntEnum):\n",
    "#     VILLAGER = 0\n",
    "#     WEREWOLF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self, num_actions, obs_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            self._layer_init(torch.nn.Linear(obs_size, 64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64,64)),\n",
    "            torch.nn.Tanh(),\n",
    "            self._layer_init(torch.nn.Linear(64, num_actions), std=0.01),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        probs = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    available_actions = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(available_actions, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    action = random.choice(legal_actions)\n",
    "    return action\n",
    "\n",
    "def true_random_policy(observation, agent):\n",
    "    return random.choice(list(range(len(observation['observation']['player_status']))))\n",
    "\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == me and i != me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    choice = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # pick a villager to vote for that is alive\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "    return action if action != None else random.choice(villagers_alive)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our mlflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "#mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "# with mlflow.start_run(run_name='Wolf Experiment'):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate wolf revenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "API request to http://mlflow:5000/api/2.0/mlflow/runs/create failed with exception HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbb2a63f820>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m six\u001b[39m.\u001b[39mraise_from(\n\u001b[1;32m     69\u001b[0m         LocationParseError(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, label empty or too long\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m host), \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fbb2a63f820>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 815 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:815\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    812\u001b[0m     log\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetrying (\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) after connection broken by \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, retries, err, url\n\u001b[1;32m    814\u001b[0m     )\n\u001b[0;32m--> 815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    816\u001b[0m         method,\n\u001b[1;32m    817\u001b[0m         url,\n\u001b[1;32m    818\u001b[0m         body,\n\u001b[1;32m    819\u001b[0m         headers,\n\u001b[1;32m    820\u001b[0m         retries,\n\u001b[1;32m    821\u001b[0m         redirect,\n\u001b[1;32m    822\u001b[0m         assert_same_host,\n\u001b[1;32m    823\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    824\u001b[0m         pool_timeout\u001b[39m=\u001b[39;49mpool_timeout,\n\u001b[1;32m    825\u001b[0m         release_conn\u001b[39m=\u001b[39;49mrelease_conn,\n\u001b[1;32m    826\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    827\u001b[0m         body_pos\u001b[39m=\u001b[39;49mbody_pos,\n\u001b[1;32m    828\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    831\u001b[0m \u001b[39m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbb2a63f820>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:167\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, retry_codes, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_http_response_with_retries(\n\u001b[1;32m    168\u001b[0m         method,\n\u001b[1;32m    169\u001b[0m         url,\n\u001b[1;32m    170\u001b[0m         max_retries,\n\u001b[1;32m    171\u001b[0m         backoff_factor,\n\u001b[1;32m    172\u001b[0m         retry_codes,\n\u001b[1;32m    173\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    174\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    175\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    176\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m to:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:98\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, retry_codes, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m session \u001b[39m=\u001b[39m _get_request_session(max_retries, backoff_factor, retry_codes)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbb2a63f820>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m self_voting \u001b[39m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m dead_voting \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39;49mstart_run(run_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mCoordinated Wolf Revenge\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_games)):\n\u001b[1;32m     37\u001b[0m         env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/tracking/fluent.py:349\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, tags, description)\u001b[0m\n\u001b[1;32m    345\u001b[0m         user_specified_tags[MLFLOW_RUN_NAME] \u001b[39m=\u001b[39m run_name\n\u001b[1;32m    347\u001b[0m     resolved_tags \u001b[39m=\u001b[39m context_registry\u001b[39m.\u001b[39mresolve_tags(user_specified_tags)\n\u001b[0;32m--> 349\u001b[0m     active_run_obj \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    350\u001b[0m         experiment_id\u001b[39m=\u001b[39;49mexp_id_for_run, tags\u001b[39m=\u001b[39;49mresolved_tags, run_name\u001b[39m=\u001b[39;49mrun_name\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    353\u001b[0m _active_run_stack\u001b[39m.\u001b[39mappend(ActiveRun(active_run_obj))\n\u001b[1;32m    354\u001b[0m \u001b[39mreturn\u001b[39;00m _active_run_stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/tracking/client.py:270\u001b[0m, in \u001b[0;36mMlflowClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_run\u001b[39m(\n\u001b[1;32m    220\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    221\u001b[0m     experiment_id: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m     run_name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    225\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Run:\n\u001b[1;32m    226\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    Create a :py:class:`mlflow.entities.Run` object that can be associated with\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m    metrics, parameters, artifacts, etc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        status: RUNNING\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mcreate_run(experiment_id, start_time, tags, run_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:131\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_run\u001b[0;34m(self, experiment_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# Extract user from tags\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# This logic is temporary; the user_id attribute of runs is deprecated and will be removed\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m# in a later release.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m user_id \u001b[39m=\u001b[39m tags\u001b[39m.\u001b[39mget(MLFLOW_USER, \u001b[39m\"\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mcreate_run(\n\u001b[1;32m    132\u001b[0m     experiment_id\u001b[39m=\u001b[39;49mexperiment_id,\n\u001b[1;32m    133\u001b[0m     user_id\u001b[39m=\u001b[39;49muser_id,\n\u001b[1;32m    134\u001b[0m     start_time\u001b[39m=\u001b[39;49mstart_time \u001b[39mor\u001b[39;49;00m get_current_time_millis(),\n\u001b[1;32m    135\u001b[0m     tags\u001b[39m=\u001b[39;49m[RunTag(key, value) \u001b[39mfor\u001b[39;49;00m (key, value) \u001b[39min\u001b[39;49;00m tags\u001b[39m.\u001b[39;49mitems()],\n\u001b[1;32m    136\u001b[0m     run_name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:175\u001b[0m, in \u001b[0;36mRestStore.create_run\u001b[0;34m(self, experiment_id, user_id, start_time, tags, run_name)\u001b[0m\n\u001b[1;32m    165\u001b[0m tag_protos \u001b[39m=\u001b[39m [tag\u001b[39m.\u001b[39mto_proto() \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tags]\n\u001b[1;32m    166\u001b[0m req_body \u001b[39m=\u001b[39m message_to_json(\n\u001b[1;32m    167\u001b[0m     CreateRun(\n\u001b[1;32m    168\u001b[0m         experiment_id\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(experiment_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 175\u001b[0m response_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_endpoint(CreateRun, req_body)\n\u001b[1;32m    176\u001b[0m run \u001b[39m=\u001b[39m Run\u001b[39m.\u001b[39mfrom_proto(response_proto\u001b[39m.\u001b[39mrun)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:56\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body)\u001b[0m\n\u001b[1;32m     54\u001b[0m endpoint, method \u001b[39m=\u001b[39m _METHOD_TO_INFO[api]\n\u001b[1;32m     55\u001b[0m response_proto \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mResponse()\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m call_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_host_creds(), endpoint, method, json_body, response_proto)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:278\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto)\u001b[0m\n\u001b[1;32m    274\u001b[0m     response \u001b[39m=\u001b[39m http_request(\n\u001b[1;32m    275\u001b[0m         host_creds\u001b[39m=\u001b[39mhost_creds, endpoint\u001b[39m=\u001b[39mendpoint, method\u001b[39m=\u001b[39mmethod, params\u001b[39m=\u001b[39mjson_body\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[39m=\u001b[39m http_request(\n\u001b[1;32m    279\u001b[0m         host_creds\u001b[39m=\u001b[39;49mhost_creds, endpoint\u001b[39m=\u001b[39;49mendpoint, method\u001b[39m=\u001b[39;49mmethod, json\u001b[39m=\u001b[39;49mjson_body\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m response \u001b[39m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[1;32m    282\u001b[0m js_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:185\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, retry_codes, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    180\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with timeout exception \u001b[39m\u001b[39m{\u001b[39;00mto\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m To increase the timeout, set the environment variable \u001b[39m\u001b[39m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m to a larger value.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAPI request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m failed with exception \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMlflowException\u001b[0m: API request to http://mlflow:5000/api/2.0/mlflow/runs/create failed with exception HTTPConnectionPool(host='mlflow', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/create (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fbb2a63f820>: Failed to establish a new connection: [Errno -2] Name or service not known'))"
     ]
    }
   ],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == me and i != me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    choice = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "with mlflow.start_run(run_name='Coordinated Wolf Revenge'):\n",
    "    for _ in tqdm(range(num_games)):\n",
    "        env.reset()\n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        \n",
    "        self_votes = 0\n",
    "        dead_votes = 0\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            day = observation['observation']['day']\n",
    "            phase = observation['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "            if role == Roles.WEREWOLF:\n",
    "                action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                wolf_brain['action'] = action\n",
    "            else:\n",
    "                action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "                # check how many times the action lines up with the agent\n",
    "                if action == observation['observation']['self_id']:\n",
    "                    self_votes += 1\n",
    "                \n",
    "                if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                    dead_votes += 1\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        # get some stats\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        self_voting.append(self_votes)\n",
    "        dead_voting.append(dead_votes)\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "        avg_game_length += (day * 1.0)/num_games\n",
    "\n",
    "        if num_games % 20 == 0:\n",
    "            mlflow.log_metric(\"avg_game_len\", f'{avg_game_length:.2f}')\n",
    "            mlflow.log_metric(\"avg_self_votes\", f'{sum(self_voting)/len(self_voting)}')\n",
    "            mlflow.log_metric(\"avg_dead_votes\", f'{sum(dead_voting)/len(dead_voting)}')\n",
    "            # mlflow.log_param(\"avg_game_len\", f'{avg_game_length:.2f}')\n",
    "            # mlflow.log_param(\"avg_self_votes\", f'{sum(self_voting)/len(self_voting)}')\n",
    "            # mlflow.log_param(\"avg_dead_votes\", f'{sum(dead_voting)/len(dead_voting)}')\n",
    "\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')\n",
    "print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "print(f'Avg amount of dead votes a game across villagers: {sum(dead_voting)/len(dead_voting)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train a policy on the given reward structure we currently have, and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_obs(observation):\n",
    "    return  np.asarray([observation['day']] + \\\n",
    "            [observation['phase']] + \\\n",
    "            [int(status) for status in observation['player_status']] + \\\n",
    "            [role for role in observation['roles']] + \\\n",
    "            [vote for vote in observation['votes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(ppo_agent.parameters(), lr=0.001, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 16 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 9000 #\n",
    "update_epochs = 3 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 6, Villager wins : 4\n"
     ]
    }
   ],
   "source": [
    "def play_and_return_stats(env, wolf_policy, agent_policy, num_games=1000):\n",
    "    wolf_wins = 0\n",
    "    villager_wins = 0\n",
    "\n",
    "    game_replays = []\n",
    "    for _ in range(num_games):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : {\"self_votes\": 0, \"dead_votes\": 0, \"lasted_for\": 0} for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = wolf_policy(observation, None, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    # action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "                    obs = torch.Tensor(flat_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = agent_policy.get_action_and_value(obs)\n",
    "                        action = action.item()\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : maybe make these callbacks?\n",
    "                    if action == observation['observation']['self_id']:\n",
    "                        magent_list[magent]['self_votes'] += 1\n",
    "                \n",
    "                    if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                        magent_list[magent]['dead_votes'] += 1\n",
    "                        \n",
    "                env.step(action)\n",
    "            \n",
    "        game_replays.append(env.history)\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "    return wolf_wins, villager_wins, game_replays\n",
    "\n",
    "\n",
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "ppo_agent = torch.load(\"ppo_agent\")\n",
    "\n",
    "wwins, vwins, game_replay = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=10)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 1,\n",
       " 'phase': <Phase.ACCUSATION: 0>,\n",
       " 'alive': ['player_3', 'player_5'],\n",
       " 'killed': ['player_4', 'player_1', 'player_2', 'player_8'],\n",
       " 'executed': ['player_7', 'player_6', 'player_10', 'player_9'],\n",
       " 'werewolves': ['player_3', 'player_10'],\n",
       " 'villagers': ['player_1',\n",
       "  'player_2',\n",
       "  'player_4',\n",
       "  'player_5',\n",
       "  'player_6',\n",
       "  'player_7',\n",
       "  'player_8',\n",
       "  'player_9'],\n",
       " 'votes': {'player_1': 0,\n",
       "  'player_2': 6,\n",
       "  'player_3': 6,\n",
       "  'player_4': 0,\n",
       "  'player_5': 6,\n",
       "  'player_6': 0,\n",
       "  'player_7': 9,\n",
       "  'player_8': 7,\n",
       "  'player_9': 5,\n",
       "  'player_10': 6},\n",
       " 'winners': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_replay[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 4,\n",
       " 'phase': 2,\n",
       " 'alive': ['player_3', 'player_5'],\n",
       " 'killed': ['player_4', 'player_1', 'player_2', 'player_8'],\n",
       " 'executed': ['player_7', 'player_6', 'player_10', 'player_9'],\n",
       " 'werewolves': ['player_3', 'player_10'],\n",
       " 'villagers': ['player_1',\n",
       "  'player_2',\n",
       "  'player_4',\n",
       "  'player_5',\n",
       "  'player_6',\n",
       "  'player_7',\n",
       "  'player_8',\n",
       "  'player_9'],\n",
       " 'votes': {'player_3': 7},\n",
       " 'winners': <Roles.WEREWOLF: 1>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_replay[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50000/50000 [1:21:10<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.00\n",
      "Wolf wins : 50000\n",
      "Villager wins: 0\n",
      "Avg amount of self votes a game across villagers: 5.38126\n",
      "Avg amount of dead votes a game across villagers: 5.52664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 16 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 50000 #\n",
    "update_epochs = 3 #\n",
    "\n",
    "# stats to keep track of for custom metrics\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "num_agents = 10\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(ppo_agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='Random Wolf behavior'):\n",
    "    for episode in tqdm(range(total_episodes)):\n",
    "        with torch.no_grad():\n",
    "            env.reset()\n",
    "\n",
    "            # brain and extra stats \n",
    "            wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "            self_votes = 0\n",
    "            dead_votes = 0\n",
    "            \n",
    "            # magent_list = {agent: [] for agent in env.agents}\n",
    "            magent_list = {agent : [] for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "            # print(magent_list.keys())\n",
    "            for magent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "                day = observation['observation']['day']\n",
    "                phase = observation['observation']['phase']\n",
    "\n",
    "                if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                    wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "                role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "                # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "                if role == Roles.WEREWOLF:\n",
    "                    # action = revenge_wolf_policy(observation, magent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    # wolf_brain['action'] = action\n",
    "                    # action = random_policy(observation, magent) if not termination or truncation else None\n",
    "\n",
    "                    action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                    wolf_brain['action'] = action\n",
    "                else:\n",
    "                    obs = torch.Tensor(flat_obs(observation['observation']))\n",
    "                    if not termination or truncation:\n",
    "                        action, logprobs, _, value = ppo_agent.get_action_and_value(obs)\n",
    "                    else:\n",
    "                        action = None\n",
    "\n",
    "                    # grab some villager stats we think are useful\n",
    "                    # TODO : maybe make these callbacks?\n",
    "                    if action == observation['observation']['self_id']:\n",
    "                        self_votes += 1\n",
    "                \n",
    "                    if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                        dead_votes += 1\n",
    "\n",
    "                    magent_list[magent].append({\n",
    "                        \"obs\": obs, \n",
    "                        \"action\": action,\n",
    "                        \"prev_reward\": reward,\n",
    "                        \"logprobs\": logprobs,\n",
    "                        \"term\": termination,\n",
    "                        \"value\": value\n",
    "                        })\n",
    "\n",
    "                env.step(action)\n",
    "            \n",
    "            # take the sequential observations of each agent, and store them appropriately\n",
    "            magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': [], 'terms': []} for agent in magent_list}\n",
    "            for key, value in magent_list.items():\n",
    "                # print(f'-- {key} --')\n",
    "                for s1, s2 in zip(value, value[1:]):\n",
    "                    magent_obs[key]['obs'].append(s1['obs'])\n",
    "                    magent_obs[key]['rewards'].append(s2['prev_reward'])\n",
    "                    magent_obs[key]['actions'].append(s1['action'])\n",
    "                    magent_obs[key]['logprobs'].append(s1['logprobs'])\n",
    "                    magent_obs[key]['values'].append(s1['value'])\n",
    "                    magent_obs[key]['terms'].append(s2['term'])\n",
    "\n",
    "\n",
    "        # POST GAME STATS #\n",
    "        winner = env.world_state['winners']\n",
    "        day = env.world_state['day']\n",
    "\n",
    "        self_voting.append(self_votes)\n",
    "        dead_voting.append(dead_votes)\n",
    "\n",
    "        if winner:\n",
    "            wolf_wins += 1\n",
    "        else:\n",
    "            villager_wins += 1\n",
    "        \n",
    "        avg_game_length += (day * 1.0)/total_episodes\n",
    "        # END OF POST GAME STATS #\n",
    "\n",
    "        mlflow.log_metric(\"avg_game_len\", f'{avg_game_length:.2f}')\n",
    "        mlflow.log_metric(\"avg_self_votes\", f'{sum(self_voting)/len(self_voting)}')\n",
    "        mlflow.log_metric(\"avg_dead_votes\", f'{sum(dead_voting)/len(dead_voting)}')\n",
    "        mlflow.log_metric(\"wolf wins\", wolf_wins)\n",
    "        mlflow.log_metric(\"villager wins\", villager_wins)\n",
    "        if episode % 50 == 0:\n",
    "            #wwins, vwins = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=100)\n",
    "            #mlflow.log_metric(\"wwins\", wwins)\n",
    "            #mlflow.log_metric(\"vwins\", vwins)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "\n",
    "        # We will do this for each agent in the episode\n",
    "        # essentially we are calculating advantages and returns\n",
    "        with torch.no_grad():\n",
    "            for player, records in magent_obs.items():\n",
    "                # print(f'{records}')\n",
    "                advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "\n",
    "                for t in reversed(range(len(records['obs']))):\n",
    "                    # print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                    # not using terms, as these are episodic\n",
    "\n",
    "                    ## this was the last one. We are not using any terminal states in a good way\n",
    "\n",
    "                    if t == len(records['obs']) - 1:\n",
    "                        #print(f'T: {t} - Rewards at end : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions at end : {torch.tensor(records[\"actions\"])[t]} ')\n",
    "                        delta = records[\"rewards\"][t] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta\n",
    "                    else:\n",
    "                        #print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                        #print(f'T: {t} - Actions : {torch.tensor(records[\"actions\"])[t]} ')                    \n",
    "                        delta = records[\"rewards\"][t] + gamma * records[\"values\"][t+1] - records[\"values\"][t]\n",
    "                        advantages[t]  = delta + gamma * gamma * advantages[t+1]\n",
    "\n",
    "                    #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "                magent_obs[player][\"advantages\"] = advantages\n",
    "                magent_obs[player][\"returns\"] = advantages + torch.tensor(records[\"values\"])\n",
    "                    #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "        \n",
    "\n",
    "        # new logic, maybe we do this after a couple of games, so we get more data overall?\n",
    "        \n",
    "\n",
    "        # optimize the policy and the value network now\n",
    "        # we can take all our observations now and flatten them into one bigger list of individual transitions\n",
    "        # TODO: could make this setting into a single loop, but maybe this is clearer. ALso could make all these tensors earlier\n",
    "        b_observations = torch.cat([torch.stack(item['obs']) for item in magent_obs.values()])\n",
    "        b_logprobs = torch.cat([torch.stack(item['logprobs']) for item in magent_obs.values()])\n",
    "        b_actions = torch.cat([torch.stack(item['actions']) for item in magent_obs.values()])\n",
    "        b_returns = torch.cat([item['returns'] for item in magent_obs.values()])\n",
    "        b_values = torch.cat([torch.stack(item['values']) for item in magent_obs.values()])\n",
    "        b_advantages =  torch.cat([item['advantages'] for item in magent_obs.values()])\n",
    "\n",
    "\n",
    "\n",
    "        # b_index stands for batch index\n",
    "        b_index = np.arange(len(b_observations))\n",
    "        clip_fracs = []\n",
    "        for epoch in range(update_epochs):\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_observations), batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = ppo_agent.get_action_and_value(\n",
    "                    b_observations[batch_index], b_actions.long()[batch_index])\n",
    "                \n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "                \n",
    "                # normalizing advantages\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = advantages.float()\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # policy loss\n",
    "                pg_loss1 = -advantages * ratio\n",
    "                pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # could move them from GPU here\n",
    "        y_pred, y_true = b_values.numpy(), b_returns.numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        \n",
    "        # if episode % 20 == 0:\n",
    "            # print(f\"Training episode {episode}\")\n",
    "            # #print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "            # #print(f\"Episode Length: {end_step}\")\n",
    "            # print(\"\")\n",
    "            # print(f\"Value Loss: {v_loss.item()}\")\n",
    "            # print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "            # print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "            # print(f\"Approx KL: {approx_kl.item()}\")\n",
    "            # print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "            # print(f\"Explained Variance: {explained_var.item()}\")\n",
    "            # print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "            # also check some stats and try to log these\n",
    "\n",
    "    # At the end, print some stuff here for overall stats\n",
    "\n",
    "    print(f'Average game length = {avg_game_length:.2f}')\n",
    "    print(f'Wolf wins : {wolf_wins}')\n",
    "    print(f'Villager wins: {villager_wins}')\n",
    "    print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "    print(f'Avg amount of dead votes a game across villagers: {sum(dead_voting)/len(dead_voting)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf wins : 8, Villager wins : 12\n"
     ]
    }
   ],
   "source": [
    "env = raw_env(num_agents=10, werewolves=1)\n",
    "env.reset()\n",
    "num_agents = 5\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "# ppo_agent = Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "\n",
    "wwins, vwins = play_and_return_stats(env, random_wolf_policy, ppo_agent, num_games=20)\n",
    "print(f'Wolf wins : {wwins}, Villager wins : {vwins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppo_agent, \"ppo_agent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinated wolf execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:01<00:00, 843.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 925\n",
      "Villager wins: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Wolves, not coordinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:00<00:00, 390.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 13\n",
      "Villager wins: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = raw_env(num_agents=10, werewolves=1)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 20\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/02/26 01:02:02 INFO mlflow.tracking.fluent: Experiment with name 'mlflow-test' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"mlflow-test\")\n",
    "mlflow.log_metric(\"test\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
