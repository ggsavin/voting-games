{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from voting_games.werewolf_env_v0 import raw_env\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import enum\n",
    "import ppo_agent as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roles(enum.IntEnum):\n",
    "    VILLAGER = 0\n",
    "    WEREWOLF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation, agent):\n",
    "    # these are the other wolves. we cannot vote for them either\n",
    "    available_actions = list(range(len(observation['observation']['player_status'])))\n",
    "    # dead players\n",
    "    action_mask = observation['action_mask']\n",
    "\n",
    "    legal_actions = [action for action,is_alive,is_wolf in zip(available_actions, action_mask, observation['observation']['roles']) if is_alive and not is_wolf]\n",
    "    # wolves don't vote for other wolves. will select another villager at random\n",
    "    action = random.choice(legal_actions)\n",
    "    return action\n",
    "\n",
    "def true_random_policy(observation, agent):\n",
    "    return random.choice(list(range(len(observation['observation']['player_status']))))\n",
    "\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == me and i != me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    choice = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "def random_wolf_policy(observation, agent, action=None):\n",
    "    # pick a villager to vote for that is alive\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "    return action if action != None else random.choice(villagers_alive)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate wolf revenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 761.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.50\n",
      "Wolf wins : 759\n",
      "Villager wins: 241\n",
      "Avg amount of self votes a game across villagers: 6.242\n",
      "Avg amount of dead votes a game across villagers: 14.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "def revenge_wolf_policy(observation, agent, action=None):\n",
    "    # we already know the agent is a werewolf\n",
    "    me = observation['observation']['self_id']\n",
    "\n",
    "    # who voted for me \n",
    "    votes_against_me = [i for i, x in enumerate(observation['observation']['votes']) if x == me and i != me]\n",
    "\n",
    "    # remove any wolves who voted for me (they should not have)\n",
    "    wolf_ids = [i for i, x in enumerate(observation['observation']['roles']) if x == 1 and i != me]\n",
    "    votes_against_me = list(set(votes_against_me)^set(wolf_ids))\n",
    "\n",
    "    # remove any players who voted for me but are dead now\n",
    "    votes_against_me = [i for i in votes_against_me if observation['observation']['player_status'][i] == True]\n",
    "\n",
    "    villagers_alive = [i for i, x in enumerate(observation['observation']['roles']) \\\n",
    "        if observation['observation']['player_status'][i] == True and x == 0]\n",
    "\n",
    "    # if there are no votes against me, pick a random villager that is alive\n",
    "    choice = random.choice(votes_against_me) if len(votes_against_me) > 0 else random.choice(villagers_alive)\n",
    "\n",
    "    return action if action != None else choice\n",
    "\n",
    "\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    self_votes = 0\n",
    "    dead_votes = 0\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = true_random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "            # check how many times the action lines up with the agent\n",
    "            if action == observation['observation']['self_id']:\n",
    "                self_votes += 1\n",
    "            \n",
    "            if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                dead_votes += 1\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    self_voting.append(self_votes)\n",
    "    dead_voting.append(dead_votes)\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games\n",
    "\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')\n",
    "print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "print(f'Avg amount of dead votes a game across villagers: {sum(dead_voting)/len(dead_voting)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train a policy on the given reward structure we currently have, and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_obs(observation):\n",
    "    return  np.asarray([observation['day']] + \\\n",
    "            [observation['phase']] + \\\n",
    "            [int(status) for status in observation['player_status']] + \\\n",
    "            [role for role in observation['roles']] + \\\n",
    "            [vote for vote in observation['votes']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "# env.reset()\n",
    "num_agents = 10\n",
    "num_actions = env.action_spaces['player_1'].n\n",
    "observation_size = flat_obs(env.observation_spaces['player_1'].sample()['observation']).shape[-1]\n",
    "\n",
    "# Learner Setup\n",
    "ppo_agent = ppo.Agent(num_actions=num_actions, obs_size=observation_size)\n",
    "optimizer = torch.optim.Adam(ppo_agent.parameters(), lr=0.001, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_coef = 0.1 #\n",
    "vf_coef = 0.1 #\n",
    "clip_coef = 0.1 #\n",
    "gamma = 0.99 #\n",
    "gae_lambda = 0.95\n",
    "batch_size = 16 #\n",
    "max_cycles = 125 #\n",
    "total_episodes = 100 #\n",
    "update_epochs = 3 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 33x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(flat_obs(observation[\u001b[39m'\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m termination \u001b[39mor\u001b[39;00m truncation:\n\u001b[0;32m---> 43\u001b[0m     action, logprobs, _, value \u001b[39m=\u001b[39m ppo_agent\u001b[39m.\u001b[39;49mget_action_and_value(obs)\n\u001b[1;32m     44\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/voting-games/notebooks/ppo_agent.py:29\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[0;34m(self, x, action)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action_and_value\u001b[39m(\u001b[39mself\u001b[39m, x, action\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 29\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(x)\n\u001b[1;32m     31\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mcategorical\u001b[39m.\u001b[39mCategorical(logits\u001b[39m=\u001b[39mlogits)\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 33x64)"
     ]
    }
   ],
   "source": [
    "# stats to keep track of for custom metrics\n",
    "self_voting = []\n",
    "dead_voting = []\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        env.reset()\n",
    "\n",
    "        # brain and extra stats \n",
    "        wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "        self_votes = 0\n",
    "        dead_votes = 0\n",
    "        \n",
    "        # magent_list = {agent: [] for agent in env.agents}\n",
    "        magent_list = {agent : [] for agent in env.agents if not env.agent_roles[agent]}\n",
    "\n",
    "        # print(magent_list.keys())\n",
    "        for magent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            day = observation['observation']['day']\n",
    "            phase = observation['observation']['phase']\n",
    "\n",
    "            if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "                wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "            role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "            # werewolves have full role TODO: add logic for wolves herevisibility\n",
    "            if role == Roles.WEREWOLF:\n",
    "                action = revenge_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "                wolf_brain['action'] = action\n",
    "            else:\n",
    "                obs = torch.Tensor(flat_obs(observation['observation']))\n",
    "                if not termination or truncation:\n",
    "                    action, logprobs, _, value = ppo_agent.get_action_and_value(obs)\n",
    "                else:\n",
    "                    action = None\n",
    "\n",
    "                # grab some villager stats we think are useful\n",
    "                # TODO : maybe make these callbacks?\n",
    "                if action == observation['observation']['self_id']:\n",
    "                    self_votes += 1\n",
    "            \n",
    "                if action in [i for i, status in enumerate(observation['observation']['player_status']) if status == False]:\n",
    "                    dead_votes += 1\n",
    "\n",
    "                magent_list[magent].append({\n",
    "                    \"obs\": obs, \n",
    "                    \"action\": action,\n",
    "                    \"prev_reward\": reward,\n",
    "                    \"logprobs\": logprobs,\n",
    "                    \"term\": termination,\n",
    "                    \"value\": value\n",
    "                    })\n",
    "\n",
    "            env.step(action)\n",
    "        \n",
    "        # take the sequential observations of each agent, and store them appropriately\n",
    "        magent_obs = {agent: {'obs': [], 'rewards': [], 'actions': [], 'logprobs': [], 'values': [], 'terms': []} for agent in magent_list}\n",
    "        for key, value in magent_list.items():\n",
    "            # print(f'-- {key} --')\n",
    "            for s1, s2 in zip(value, value[1:]):\n",
    "                magent_obs[key]['obs'].append(s1['obs'])\n",
    "                magent_obs[key]['rewards'].append(s2['prev_reward'])\n",
    "                magent_obs[key]['actions'].append(s1['action'])\n",
    "                magent_obs[key]['logprobs'].append(s1['logprobs'])\n",
    "                magent_obs[key]['values'].append(s1['value'])\n",
    "                magent_obs[key]['terms'].append(s2['term'])\n",
    "\n",
    "\n",
    "    # POST GAME STATS #\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    self_voting.append(self_votes)\n",
    "    dead_voting.append(dead_votes)\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/total_episodes\n",
    "    # END OF POST GAME STATS #\n",
    "    \n",
    "\n",
    "    # We will do this for each agent in the episode\n",
    "    # essentially we are calculating advantages and returns\n",
    "    with torch.no_grad():\n",
    "        for player, records in magent_obs.items():\n",
    "            # print(f'{records}')\n",
    "            advantages = torch.zeros_like(torch.tensor(records['rewards']))\n",
    "\n",
    "            for t in reversed(range(len(records['obs']))):\n",
    "                # print(f'T: {t+1} - Rewards : {torch.tensor(records[\"rewards\"])[t+1]} ')\n",
    "                # not using terms, as these are episodic\n",
    "\n",
    "                ## this was the last one. We are not using any terminal states in a good way\n",
    "\n",
    "                if t == len(records['obs']) - 1:\n",
    "                    #print(f'T: {t} - Rewards at end : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                    #print(f'T: {t} - Actions at end : {torch.tensor(records[\"actions\"])[t]} ')\n",
    "                    delta = records[\"rewards\"][t] - records[\"values\"][t]\n",
    "                    advantages[t]  = delta\n",
    "                else:\n",
    "                    #print(f'T: {t} - Rewards : {torch.tensor(records[\"rewards\"])[t]} ')\n",
    "                    #print(f'T: {t} - Actions : {torch.tensor(records[\"actions\"])[t]} ')                    \n",
    "                    delta = records[\"rewards\"][t] + gamma * records[\"values\"][t+1] - records[\"values\"][t]\n",
    "                    advantages[t]  = delta + gamma * gamma * advantages[t+1]\n",
    "\n",
    "                #delta = records['rewards'][t] + gamma * records['values'][t+1] - records['values'][t]\n",
    "            magent_obs[player][\"advantages\"] = advantages\n",
    "            magent_obs[player][\"returns\"] = advantages + torch.tensor(records[\"values\"])\n",
    "                #advantages[t] = delta + gamma * gamma * advantages[t+1]\n",
    "    \n",
    "\n",
    "    # optimize the policy and the value network now\n",
    "    # we can take all our observations now and flatten them into one bigger list of individual transitions\n",
    "    # TODO: could make this setting into a single loop, but maybe this is clearer. ALso could make all these tensors earlier\n",
    "    b_observations = torch.cat([torch.stack(item['obs']) for item in magent_obs.values()])\n",
    "    b_logprobs = torch.cat([torch.stack(item['logprobs']) for item in magent_obs.values()])\n",
    "    b_actions = torch.cat([torch.stack(item['actions']) for item in magent_obs.values()])\n",
    "    b_returns = torch.cat([item['returns'] for item in magent_obs.values()])\n",
    "    b_values = torch.cat([torch.stack(item['values']) for item in magent_obs.values()])\n",
    "    b_advantages =  torch.cat([item['advantages'] for item in magent_obs.values()])\n",
    "\n",
    "\n",
    "\n",
    "    # b_index stands for batch index\n",
    "    b_index = np.arange(len(b_observations))\n",
    "    clip_fracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_observations), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = ppo_agent.get_action_and_value(\n",
    "                b_observations[batch_index], b_actions.long()[batch_index])\n",
    "            \n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [\n",
    "                    ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                ]\n",
    "            \n",
    "            # normalizing advantages\n",
    "            advantages = b_advantages[batch_index]\n",
    "            advantages = advantages.float()\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            # policy loss\n",
    "            pg_loss1 = -advantages * ratio\n",
    "            pg_loss2 = -advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # could move them from GPU here\n",
    "    y_pred, y_true = b_values.numpy(), b_returns.numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "    \n",
    "    if episode % 20 == 0:\n",
    "        print(f\"Training episode {episode}\")\n",
    "        #print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "        #print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "        # also check some stats and try to log these\n",
    "\n",
    "# At the end, print some stuff here for overall stats\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')\n",
    "print(f'Avg amount of self votes a game across villagers: {sum(self_voting)/len(self_voting)}')\n",
    "print(f'Avg amount of dead votes a game across villagers: {sum(dead_voting)/len(dead_voting)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinated wolf execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 843.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.15\n",
      "Wolf wins : 925\n",
      "Villager wins: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = raw_env(num_agents=10, werewolves=2)\n",
    "env.reset()\n",
    "\n",
    "num_games = 1000\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "    env.reset()\n",
    "    wolf_brain = {'day': 1, 'phase': 0, 'action': None}\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        day = observation['observation']['day']\n",
    "        phase = observation['observation']['phase']\n",
    "\n",
    "        if wolf_brain['day'] != day or wolf_brain['phase'] != phase:\n",
    "            wolf_brain = {'day': day, 'phase': phase, 'action': None}\n",
    "\n",
    "        role = observation['observation']['roles'][observation['observation']['self_id']]\n",
    "\n",
    "        if role == Roles.WEREWOLF:\n",
    "            action = random_wolf_policy(observation, agent, action=wolf_brain['action']) if not termination or truncation else None\n",
    "            wolf_brain['action'] = action\n",
    "        else:\n",
    "            action = random_policy(observation, agent) if not termination or truncation else None\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    # get some stats\n",
    "    winner = env.world_state['winners']\n",
    "    day = env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Wolves, not coordinated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 826.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average game length = 4.25\n",
      "Wolf wins : 919\n",
      "Villager wins: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ten_player_env = raw_env(num_agents=10, werewolves=2)\n",
    "\n",
    "avg_game_length = 0\n",
    "wolf_wins = 0\n",
    "villager_wins = 0\n",
    "\n",
    "num_games = 1000\n",
    "\n",
    "ten_player_env.reset()\n",
    "\n",
    "for _ in tqdm(range(num_games)):\n",
    "\n",
    "    for agent in ten_player_env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = ten_player_env.last()\n",
    "        action = random_policy(observation, agent) if not termination or truncation else None\n",
    "        ten_player_env.step(action)\n",
    "    \n",
    "    # get some stats\n",
    "    winner = ten_player_env.world_state['winners']\n",
    "    day = ten_player_env.world_state['day']\n",
    "\n",
    "    if winner:\n",
    "        wolf_wins += 1\n",
    "    else:\n",
    "        villager_wins += 1\n",
    "    \n",
    "    avg_game_length += (day * 1.0)/num_games \n",
    "\n",
    "    # reset \n",
    "    ten_player_env.reset()\n",
    "\n",
    "print(f'Average game length = {avg_game_length:.2f}')\n",
    "print(f'Wolf wins : {wolf_wins}')\n",
    "print(f'Villager wins: {villager_wins}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
